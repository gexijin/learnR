[["index.html", "Learn R through examples Preface", " Learn R through examples Xijin Ge, Jianli Qi, and Rong Fan 2024-01-16 Preface Targeted at absolute beginners, this book is written based on the philosophy that people learn best through examples. Rather than delving into complex rules, this book focuses on analyzing various datasets from the very beginning. This approach offers an alternative to the traditional, more rigorous R programming textbooks. We begin with small, tidy datasets and gradually move on to larger, messier ones. With each dataset, our goal is to craft a story through analysis. We invite you, our intrepid reader, to join us on this journey. Motivated readers, including biologists, can navigate the book effectively and independently. I strongly encourage you to type in the example code, examine the outputs, and then tackle the challenges and exercises. Originally, this book was started with the material in first several chapters, which were designed for 2-hour hands-on workshops, providing a swift introduction/demonstration for students and researchers new to R. These workshops have been conducted multiple times, serving diverse audiences, including high-school students and mathematicians. Given the constraints of a 2-hour session, we prioritize keeping it use-friendly, interactive, and enjoyable, sometimes at the expense of strict rigor. Rather than dwelling on all the rules, grammar, and syntax, we found it more effective to focus on a single dataset and guide participants through various possible analyses using R. These materials later evolved into a one-credit online class and then a three-credit course. Throughout, we maintain our unconventional focus on datasets and practical examples. Another distinctive feature of this book is our review of the underlying statistical concepts. R serves as a language for statistical computing, making it inseparable from its statistical context. Coding and cooking. If you are new to coding, think of it as drafting a recipe. Your objective is to offer precise, step-by-step instructions that even a 10-year-old can use to transform raw ingredients (data) into delicious pasta (results). Just as a good recipe should be followed repeatedly to make pasta with the same ingredients, computer programs can process data with the exact specifications, much like how millions of people share their code on repositories like GitHub. The R programming environment is like a wonderful kitchen. It’s free, powerful, interactive, and beginner friendly. In this kitchen, you can find many tools (akin to knives) and complex appliances (similar to stoves); in R, we call them functions, created by others (sometimes painstakingly over many years) and ready to be harnessed for data processing. Just as we must familiarize ourselves with kitchen knives, it’s imperative to acquaint ourselves with commonly used R functions. However, just like people rarely read instruction manual foreach kitchen tool, most people learn R functions from example codes provided by others on sites like StackOverflow. IIf you want to make a veggie smoothie, the recipe requires a fancy blender. You can conveniently go to a marketplace such as Amazon to buy one. Similarly, in the realm of the R programming language, you can conveniently download additional R packages from The Comprehensive R Archive Network (CRAN), a FREE marketplace where tens of thousands of people contribute. The open and collaborative user community is uniquely productive. People build upon one another’s work, continually enhancing functionality while preserving simplicity. There are R packages that can help you create complex charts, write a book (like this one), host a website, or even find a girlfriend (just kidding). Imagine a free appliance that can turn uncooked chicken, vegetables, oil, and spices into delicious Kung Pao Chicken! That is precisely how I feel every time I employ other people’s R packages to analyze genomic data. In your kitchen, you also find jars, dishes, salt dispensers, pots, and so on; each serves a distinct purpose for different ingredients or foods. Even though some containers are only needed to store intermediate products, it is essential to know what kind of containers there are before starting to cook. Similarly, in programming, we encounter different pre-defined data types. A scalar variable can hold a single number, some text(strings), or a true/false indicator (logical values). A vector contains a sequence of scalars of the same kind. Resembling an Excel spreadsheet with rows and columns, a data frame can be visualized as multiple vectors of the same length. In computer programming, it’s crucial to familiarize ourselves with these data structures. Common R data structures include scalars, vectors, matrices, data frames, and lists. In my early cooking endeavors, I was always frustrated when people or recipes used vague terms like “a little” olive oil, as seen in this recipe in the picture above. Without any experience, I had no idea whether that meant one drip, a teaspoon, or 1 cup of olive oil! Likewise, a 10-year-old, the target audience for our receipt, might not even understand how small “small pieces” are or even recognize what “boiling water” looks like. Computers are essentially machines that perform calculations faithfully and fastly, devoid of any common sense, unlike the human “computers” in history who could calculate, either mentally or with mechanical calculators. In programming, two key principle must be upheld: (1) provide precise, specific step-by-step commands at each step and (2) define the correct sequence of operations, accounting for exceptions such as data being zero or missing. Just like composing a recipe, the process of programming can be frustrating. Patience and trial-and-error remain the only solutions. Imagine you’ve asked your 6-year-old baby sister to help peeling the carrots. Before moving on to the chopping step, you should inspect the carrots to ensure they have been appropriately peeled. In debugging, one of the primary strategies is to pause and review the intermediate products. The previous steps may not be executed correctly, even though you believe your instructions are clear and accurate. Sometimes typos creep into the code or the correct inputs are omitted. In such cases, printing out the data for a closer look is invaluable. If the data is large, we might examine the first few rows or even just the dimensions(number of rows and columns). The intermediate data objects are created in the computer memory as you execute your code. The coding process involves the step-by-step creation and modification of data objects in memory. Many students, including the authors, have contributed to this book. Special recognition goes to Quazi Irfan, who worked as a teaching assistant, fixed many errors, and provided constructive feedback. In the fall of 2018, a group of highly motivated students in the STAT 442 Exploratory Data Analysis worked on some of the datasets presented in this book. We owe gratitude to Samuel Ivanecky, Kory Heier, Audrey Bunge, Jacie McDonald, Shae Olson, Nathan Thirsten, and Alex Wieseler for inspiring some of the plots. The most effective way to master R is by using it actively, much like learning a foreign language. After the initial first two chapters, you will be well-equipped to start working on your own datasets without needing to master everything in advance. In fact, it’s nearly impossible, as the R community produces extremely useful and cool packages every day. There is no obligation to finish all the chapters of this book. Feel free to search for and borrow example code. Even programmers with 20 years of experience acknowledged that they still google basic functions daily. If you do not have a dataset of your own, you can source one from platforms such as TidyTuesday. Chapter 3 delves into more traditional R programming content, explaining data objects and common functions. If you are self-learning, there is no need to feel guilty about skipping it. You can expedite your progress by swiftly going through the later chapters of the book. There is no need to memorize the functions; instead, focus on understanding their utility. It’s worth mentioning that many students, even the authors themselves, keep revisiting the later chapters to recall how a particular plot is generated. Once you go through this book, you can use it as a reference resource. This book is still a work in progress. The later chapters need to be extensively edited, particularly. We welcome any comments and suggestions to enhance this draft, be it typos, errors, or organizational issues. The best way to reach us is through the GitHub issues page. If you prefer not to create yet another account, you can reach us via email at Xijin.Ge@sdstate.edu. "],["step-into-r-programmingthe-iris-flower-dataset.html", "Chapter 1 Step into R programming:The iris flower dataset 1.1 Getting started 1.2 The proper way of using RStudio 1.3 Data frames contain rows and columns: the iris flower dataset 1.4 Analyzing one set of numbers 1.5 Analyzing a categorical variable 1.6 The relationship between two numerical variables 1.7 Testing the differences between two groups 1.8 Testing differences among multiple groups (ANOVA)", " Chapter 1 Step into R programming:The iris flower dataset Learning objectives: Utilize R via Rstudio Cloud or through a local installation. Familiarize yourself with the RStudio interface, including Console, Script, Environment, Plots panels. Understand and manipulate data frames and vectors. Master the basics workflow: importing data, checking data types, defining vectors from columns, and data through plotting. 1.1 Getting started 1. Using RStudio Cloud: R and RStudio are accessible through a web browser via Rstudio Cloud. You can create a free account, or simply log in using your Google account. Begin by clicking the New Project button. 2. Local Installation: Alternatively, you can install R on your laptop from www.R-project.org. Then install RStudio Desktop from www.RStudio.com. If you’re using older versions of R or Rstudio, it’s wise to uninstall them, delete the folder containing R packages, and install the latest versions for an optimal experience. Figure 1.1: Interface of RStudio. RStudio uses R in the background but provides a more user-friendly interface. R commands can be typed directly into the “Console” window, as shown in Figure 1.1. For a quick demonstration, try executing the following commands in the Console window. Remember, R is case-sensitive: “Species” and “species” are two different variables. If these commands seem like a breeze, perhaps this book is too basic for you. Consider exploring more advanced books like R for Data Science. Conversely, if typing these 248 characters feels like a multi-month project, consider a typing tutorial at www.RapidTyping.com. head(iris) str(iris) summary(iris) df &lt;- iris[, 1:4] boxplot(df) pairs(df) stars(df) PL &lt;- df$Petal.Length barplot(PL) hist(PL) SP &lt;- iris$Species pie(table(SP)) boxplot(PL ~ SP) summary(aov(PL ~ SP)) PW &lt;- df$Petal.Width plot(PL, PW, col = SP) abline(lm(PW ~ PL)) Animated GIF for screen shot. 1.2 The proper way of using RStudio Create a project: In RStudio, a project serves as a convenient means to organize your work. For each new research project, we put all related files (source code, data, results) into a designated folder. On RStudio.cloud, this is a must for starting an R session. Click the Untitled Project on the top of the window to rename it as something meaningful, such as “Workshop”. For local R users, navigate to File in the main menu, select New Project, then follow the instructions to choose or create a new folder. Create a script file: Begin by going to File &gt; New File &gt; R Script, or by clicking on the green plus icon. Instead of directly typing commands into the console window, we will use the R Script window. This allows us to save our work. To execute one line of code, simply click anywhere on the line and then click the Run button. You can also execute a block of code by highlighting it and clicking Run. For practice, type these two lines into the Script window, save the script as ‘Workshop_script.R’. Then click each line and Run from the icon. After that, highlight both lines and click Run. head(iris) plot(iris) Copy file into project folder: To integrate the dataset into your project, download it from GitHub and save it as ‘iris.csv’. 1.3 Data frames contain rows and columns: the iris flower dataset In 1936, Edgar Anderson collected data on [iris flowers] to quantify the geographic variations (https://en.wikipedia.org/wiki/iris_flower_data_set). The dataset includes 50 samples from each of the three sub-species ( iris setosa, iris virginica, and iris versicolor), with measurements in centimeters (cm) for lengths and widths of sepals and petals. This is one of many built-in datasets in R. You can also download the dataset from GitHub. Figure 1.2: iris flower. Photo from Wikipedia. If you are using Rstudio Cloud through a web browser, remember that it operates on cloud-based servers. This means you cannot directly access local files, and must first upload them using the Upload button in the Files tab of the lower right panel. To import the dataset into R, go to Import Dataset in the File menu, then select “From Text(base)…”. The default settings should suffice; just click Import in the pop-up window. Thankfully, importing data in R isn’t a lengthy process like in some other software(says SAS). If you have trouble with importing, skip this step as this dataset is already included in R. Figure 1.3: Example of a data frame. Let’s open the dataset in Excel for a quick review. Think about what distinguishes the three species. Imagine you find an iris flower with sepals of 6.5cm long by 3.0cm wide, and petals of 6.2cm long by 2.2cm wide. Which species do you think it belongs to? Think (!) for a few minutes while eyeballing the data in Excel. Write down your guess about the species. Getting familiar with this dataset is crucial for this chapter and those that follow. To answer these questions, let’s visualize and analyze the data with R. Type the commands, omit those marked by “#”. View(iris) # View as a spreadsheet head(iris) # Display the first few rows ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa class(iris) # Reveal the data type ## [1] &quot;data.frame&quot; str(iris) # Expose the structure ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The imported data is stored in a data frame named iris, which contains information for 150 observations (flowers) across 5 variables. The first 4 variables/columns, Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width, are numeric. The 5th variable/column, Species, consists of character strings indicating the sub-species of each sample. It’s important to note that numeric and character data are treated differently in R. Sometimes we need to overwrite data types guessed by R, which can be done during the data importing process. If you are using the built-in iris data, the variable/column Species is a factor with three levels. summary(iris) # Provide summary statistics ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## The summary function offers insights into the distribution of each variables, highlighting that there are 50 observations for each of the three subspecies. Individual values in a data frame can be accessed using row and column indices. iris[3, 4] # Value at row 3 and column 4 iris[3, ] # All values in row 3 iris[, 4] # All values in column 4 iris[3, 1:4] # Values in row 3 for columns 1 to 4 To do the exercises, you need to start with a blank Word document. Copy-and-paste the R code and corresponding outputs into the document, then save it as both a Word and a PDF file. Submit these files to the Dropbox. ::: {.exercise #unnamed-chunk-9} Type data() in the R Console window. The pop-up window - “R data sets” - contains all built-in data sets in package datasets. Choose a data set whose structure is data frame, then answer the following questions: Display the first few rows of the data set. NOT the entire set. Show the dimensions of the data set. Extract a subset containing values from rows 2 to 5 and columns 1 to 4. Choose another data set if yours has fewer rows or columns. ::: Feel free to replace the iris data set with any of the built-in data sets as long as it’s a data frame. Make sure to check the type of the data set using class() or str() function. Note that you will need to change the variable names and row/column ranges accordingly. colnames(iris) # Display column names ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Memorize these column names; they are essential for our analysis. We can retrieve an entire column by using the data frame name followed by the column name. For example, sepal length information is under the column Sepal.Length. Be mindful that R is case-sensitive; a term like “iris$petal.length” will not be recognized. iris$Petal.Length # Display 150 petal lengths To minimize typing, let’s store the data in a new vector named PL. PL &lt;- iris$Petal.Length # Store Petal.Length values in a new vector Someone might complain that nothing happens after typing in this command. Actually, you have already created a new vector called PL in the memory. You can find it in the Environment tab in the top right panel. Programming mainly involves creating and modifying data objects in memory. Like withdrawing money saved in a bank, these data can be retrieved and used. PL # Output the 150 numbers stored in PL ## [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3 1.4 ## [19] 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4 1.5 1.2 ## [37] 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7 4.5 4.9 4.0 ## [55] 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1 4.5 3.9 4.8 4.0 ## [73] 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5 4.5 4.7 4.4 4.1 4.0 ## [91] 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1 5.9 5.6 5.8 6.6 4.5 6.3 ## [109] 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9 5.0 5.7 4.9 6.7 4.9 5.7 6.0 ## [127] 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 ## [145] 5.7 5.2 5.0 5.2 5.4 5.1 mean(PL) # Calculate the mean. Pretty straightforward! ## [1] 3.758 ::: {.exercise #unnamed-chunk-15} Calculate the mean of the sepal length in the data set iris. Find the mean of speed in the built-in data set cars. ::: The best way to discover and learn R functions is through a Google search. The R community is incredibly supportive. Websites like StackOverflow.com are treasure troves for code examples and answers to even the most basic questions. ::: {.exercise #unnamed-chunk-16} Look up “R square root function” on Google, then compute the value of \\(\\sqrt{56.7}\\). Find the maximum value of the variable mpg in the data set mtcars. ::: 1.4 Analyzing one set of numbers A vector in R holds a series of numbers or characters, and R’s diverse functions make it simple to perform calculations (like addition, subtraction, multiplication, etc.) on these elements, either individually or collectively. summary(PL) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 The summary function provides a detailed overview of the petal length distribution: The minimum is 1.000, while the maximum reaches 6.900. The average petal length stands at 3.758. The mid-point, or median, at 4.350, indicates that half the values are smaller than this. Ever wonder why the median and mean differ? Imagine if there’s a typo and a petal length is erroneously entered as 340cm instead of 3.40cm. That would skew things quite a bit! The 3rd quartile, or 75th percentile, is 5.100, meaning 75% of the petals are shorter than this. Conversely, a student ranked 5th in a class of 25 would be in the 80th percentile. The minimum is the 0th percentile and the maximum is the 100th percentile. The 1st quartile, or 25th percentile, is at 1.600, indicating that Only 25% of the flowers have petals shorter than this. These summary statistics are visually represented in a boxplot (Figure 1.4A). Boxplots are particularly useful when comparing multiple data sets. boxplot(PL) # Boxplot of Petal Length. boxplot(iris[, 1:4]) # Boxplots of four columns of iris. Figure 1.4: Boxplot of petal length (A) and of all 4 columns (B). In RStudio, you can copy a plot to the clipboard using the Export button on top of the plot area. Or you can click zoom, right-click on the pop-up plot, and select “Copy Image”. Then you can paste the plot into Word. ::: {.exercise #unnamed-chunk-18} Check the data structure of the built-in data set mtcars. Create a boxplot of Mile Per Gallon (mpg) in mtcars. ::: To quantify the variance, we compute the standard deviation σ as follows: \\[\\begin{align} σ=\\sqrt{\\frac{1}{N-1}[(x_{1}-\\bar x)^2+(x_{2}-\\bar x)^2+...+(x_{N}-\\bar x)^2]} \\end{align}\\] where N is the sample size and \\(\\bar{x}\\) is the mean: \\[\\begin{align} \\bar x=\\frac{1}{N}(x_{1}+x_{2}+\\cdots+x_{N}) \\end{align}\\] A small standard deviation implies that the measurements cluster closely around the mean. sd(PL) # Standard deviation of Petal Length ## [1] 1.765298 SW &lt;- iris$Sepal.Width sd(SW) # Standard deviation of Sepal Width ## [1] 0.4358663 The results suggest that these flowers have similar sepal width but exhibit significant variation in petal length, as indicated by the boxplot above. This variation might be an adaptation survival in different habitats. With R, it is straightforward to generate graphs. barplot(PL) # Bar plot of Petal Length Figure 1.5: Barplot of petal length. This bar plot suggests that the first 50 flowers (iris setosa) have much shorter petals than the other two species, and the last 50 flowers (iris virginica) have slightly longer petals than iris versicolor. plot(PL) # Scatter plot hist(PL) # Histogram lag.plot(PL) # Lag plot qqnorm(PL) # Q-Q plot for normal distribution qqline(PL) #Add the regression line Figure 1.6: Scatter plot, histogram, lag plot and normal Q-Q plot. In the scatter plot, three distinct groups are apparent, with one group having much smaller petal lengths. Histogram shows the distribution of data by plotting the frequency of data in bins. Figure 1.6 reveals that there are more flowers with Petal Length between 1 cm and 1.5 cm. It also indicates that the data does not show a bell-curved distribution. The lag plot is a scatter plot against the same set of number with an offset of 1. Any structure in a lag plot indicates non-randomness in the order in which the data is presented. We can clearly see three clusters, indicating that values are centered around three levels sequentially. Q-Q (quantile-quantile) plots are used to check if data follows a normal distribution, a pre-requisite for many statistical methods. Quantiles of the data are compared against those in a normal distribution. If the data points on a Q-Q plot form a straight line, the data likely adheres to a normal distribution. See Figure 1.7 for an example. ::: {.exercise #unnamed-chunk-21} Run x = rnorm(500) to generate 500 random numbers following the Standard Normal distribution. Generate scatter plot, histogram, lag plot, and Q-Q plot of these numbers. Your plots should resemble those in Figure 1.7. ::: Figure 1.7: Plots for randomly generated numbers following a normal distribution. ::: {.exercise #unnamed-chunk-22} Create scatter plot, histogram, lag plot, and Q-Q plot for the Septal Length in the iris dataset. ::: A one-sample t-test is ideal for testing the mean of a normally distributed data. Suppose the pental length of the species setosa, the first 50 observations of PL, follows a normal distribution. We aim to test if its mean is different from 1.5 cm at the significance level of \\(\\alpha = 0.05\\). Here is the code to perform this test: t.test(PL[1:50], mu = 1.5) # Two-sided t-test for the mean petal length of setosa ## ## One Sample t-test ## ## data: PL[1:50] ## t = -1.5472, df = 49, p-value = 0.1282 ## alternative hypothesis: true mean is not equal to 1.5 ## 95 percent confidence interval: ## 1.412645 1.511355 ## sample estimates: ## mean of x ## 1.462 In this case, our null hypothesis is that the mean petal length of setosa is 1.5 cm. Since p-value 0.1282 is greater than the significance level 0.05, there is no strong evidence to reject the null hypothesis. This function also provides the 95% confidence interval for the mean. Based on our sample of the 50 observations, we are 95% confident that the actual mean petal length of setosa (if we were to measure all setosa in the world) lies between 1.412645 cm and 1.511355 cm. ::: {.exercise #unnamed-chunk-24} Compute the 95% confidence interval for the sepal length of setosa. ::: Hypothesis testing can also determine whether a set of numbers is derived from normal distribution. It’s crucial to clearly state the null hypothesis when interpreting results. Here the null hypothesis is that the data comes from a normal distribution. # Normality test shapiro.test(PL) ## ## Shapiro-Wilk normality test ## ## data: PL ## W = 0.87627, p-value = 7.412e-10 Given the test statistic of 0.87627, the chance that the petal length is normally distributed is only 7.412×10-10. In other words, it is highly unlikely that the petal length follows a normal distribution. We reject the normal distribution hypothesis, corroborating our earlier plots. # Normality test of the first 50 observations of PL shapiro.test(PL[1:50]) ## ## Shapiro-Wilk normality test ## ## data: PL[1:50] ## W = 0.95498, p-value = 0.05481 With a p-value of 0.05481, slightly above the threshold of \\(\\alpha=0.05\\), we fail to reject the null hypothesis. This indicates that we don’t have sufficient evidence to refute the claim that the petal length of setosa follows a normal distribution. In statistics, combining graphical insights with statistical models leads to well-rounded conclusions. ::: {.exercise #unnamed-chunk-27} Conduct Shapiro’s test on sepal width. Does it follow a normal distribution given the significant level \\(\\alpha = 0.05\\)? Generate histogram and Q-Q plot for sepal width. Do these plots show a normal distribution? ::: 1.5 Analyzing a categorical variable In the iris dataset, the species information in the last column is what we call a categorical variable. Bar and Pie charts are excellent for displaying proportions. In this case, they display equal representation of the three species, with 50 observations each. counts &lt;- table(iris$Species) # Tabulate the frequencies counts ## ## setosa versicolor virginica ## 50 50 50 pie(counts) # Create a pie chart barplot(counts) # Generate a bar plot Figure 1.8: Frequencies of categorical values visualized by Pie chart (A) and bar chart (B). 1.6 The relationship between two numerical variables Scatter plot is very effective in visualizing the correlation between two numerical variables. PW &lt;- iris$Petal.Width # Shortcut for petal width plot(PW, PL) # Scatter plot of petal width vs petal length Figure 1.9: Scatter plot of petal width and petal length. Figure 1.9 shows that there is a positive correlation between petal length and width. This implies that flowers with longer petals are also wider, suggesting a proportional increase in both dimensions. Another unusual feature is that there seem to be two distinct clusters. Do the points in the small cluster represent one particular species of iris? To explore this, we can color-code the species in the plot. The resultant Figure 1.10 clearly demonstrates that one particular species, setosa forms the smaller cluster in the low left corner. The other two species, while not as distinctly separated, also exhibit a notable differences. This insight is crucial for understanding the dataset. SP &lt;- as.factor(iris$Species) plot(PW, PL, col = SP) # Colors based on Species legend(&quot;topleft&quot;, levels(SP), fill = 1:3) # Add legends on top left. Figure 1.10: Scatter plot showing the correlation of petal width and petal length. To enable species-based color coding, we defined a new variable SP by converting the last column into a factor. The function str(SP) shows that the SP is a factor with 3 levels, internally coded as 1, 2, and 3, which we use for color coding. This method of base plotting in R has been simplified by modern systems such as ggplot2, which we will discuss later. So it’s not necessary to remember any of these steps. str(SP) # Display the structure of Species. ## Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The variation in petal length might be an adaptation to different environments. For instance, setosa, with the smallest petals, is typically found in Arctic regions. versicolor is often found in the Eastern United States and Eastern Canada. While virginica is common along the coastal plain from Florida to Georgia in the Southeastern United States–Wikipedia.” It appears that the iris flowers in warmer places are much larger than those in colder ones. R makes graph creation easy, but interpreting these graphs in context still requires thoughtful analysis. ::: {.exercise #unnamed-chunk-30} Explore the mtcars dataset available at: https://stat.ethz.ch/R-manual/R-patched/RHOME/library/datasets/html/mtcars.html. Create a scatter plot to analyze the correlation between Miles/(US) gallon and Displacement (cu.in.). Since the type of cyl is numeric, you will need to use function newcyl -&gt; as.factor(cyl) to convert it into factor. Replace all ‘mtcars$cyl’ with ‘newcyl’. Color-code the scatter plot by the Number of cylinders; Include a legend to the top right corner. ::: We can quantitatively measure the strength of a correlation using metrics like Pearson’s correlation coefficient, which ranges from -1 to 1. A value of zero signifies no linear correlation. cor(PW, PL) ## [1] 0.9628654 This outcome indicates a strong, positive correlation between petal width and petal length. To visually represent this on a plot, we can add a text annotation: text(1.5, 1.5, paste(&quot;R=0.96&quot;)) A correlation test can further assess the statistical significance of the correlation: cor.test(PW, PL) ## ## Pearson&#39;s product-moment correlation ## ## data: PW and PL ## t = 43.387, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9490525 0.9729853 ## sample estimates: ## cor ## 0.9628654 This test allows us to reject the null hypothesis that the true correlation is zero (no correlation), which indicates that the correlation is statistically significant. It’s worth noting that Pearson’s coefficient can be influenced by outliers, and alternatives like Spearman’s coefficient may be used for more robust analysis. For more information, consult the help documentation: ?cor # Display help information for the cor() function Regression analysis can further reveal the relationship between petal length and width using an equation. We consider a linear model: \\(Petal.Length = \\alpha × Petal.Width + c + \\epsilon\\), where \\(\\alpha\\) is the slope parameter, \\(c\\) is a constant, and \\(\\epsilon\\) represents random error. This model can be established using a least squared-error method: model &lt;- lm(PL ~ PW) # Linear Model (lm) summary(model) # Display model details ## ## Call: ## lm(formula = PL ~ PW) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.33542 -0.30347 -0.02955 0.25776 1.39453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## PW 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4782 on 148 degrees of freedom ## Multiple R-squared: 0.9271, Adjusted R-squared: 0.9266 ## F-statistic: 1882 on 1 and 148 DF, p-value: &lt; 2.2e-16 In the regression model, the tilde(~) links a response variable on the left with one or more independent variables on the right side. We are defining a statistical model where PL is modeled as a function of PW. Our analysis estimates \\(\\alpha=2.22944\\) and \\(c=1.08356\\), with both parameters significantly different from zero (p-values&lt;2×10-16) in both cases. Consequently, we can reliably predict the petal length with the equation: \\(Petal.Length = 2.22944 × Petal.Width + 1.08356\\). Formulas like this are also used in plots. This model can be visualized on a scatter plot as a regression line: abline(model) # Add the regression line to plot To analyze the model’s accuracy and assumptions, diagnostic plots are useful: plot(model) # Diagnostic plots for regression Regression analysis is a powerful tool for examining the significance of relationships between variables. ::: {.exercise #unnamed-chunk-38} Investigate the relationship between sepal length and sepal width using scatter plots, correlation coefficients, test of correlation, and linear regression. Again interpret all your results in clear and straightforward English. ::: 1.7 Testing the differences between two groups In research, we often encounter questions like “Are boys taller than girls of the same age?” To address such queries, we analyze a random sample from a population to determine if observed differences between two groups reflect actual population differences or are merely due to random sampling errors. Species &lt;- iris$Species # Compare distributions of petal length for the three species via boxplot boxplot(PL ~ Species) Figure 1.11: Boxplot of petal length, grouped by species. Here, R automatically splits the dataset into three groups based on species and creates a boxplot for each. The boxplot clearly shows that Setosa has much shorter petals. But are the differences between versicolor and virginica statistically significant? Despite our limited sample size of 50 flowers per species, we seek to draw broader conclusions. We could measure all the iris flowers across the world or use statistical inference. Let’s extract corresponding data. PL1 &lt;- PL[51:100] # Petal Length of versicolor PL1 PL2 &lt;- PL[101:150] # Petal length of virginica PL2 boxplot(PL1, PL2) # Boxplot of the two groups t.test(PL1, PL2) # Two-sample t-test ## ## Welch Two Sample t-test ## ## data: PL1 and PL2 ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean of x mean of y ## 4.260 5.552 In this two sample t-test, our null hypothesis states that the mean petal length for versicolor is the same as for virginica. The small p-value of 2.2x10-16 indicates that it is extremely unlikely to observe the difference of 1.292cm through random sampling. Hence, we reject this hypothesis and conclude that the means for the two species are significantly different. If we measure all versicolor and virginica flowers in the world and compute their average petal lengths, it is very likely that the two averages are different. Conversely, a p-value larger than a chosen significance level, let’s say \\(\\alpha = 0.05\\), would imply insufficient evidence to reject the hypothesis of equal means. Interestingly, we do not need separate data objects for a t-test. It can be performed directly within the dataframe, with R separateing data points by columns. # Extract observations of species versicolor which are lined at rows from 51 to 150 df &lt;- iris[51:150, ] t.test(Petal.Length ~ Species, data = df) # t-test within the data frame boxplot(Petal.Length ~ Species, data = droplevels(df)) # Boxplot with adjusted species levels ::: {.exercise #unnamed-chunk-43} Use boxplot and t-test to investigate whether there are significant differences in sepal width between versicolor and virginica. Interpret your results. ::: 1.8 Testing differences among multiple groups (ANOVA) As indicated in Figure 1.12, sepal width variation across species is small. We aim to determine if the mean sepal widths for the 3 species are equal. This is a perfect task for Analysis of Variance (ANOVA). SW &lt;- iris$Sepal.Width boxplot(SW ~ SP) Figure 1.12: Boxplot of sepal width across 3 species. summary(aov(SW ~ SP)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## SP 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 With the p-value significantly below 0.05, we reject the null hypothesis, concluding that not all the 3 species share the same mean sepal width. The boxplot in Figure 1.12 suggests that setosa has wider sepals, though ANOVA only confirms that at least two species differ in mean sepal width, this is how the hypothesis was set up. ::: {.exercise #unnamed-chunk-44} Explore if the mean spetal widths are the same across the 3 species using boxplot and ANOVA. ::: Hopefully, this chapter has given you a taste of what it’s like to use R for data visualization and analysis. It’s an interactive and graphical process. If I’ve made this seem overly complex, I apologize; that’s not my intention as an instructor. Many individuals can learn R independently by working on a small dataset of personal interest. The key is to Google everything and keep trying. From Tenor.com "],["visualizing-the-iris-flower-data-set.html", "Chapter 2 Visualizing the iris flower data set 2.1 Basic concepts of R graphics 2.2 Scatter plot matrix 2.3 Star and segment diagrams 2.4 The ggplot2 package is intuitive and powerful 2.5 Other types of plots with ggplot2 2.6 Hierarchical clustering and heat map 2.7 Principal component analysis (PCA) 2.8 Logistic Regression for Binary Outcomes Prediction", " Chapter 2 Visualizing the iris flower data set Learning objectives: Understand basic concepts of R graphics Install and load R packages Create basic plots with ggplot2 Explore various methods to visualize the iris flower dataset 2.1 Basic concepts of R graphics Let’s start with base R graphics. We have two main categories of graphics functions in base R: high-level and low-level. High-level functions are the initiators, starting a new plot and setting the stage for further additions using low-level functions. Here’s a table for a clearer picture: High-level graphics functions that start a new plot. Function Description plot Generic plotting function boxplot boxplot. Works with matrix columns or equations (e.g., boxplot(y ~ x)) hist histogram qqnorm Quantile-quantile (Q-Q) plot to check for normality curve Graph an arithmetic function barplot Barplot mosaicplot Using mosaics to represent the frequencies of tabulated counts. heatmap Using colors to visualize a matrix of numeric values. In contrast, Low-level functions are the collaborators, adding elements to an existing plot without starting afresh, as shown below: Low-level graphics functions that add elements to an existing plot. Function Description points Add points to a plot lines Draw lines on a plot abline Plot a straight line segments Add line segments text Insert text into a plot legend Append a legend to a plot arrows Add arrows to a plot Another crucial distinction about data visualization is between plain, exploratory plots and refined, annotated ones. Both types are essential. To get some sense of what the data looks like, Exploratory Data Analysis (EDA) often involves generating numerous “plain” graphics with default settings. R is a very powerful EDA tool. When certain aspects of the data catch our eye, we may aim for more polished graphics suitable for publications or presentations. This often involves additional coding to tweak various parameters. For many, including myself, this process is a mix of Google searches, example code snippets, and a healthy dose of trial and error. One of the open secrets of R programming is that you can start from a plain figure and refine it step by step. Here is an example using the base R graphics, which produces a basic scatter plot with the petal length on the x-axis and petal width on the y-axis. Since iris is a data frame, we will use the iris$Petal.Length to refer to the Petal.Length column. PL &lt;- iris$Petal.Length PW &lt;- iris$Petal.Width plot(PL, PW) To change the type of symbols: plot(PL, PW, pch = 2) # pch = 2 means triangle symbols The pch parameter ranges from 0 to 25, with each number representing a different symbol. Figure 2.1 illustrates commonly used values and symbols. Figure 2.1: Points Symbols To explore more customization options for the plot function, consult the R documentation: ? plot Altering data point colors is effortless with the col = parameter. plot(PL, PW, pch = 2, col = &quot;green&quot;) # Change the symbol color to green Next, Let’s use different symbols for different species. Begin by extracting the species information. iris$Species ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica This output shows that the 150 observations are classed into three species: setosa, versicolor, and virginica. If reading the iris data from a file, like what we did in Chapter 1, it’s necessary to convert the Species column into a factor from character. iris$Species &lt;- as.factor(iris$Species) str(iris$Species) ## Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... factors are used to store categorical variables as levels. Once converted into a factor, each observation is represented by one of the three species. To numerically plot these factors, use the as.numeric function. Since we do not want to change the data frame, we will define a new variable called speciesID. speciesID &lt;- as.numeric(iris$Species) speciesID ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 ## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [149] 3 3 Now, assign different markers to different species using pch = speciesID. plot(PL, PW, pch = speciesID, col = &quot;green&quot;) Here, 150 integers stored in the speciesID factor are used to alter marker types. The first 50 data points (setosa) are represented by open circles (pch = 1), the next 50 (versicolor) by triangles (pch = 2), while the last 50 (virginica) by crosses (pch = 3). Similarly, different colors can be assigned to each species. # Red, green, and blue for *setosa*, *versicolor*, and *virginica* respectively plot(PL, PW, pch = speciesID, col = speciesID) Our figure is starting to look nice, with each iris species distinctly marked by color and shape. This might seem like overkill, but it’s incredibly helpful, especially considering that some folks are colorblind. Let’s enhance our plot further by adjusting labels and adding a title. plot(PL, PW, # x and y pch = speciesID, # Symbol type col = speciesID, # Color xlab = &quot;Petal length (cm)&quot;, # x label ylab = &quot;Petal width (cm)&quot;, # y label main = &quot;Petal width vs. length&quot; # Chart title ) Notice the multi-line structure of this chunk? Breaking long lines into shorter ones enhances clarity and readability. And fear not when typing in the Console window, R knows that you are not done, and waits patiently for your closing parenthesis. Remember: indentation with two spaces and ending with a right parenthesis are key for reusable and understandable code, for others or your future self. Check out these style guides for more tips: breif and detailed. It’s like knowing the dress code for a fancy event. A true perfectionist never settles. Spot the strong linear correlation between petal length and width? Let’s add a trend line using abline(), a low level graphics function. abline(lm(PW ~ PL)) # The order is reversed as we need y ~ x. Here, lm(PW ~ PL) generates a linear model (lm) of petal width as a function of petal length. y ~ x is formula notation that used in many different situations. The linear regression model is used to plot the trend line. Next, we calculate the Pearson’s correlation coefficient and mark it on the plot. PCC &lt;- cor(PW, PL) # Pearson&#39;s correlation coefficient PCC &lt;- round(PCC, 2) # Round to the 2nd place after decimal point. paste(&quot;R =&quot;, PCC) ## [1] &quot;R = 0.96&quot; The paste function glues two strings together. Then we use the text function to place the merged string in the lower right corner by specifying the coordinate of (x=5, y=0.5). text(5, 0.5, paste(&quot;R=&quot;, PCC)) # Add text annotation legend(&quot;topleft&quot;, # Specify the location of the legend levels(iris$Species), # Specify the levels of species pch = 1:3, # Symbol parade col = 1:3 # Color fiesta ) Figure 2.2: A refined scatter plot using base R graphics. The last expression adds a legend at the top left using the legend function. While plot is a high-level graphics function that starts a new plot, abline, text, and legend are low-level functions that add to our existing plot. Now you should be proud of yourself if you are able to generate such a plot. This is how we create complex plots step-by-step with trial-and-error. Creating such intricate plots is an exercise in patience and creativity. But let’s not forget another open secret of coding: “borrowing” ideas and code is totally fine! The R Graphics Cookbook includes all kinds of R plots and sample code. And websites? Check out website: http://www.r-graph-gallery.com/ for more than 200 examples or dive into another collection at here: http://bxhorn.com/r-graphics-gallery/. These resources are goldmines when you have a graph in mind and just need the right template to tweak. 2.2 Scatter plot matrix While data frames can have a mixture of numbers and characters in different columns, matrices typically contain only numbers. Let’s extract the first 4 columns from the data frame iris and convert them into a matrix: ma &lt;- as.matrix(iris[, 1:4]) # Convert to matrix colMeans(ma) # Calculate column means ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 colSums(ma) # Calculate column sums ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 876.5 458.6 563.7 179.9 This technique can also be applied to rows using rowMeans(x) and rowSums(x). Next up, let’s create a scatter plot matrix using pairs() function. pairs(ma) For a more colorful representation: pairs(ma, col = rainbow(3)[speciesID]) # Set colors by species Figure 2.3: Scatter plot matrix ::: {.exercise #unnamed-chunk-60} Interpret the scatter plot matrix in Figure 2.3. :::   ::: {.exercise #unnamed-chunk-61} Plot a scatter plot matrix for dataset mtcars, utilizing different point symbols and colors based on the types of cyl. ::: 2.3 Star and segment diagrams Star plot, also known as Radar chart, uses stars to visualize multidimensional data. It is a useful way to display multivariate observations with an arbitrary number of variables. Each observation is represented as a star-shaped figure with one ray for each variable. For a given observation, the length of each ray is made proportional to the size of that variable. The star plot was firstly used by Georg von Mayr in 1877! df &lt;- iris[, 1:4] stars(df) # Do I see any diamonds? stars(df, key.loc = c(17, 0)) # What does this tell you? ::: {.exercise #unnamed-chunk-63} Based on the star plot, What insights can you gather about the differences among the three iris species? ::: The stars() function can also be used to generate segment diagrams, where each variable is used to generate colorful segments. The sizes of the segments are proportional to the measurements. stars(df, key.loc = c(20, 0.5), draw.segments = TRUE) Figure 2.4: Star plots and segments diagrams. ::: {.exercise #unnamed-chunk-65} Produce the segments diagram for the state data (state.x77) and offer interpretation of the results, especially in relation to South Dakota compared to other states. Hints: Convert the matrix to a data frame using df.state.x77 &lt;- as.data.frame(state.x77), then plot the segments diagram based on the dataset df.state.x77. ::: 2.4 The ggplot2 package is intuitive and powerful In our R graphics toolkit, we have not just the base R graphics, but also the lattice and ggplot2 packages. These are the three musketeers of R graphics! Lattice builds upon base R for multi-faceted graphs, while ggplot2, developed on the principles of the Grammar of Graphics(hence the “gg”), adds layers to create complex graphics. Renowned for its intuitive design and powerful features, ggplot2 is becoming increasingly popular. ggplot2’s magic lies in its modular approach, enabling step-by-step refinement of charts through: Aesthetic mapping:Linking variables to visual characteristics. Geometric shapes: Crafting the visuals (points, lines, boxes, bars, histograms, maps, etc) Scales and statistical transformations: Adjusting scales, applying transformations like log, reverse, count, etc. coordinate systems: Framing the plot space. facets: Creating multiple plots for comparative analysis. Labels and annotations: Adding textual elements for clarity. For a deeper dive, check out these comprehensive tutorials on ggplot2 here and by its author. Not part of the base R distribution, ggplot2 is available on the CRAN website. It need to be downloaded and installed. One of the main advantages of R is that it is open, and users can contribute their codes as packages. If you are using RStudio, you can choose Tools-&gt;Install packages from the main menu, and then enter the name of the package. If you are using R software, you can install additional packages, by clicking Packages in the main menu, and select a mirror site. All these mirror sites work the same, but some may be faster. After choosing a mirror and clicking “OK”, you can scroll down the long list to find your package. Alternatively, you can type this command to install packages. # Install the package. You can also do it through the Packages Tab install.packages(&quot;ggplot2&quot;) Installation of a new package is a one-time affair, but remember, you need to load it from your hard drive into memory every time you use its functions or data. library(ggplot2) # Load the ggplot2 package Let’s walk through the steps of creating a scatter plot using ggplot2, adding layers one at a time. # Start with the canvas ggplot(data = iris) # Map data to x and y coordinates ggplot(data = iris) + aes(x = Petal.Length, y = Petal.Width) Figure 2.5: Basic scatter plot using the ggplot2 package. # Add data points ggplot(data = iris) + aes(x = Petal.Length, y = Petal.Width) + geom_point() Figure 2.6: Basic scatter plot using the ggplot2 package. # Customize color &amp; symbol type ggplot(data = iris) + aes(x = Petal.Length, y = Petal.Width) + geom_point(aes(color = Species, shape = Species)) Figure 2.7: Basic scatter plot using the ggplot2 package. # Incorporate a trend line ggplot(data = iris) + aes(x = Petal.Length, y = Petal.Width) + geom_point(aes(color = Species, shape = Species)) + geom_smooth(method = lm) Figure 2.8: Basic scatter plot using the ggplot2 package. # Annotate with text to a specified location by setting coordinates x = , y = ggplot(data = iris) + aes(x = Petal.Length, y = Petal.Width) + geom_point(aes(color = Species, shape = Species)) + geom_smooth(method = lm) + annotate(&quot;text&quot;, x = 5, y = 0.5, label = &quot;R=0.96&quot;) Figure 2.9: Basic scatter plot using the ggplot2 package. ggplot2 commands often span multiple lines. Each command is like a brushstroke, gradually building up your visual masterpiece. The ‘+’ sign is a cue of the addition of another layer to your plot. ggplot2 takes care of many details, simplifying the process of creating complex plots like Figure 2.2. ggplot(iris) + aes(x = Petal.Length, y = Petal.Width) + # Define the space geom_point(aes(color = Species, shape = Species)) + # Add points geom_smooth(method = lm) + # Add trend line annotate(&quot;text&quot;, x = 5, y = 0.5, label = &quot;R=0.96&quot;) + # Annotate with text xlab(&quot;Petal length (cm)&quot;) + # X-axis labels ylab(&quot;Petal width (cm)&quot;) + # Y-axis labels ggtitle(&quot;Correlation between petal length and width&quot;) # Title ## `geom_smooth()` using formula = &#39;y ~ x&#39; Creating a plot in ggplot2 resembles painting: startING with a blank canvas, gradually sketching the background, outlining the main features, and finally adding the detailed touches. Imagine yourself as an artist, with data as your palette. ::: {.exercise #unnamed-chunk-71} Create a scatter plot of sepal length vs sepal width, differentiate species by colors and shapes, and include a trend line. ::: 2.5 Other types of plots with ggplot2 Creating box plots, a fundamental data visualization tool, is straightforward with ggplot2. For instance, examining sepal length across different iris species: library(ggplot2) # Load ggplot2 ggplot(data = iris) + aes(x = Species, y = Sepal.Length, color = Species) + geom_boxplot() # Classic box plot Here, the categorical variable ‘Species’ is used on the x-axis to group data. The y-axis displays the distribution of sepal lengths. One of ggplot2’s strengths is its ability to refine plots effortlessly. ggplot(data = iris) + aes(x = Species, y = Sepal.Length, color = Species) + geom_boxplot() + geom_jitter(position = position_jitter(0.2)) # Add jittered raw data points Figure 2.10: Enhancing box plot with raw data points Adding raw data points on top of the boxplot provides a richer view. To avoid cluttering, we jitter these points within each subgroup. The color-coding is simply achieved with “color = Species”. ggplot2 gracefully handles the finer details, like the automatic generation of legends in these density plots. ggplot(data = iris) + aes(x = Petal.Length, fill = Species) + geom_density(alpha = 0.3) # A density plot with a touch of transparency Figure 2.11: Density plot of petal length by species. ggplot(data = iris) + aes(x = Petal.Length, fill = Species) + geom_density(alpha = 0.3) + facet_wrap(~Species, nrow = 3) # Facet for comparison across species Figure 2.12: Density plot by subgroups using facets. ggplot(iris) + aes(x = Species, y = Sepal.Width, fill = Species) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + stat_summary(geom = &quot;errorbar&quot;, fun.data = &quot;mean_se&quot;, width = .3) # Mean and error bar Figure 2.13: Dynamite Plot The bar plot with error bar, as seen in 2.13 is humorously termed as “dynamite plot” due to its appearance. Rafael Irizarry, a renowned statistician, criticized dynamite plot in his blog: “The dynamite plots must die!”. He pointed out dynamite plots’ limitations in conveying detailed data information, obscure the outliers and overall distribution. So many scientists prefer box plots with jittered points for a more comprehensive view. ::: {.exercise #unnamed-chunk-75} Use boxplot and density plots to investigate the similarity and differences of petal width across the three species in the iris dataset. ::: 2.6 Hierarchical clustering and heat map Hierarchical clustering summarizes observations into trees representing the overall similarities. This technique is superb for visualizing how different samples relate to each other. ma &lt;- as.matrix(iris[, 1:4]) # Convert to matrix disMatarix &lt;- dist(ma) # Calculate distance matrix plot(hclust(disMatarix)) # Generate and plot hierarchical clustering tree The dist() function calculates a distance matrix with the default Euclidean distance method. The distance matrix is then used by the hclust() function to generate a hierarchical clustering tree with the default complete linkage method, which is then plotted in a nested command. The 150 samples of flowers are organized in this cluster dendrogram based on their Euclidean distance, which is labeled vertically by the bar to the left side. Highly similar flowers are grouped together in smaller branches, and their distances can be found according to the vertical position of the branching point. We are often more interested in looking at the overall structure of the dendrogram. For example, we see two big clusters. The detailed process of Hierarchical clustering: First, each of the flower samples is treated as a cluster. The algorithm joins the two most similar clusters based on a distance function. This is performed iteratively until there is just a single cluster containing all 150 flowers. At each iteration, the distances between clusters are recalculated according to one of the methods—Single linkage, complete linkage, average linkage, and so on. In the single-linkage method, the distance between two clusters is defined by the smallest distance among the all possible object pairs. This approach puts ‘friends of friends’ into a cluster. On the contrary, the complete linkage method defines the distance as the largest distance between object pairs. It finds similar clusters. Between these two extremes, there are many options in between. The most robust linkage method is the average linkage, which uses the average of all distances. However, the default seems to be the complete linkage. Thus we need to change that in our final version. Heat maps, especially when combined with hierarchical clustering, are fantastic for visualizing data matrices. The rows and columns are reorganized based on hierarchical clustering, and the values in the matrix are coded by colors. Heat maps can directly visualize millions of numbers in one plot. The hierarchical trees also show the similarity among rows and columns. heatmap(ma, scale = &quot;column&quot;, RowSideColors = rainbow(3)[iris$Species] ) Scaling is handled by the scale() function, which subtracts the mean from each column and then divides by the standard division. Afterward, all the columns have the same mean of approximately 0 and standard deviation of 1. This is also called standardization. The default color scheme uses yellow for higher values and red for lower ones. The color bar on the left corresponds to different species. This basic heatmap lacks a legend and could use some polishing. Instead of going down the rabbit hole of adjusting dozens of parameters to basic heatmap function (or it’s enhanced version heatmap.2 from the gplots package), We will refine this plot using another R package called pheatmap. install.packages(&quot;pheatmap&quot;) library(pheatmap) ma &lt;- as.matrix(iris[, 1:4]) # Convert to matrix row.names(ma) &lt;- row.names(iris) # Assign row names in the matrix pheatmap(ma, scale = &quot;column&quot;, clustering_method = &quot;average&quot;, # Use average linkage annotation_row = iris[, 5, drop = FALSE], # Add the 5th column as color bar show_rownames = FALSE ) Figure 2.14: Heatmap for the iris flower dataset. First, we convert the first 4 columns of the iris data frame into a matrix. Then the row names are assigned to be the same, namely, “1” to “150”. This is required because row names are used to match with the column annotation information, specified by the annotation_row parameter. Even though we only need the 5th column, i.e., Species, this has to be a data frame. To prevent R from automatically converting a one-column data frame into a vector, we used drop = FALSE option. Multiple columns can be contained in the column annotation data frame to display multiple color bars. The rows could be annotated the same way. More information about the pheatmap function can be obtained by reading the help document. But most of the times, I rely on the online tutorials. Beyond the official documents prepared by the author, there are many documents created by R users across the world. The R user community is uniquely open and supportive. For instance, when I was researching heatmap.2, a more refined version of heatmap, part of the gplots package, and landed on Dave Tang’s blog, which mentioned that there is a more user-friendly package called pheatmap described in his other blog. Kamil Slowikowski’s blog also provide valuable insights into function pheatmap. Come back to our Figure 2.14. It offers deep insights into the dataset. The 150 flowers in the rows are organized into different clusters. I. Setosa samples obviously formed a unique cluster, characterized by smaller (blue) petal length, petal width, and sepal length. The other two subspecies are not clearly separated but I. Virginica samples form a small subcluster showing bigger petals. The columns are also organized into dendrograms, reaffirming the strong correlation between petal length and width. 2.7 Principal component analysis (PCA) Principal Component Analysis (PCA) is a linear dimension-reduction method, simplifying the complexity in high-dimensional data by reducing its number of dimensions. This section may be skipped if you would like, as it contains more statistics than R programming. However, PCA is crucial for visualizing and interpreting data in a more comprehensible manner. As illustrated in Figure 2.15, PCA transforms the data into a new set of orthogonal coordinates, ranking them based on the variation or information they capture. PCA process results in: Definitions of new coordinates, Percentages of variances captured by these coordinates, Representations of data points on these new coordinates. Here the first component x’ gives a relatively accurate representation of the data. Figure 2.15: Concept of PCA. Let’s take the first 4 columns of the iris data for a PCA spin. Note that “scale = TRUE” normalizes the data before conducting PCA, ensuring each variable has unit variance. pca &lt;- prcomp(iris[, 1:4], scale = TRUE) pca # Display PCA results. ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 The first principal component is positively correlated with Sepal length, petal length, and petal width. Recall that these three variables are highly correlated. Sepal width, which is relatively constant across species, influences PC2 the most, while sepal length influences PC2 the least. plot(pca) # Plot the variance captured by each principal component. str(pca) # Explore PCA object structure ## List of 5 ## $ sdev : num [1:4] 1.708 0.956 0.383 0.144 ## $ rotation: num [1:4, 1:4] 0.521 -0.269 0.58 0.565 -0.377 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## $ center : Named num [1:4] 5.84 3.06 3.76 1.2 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ scale : Named num [1:4] 0.828 0.436 1.765 0.762 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ x : num [1:150, 1:4] -2.26 -2.07 -2.36 -2.29 -2.38 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; head(pca$x) # New coordinate values for the 150 iris samples ## PC1 PC2 PC3 PC4 ## [1,] -2.257141 -0.4784238 0.12727962 0.024087508 ## [2,] -2.074013 0.6718827 0.23382552 0.102662845 ## [3,] -2.356335 0.3407664 -0.04405390 0.028282305 ## [4,] -2.291707 0.5953999 -0.09098530 -0.065735340 ## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870 ## [6,] -2.068701 -1.4842053 -0.02687825 0.006586116 To visualize the PCA results, we first construct a data frame suitable for ggplot2 format. pcaData &lt;- as.data.frame(pca$x[, 1:2]) # Extract first two components pcaData &lt;- cbind(pcaData, iris$Species) # Add species information colnames(pcaData) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Species&quot;) # Rename columns library(ggplot2) ggplot(pcaData) + aes(PC1, PC2, color = Species, shape = Species) + # Define plot area geom_point(size = 2) # Add data points Now we have a basic plot. We can add elements one by one using the “+” sign at the end of each line to add details to this plot. percentVar &lt;- round(100 * summary(pca)$importance[2, 1:2], 0) # Calculate percentage variances ggplot(pcaData, aes(PC1, PC2, color = Species, shape = Species)) + geom_point(size = 2) + # Add data points xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + # x label ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + # y label ggtitle(&quot;Principal component analysis (PCA)&quot;) + # Title theme(aspect.ratio = 1) # Width and height ratio Figure 2.16: PCA plot of the iris flower dataset The plot (Figure 2.16) projects the 4-dimensional iris data onto a 2-dimensional space using the first two principal components. Based on its value, PC1 alone can distinguish the three species quite effectively: PC1 &lt; -1: Iris setosa; PC1 &gt; 1.5: Iris virginica; -1 &lt; PC1 &lt; 1: Iris versicolor. ::: {.exercise #unnamed-chunk-83} Perform PCA on the state.x77 dataset (converted to a data frame) using state.region for color coding. Don’t forget to normalize the data using scale function. Interpret your findings. ::: 2.8 Logistic Regression for Binary Outcomes Prediction Because this section contains more statistics than R programming like PCA, you may skip it too if you would like. PCA helps us see the big picture, which reveals that I. setosa is easy to be distinguished from the other two species based on petal length alone. While logistic regression is a statistical method for predicting binary outcomes, particularly between I. versicolor and I. virginica. Let’s build a model using the odds ratio of being I. virginica as a function of all of the 4 measurements: \\[ln(odds)=ln(\\frac{p}{1-p}) =a×Sepal.Length + b×Sepal.Width + c×Petal.Length + d×Petal.Width+c+e.\\] df &lt;- iris[51:150, ] # ocus on versicolor and virginica. df &lt;- droplevels(df) # Removes setosa, now an empty levels of species model &lt;- glm(Species ~ ., # Model: Species as a function of other variables family = binomial(link = &quot;logit&quot;), data = df ) summary(model) ## ## Call: ## glm(formula = Species ~ ., family = binomial(link = &quot;logit&quot;), ## data = df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -42.638 25.707 -1.659 0.0972 . ## Sepal.Length -2.465 2.394 -1.030 0.3032 ## Sepal.Width -6.681 4.480 -1.491 0.1359 ## Petal.Length 9.429 4.737 1.991 0.0465 * ## Petal.Width 18.286 9.743 1.877 0.0605 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.629 on 99 degrees of freedom ## Residual deviance: 11.899 on 95 degrees of freedom ## AIC: 21.899 ## ## Number of Fisher Scoring iterations: 10 Interestingly, sepal measurements are not very helpful in distinguishing versicolor from virginica. Petal length, however, emerges as a significant factor with P=0.0465. One unit increase in petal length will increase the log-odds of being virginica by 9.429. Petal.Width is a marginally significant effect. Don’t worry if the mathematics behind logistic regression or logistic regression seems complex for you. The key is learning how to apply these analyses in R and interpret the results. Think R is a tool best learned through practice, much like a computer. I do not understand how computers work, yet I use it every day. You do not need to finish the rest of this book. Now it’s entirely possible to start working on one of your own datasets after reading the first two chapters. If you do not have a dataset, consider sources such as TidyTuesday. Set a research question and start experimenting. Feel free to borrow some example code as needed. ::: {.exercise #unnamed-chunk-85} So far, we used a variety of techniques to investigate the iris flower dataset. Recall that in the very beginning, I asked you to eyeball the data and answer two questions: What distinguishes these three species? If we have a flower with sepals of 6.5cm long and 3.0cm wide, petals of 6.2cm long, and 2.2cm wide, which species does it most likely belong to? Review the raw data and previous analyses, write a paragraph with your conclusions supported by evidence. Feel free to conduct additional analysis if necessary. ::: References: 1 Beckerman, A. (2017). Getting started with r second edition. New York, NY, Oxford University Press. "],["data-structures.html", "Chapter 3 Data structures 3.1 Basic concepts 3.2 Data structures", " Chapter 3 Data structures Learning objectives: Understand basic concepts of R programming, including functions and data objects. Learn to seek help within R and utilize Google for programming queries. Get familiar with common data objects such as scalars, vectors, lists, data frames, and matrices. R scripts are a mix of function calls and data handling. There’s a buffet of data structures in R, including scalars, vectors, factors, matrices, factors, data frames, and lists. These structures can store one or more individual data elements of various types, such as numeric (e.g. 2.5), character (e.g. “Go Jacks”), or logical (e.e. TRUE or FALSE). 3.1 Basic concepts 3.1.1 Expressions In R, just type a command and watch the magic happen! For instance: 1 + 1 ## [1] 2 Here’s your result, ‘2’. It’s printed on the console right after your entry. Now enter the string “Go Jacks”. (quotes are a must!) &quot;Go Jacks&quot; ## [1] &quot;Go Jacks&quot; ::: {.exercise #unnamed-chunk-88} Multiply 45.6 by 78.9 and see what R computes. ::: 3.1.2 Logical Values R can yield logical values TRUE or FALSE , known as “Boolean” values in many languages. Let’s try an expression that gives us a logical value: 3 &lt; 4 # Result in TRUE ## [1] TRUE And another logical value: 2 + 2 == 5 # Spoiler: It&#39;s FALSE ## [1] FALSE Note that you need a double-equal sign to check whether two values are equal - a single-equal sign won’t work. 3.1.3 Variables You can store values in variables for future use, just like in other languages. Type x = 42 to keep 42 in x. x is a scalar that contains only one data element. x = 42 Alternatively, use this conventional and safer syntax for assignment: x &lt;- 42 This is the recommended way of assignment, according to the Google R style Guide, although x = 42 generally works fine. Once assigned, x is ready for action. Have fun with x to divide, log, square, or use it in a logical operation. x / 2 # Division ## [1] 21 log(x) # Logarithm ## [1] 3.73767 x^2 # Squaring ## [1] 1764 sqrt(x) # Square root ## [1] 6.480741 x &gt; 1 # Logical comparison ## [1] TRUE You can re-assign any value to a variable at any time. Try assigning “Go Jacks!” to x. x &lt;- &quot;Go Jacks!&quot; # Now, x is a cheerleader. To peek into the variable’s value, just type its name: x ## [1] &quot;Go Jacks!&quot; Switching to a logical value? No problem! Assign TRUE to x: x &lt;- TRUE You can store multiple values in a variable or object called vector, or matrix and data frame, which are tables with rows and columns, like an Excel spreadsheet. These will be explained later. 3.1.4 Functions Functions are fundamental building blocks of R. Most of the times when we run R commands, we are calling and executing functions. You can call a function by its name, followed by one or more arguments in parenthesis. Let’s use the sum function to add up a few numbers: sum(1, 3, 5) ## [1] 9 Save a function’s result to a variable: y &lt;- sum(1, 3, 5) y ## [1] 9 Some arguments have “names”. For example, to repeat a value 3 times, you would call the rep function and provide it to times argument: rep(&quot;Yo ho!&quot;, times = 3) # # Three cheers from R! ## [1] &quot;Yo ho!&quot; &quot;Yo ho!&quot; &quot;Yo ho!&quot; ::: {.exercise #unnamed-chunk-100} For vector x &lt;- c(12, 56, 31, -5, 7): a. Calculate the mean of all elements in x and assign it to y. b. Square each element in x and assign the results to a new vector z. ::: ::: {.exercise #unnamed-chunk-101} Use Google to find functions for setting and getting the current working directory, respectively. ::: ::: {.exercise #unnamed-chunk-102} Discover the function that lists all the files in the current working folder. ::: Reusing code efficiently can be achieved by wrapping it in a function, clearly defining the input and the output. Tested and documented R functions are often available as R packages. You can also define your own functions, which will be discussed later. 3.1.5 Seeking help and example Code To find help in R, use the help command. For instance, to learn about the sum function, type: ? sum A helpful document will pop up in the Help Window. See the Figure 3.1. Figure 3.1: Help formation on the ‘sum’ function. Scroll to the bottom of the help page to find example code, as shown in Figure 3.2. Figure 3.2: Code examples using ‘sum’ function. The quickest way to grasp an R function is by playing around with these example codes, observing the inputs and outputs. Feel free to copy, paste, and modify these examples for your analysis needs. example() brings up usage examples for the given function. Try it for the ‘min’ function: example(min) ## ## min&gt; require(stats); require(graphics) ## ## min&gt; min(5:1, pi) #-&gt; one number ## [1] 1 ## ## min&gt; pmin(5:1, pi) #-&gt; 5 numbers ## [1] 3.141593 3.141593 3.000000 2.000000 1.000000 ## ## min&gt; x &lt;- sort(rnorm(100)); cH &lt;- 1.35 ## ## min&gt; pmin(cH, quantile(x)) # no names ## [1] -2.21172654 -0.84536856 0.02474012 0.91307143 1.35000000 ## ## min&gt; pmin(quantile(x), cH) # has names ## 0% 25% 50% 75% 100% ## -2.21172654 -0.84536856 0.02474012 0.91307143 1.35000000 ## ## min&gt; plot(x, pmin(cH, pmax(-cH, x)), type = &quot;b&quot;, main = &quot;Huber&#39;s function&quot;) ## ## min&gt; cut01 &lt;- function(x) pmax(pmin(x, 1), 0) ## ## min&gt; curve( x^2 - 1/4, -1.4, 1.5, col = 2) ## ## min&gt; curve(cut01(x^2 - 1/4), col = &quot;blue&quot;, add = TRUE, n = 500) ## ## min&gt; ## pmax(), pmin() preserve attributes of *first* argument ## min&gt; D &lt;- diag(x = (3:1)/4) ; n0 &lt;- numeric() ## ## min&gt; stopifnot(identical(D, cut01(D) ), ## min+ identical(n0, cut01(n0)), ## min+ identical(n0, cut01(NULL)), ## min+ identical(n0, pmax(3:1, n0, 2)), ## min+ identical(n0, pmax(n0, 4))) Here’s a quick demonstration: min(5:1, pi) # Return a single number ## [1] 1 Example commands and plots will show up automatically by pressing ‘Return’ in RStudio. In R, you’ll need to manually click on the plots to view them. example(boxplot) # Show example of boxplot Google is a surprisingly patient teacher for R-related queries, forgiving typos, grammar errors, and different notations. Most of your questions (99 %) have been asked and answered on various forums. Many R gurus share their wisdom on web sites like** stackoverflow.com**, providing answers with example codes! You can also use Google as a reference. Remember, it is important to add comments to your code. Everything after the “#” will be ignored by R during execution. Since we often recycle and re-purpose our codes, these comments are our breadcrumbs through the forest of code, helping us remember our intentions. max(1, 3, 5) # Return the maximum value of a vector ## [1] 5 3.2 Data structures 3.2.1 Vectors A vector is an object that holds a sequence of values of the same type, such as numbers, strings, logical values, or any other type, as long as they’re all the same type. They can come from a column of a data frame too. Here’s how you create a vector x: x &lt;- c(5, 2, 22, 11, 5) x ## [1] 5 2 22 11 5 The c here stands for concatenate. Treat it with respect and avoid using it as a variable name; it’s much too valuable for that! Vectors can not contain values with different modes (types). Try a motley crew with mixing modes and see what happens: c(1, TRUE, &quot;three&quot;) # Converts all elements to characters ## [1] &quot;1&quot; &quot;TRUE&quot; &quot;three&quot; The result? All values get converted to ‘characters’, the universal diplomat of data types, to coexist peacefully in the vector. To hold diverse types of values, you will need a list, which is explained later in this chapter. For a vector with a sequence of numbers, use start:end notation, which is often used in loops and operations on the indices of vectors, etc. Let’s create a vector with values from 5 through 9: 5:9 ## [1] 5 6 7 8 9 The seq function offers more flexibility for creating sequences. Here is the basic seq(): seq(from = 5, to = 9) ## [1] 5 6 7 8 9 seq also allows you to use increments other than 1. Try it with step of 0.5: seq(from = 5, to = 9, by = .5) ## [1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 Create a sequence from 5 to 9 with length 15: seq(from = 5, to = 9, length = 15) ## [1] 5.000000 5.285714 5.571429 5.857143 6.142857 6.428571 6.714286 7.000000 ## [9] 7.285714 7.571429 7.857143 8.142857 8.428571 8.714286 9.000000 ::: {.exercise #unnamed-chunk-114} Compute 1+2+3… +1000 using single line of R code. Hint: check the example code for sum( ) function in the R help document. ::: 3.2.1.1 Vectors operations First let’s find out the 4th element of our vector x &lt;- c(5, 2, 22, 11, 5), or the elements from 2 to 4. x[4] ## [1] 11 x[2:4] ## [1] 2 22 11 Creat a new vector y from a segment of x: y &lt;- x[2:4] No result is returned but you “captured” the result in a new vector y, which holds 3 numbers. You can type y and hit ‘enter’ to see the results. y # Meet y ## [1] 2 22 11 Or, combine creation and revelation in one magical line. A semicolon separates multiple commands. y &lt;- x[2:4]; y # Voila! ## [1] 2 22 11 Now discover the vector length: length(x) ## [1] 5 It’s also easy to know basic statistics about the vector such as maximum, minimum, sum, mean and median, individually or together. And standard deviation too. max(x) ## [1] 22 min(x) ## [1] 2 sum(x) ## [1] 45 mean(x) ## [1] 9 median(x) ## [1] 5 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2 5 5 9 11 22 sd(x) ## [1] 7.968689 rank() function ranks the elements. It plays fair, giving ties an average rank. sort() orders elements either ascending (from the smallest to the biggest) or descending (decreasing = T will make it sort from the biggest to the smallest). rank(x) ## [1] 2.5 1.0 5.0 4.0 2.5 sort(x) ## [1] 2 5 5 11 22 sort(x, decreasing = T) ## [1] 22 11 5 5 2 diff() iterates the differences between adjacent elements of vector x. diff(x) ## [1] -3 20 -11 -6 rev() will reverse the position of the elements in the vector. rev(x) ## [1] 5 11 22 2 5 Operations are performed element by element, like log, sqrt, x^2, etc. They return vectors too. log(x) ## [1] 1.6094379 0.6931472 3.0910425 2.3978953 1.6094379 sqrt(x) ## [1] 2.236068 1.414214 4.690416 3.316625 2.236068 x^2 ## [1] 25 4 484 121 25 2*x + 1 ## [1] 11 5 45 23 11 Exclude the second element from x and save as y: y &lt;- x[- 2] y ## [1] 5 22 11 5 Add an element 100 to vector x between its second and the third element: x &lt;- c(5, 2, 22, 11, 5) x &lt;- c(x[1:2], 100, x[3:5] ) x # Now it contains the added 100 ## [1] 5 2 100 22 11 5 The length of the new created x is: length(x) ## [1] 6 Use the two commands below to add a new element to the end. They generate the same results. x &lt;- c(5, 2, 22, 11, 5) c(x, 7) ## [1] 5 2 22 11 5 7 append(x, 7) ## [1] 5 2 22 11 5 7 Create an empty vector y: y &lt;- c() y ## NULL length(y) ## [1] 0 Here’s how to find unique elements: x &lt;- c(5, 2, 22, 11, 5) unique(x) ## [1] 5 2 22 11 And the frequencies of the unique elements: x &lt;- c(5, 2, 22, 11, 5) table(x) ## x ## 2 5 11 22 ## 1 2 1 1 To locate the max or min, find a particular value, or satisfy a condition like \\(x^2&gt;100\\): x &lt;- c(5, 2, 22, 11, 5) which.max(x) ## [1] 3 which.min(x) ## [1] 2 which(x == 11) ## [1] 4 which(x^2 &gt; 100) ## [1] 3 4 You can randomly select some elements from the vector. Do you always get the same results if you run the following code more than once? The answer is “No”. Each run is a surprise, like a mini lottery: x &lt;- c(5, 2, 22, 11, 5) sample(x, 3) ## [1] 22 11 2 Elements in the vector can have names. Personalize elements with names and type “x” to see the difference. x &lt;- c(5, 2, 22, 11, 5) names(x) &lt;- c(&quot;David&quot;, &quot;Beck&quot;, &quot;Zach&quot;, &quot;Amy&quot;, &quot;John&quot;) x # Now it&#39;s a who&#39;s who of data! ## David Beck Zach Amy John ## 5 2 22 11 5 You can refer to the elements by their names now. x[&quot;Amy&quot;] # Amy says hi! ## Amy ## 11 The any() and all() functions produce logical values. They return if any or all of their arguments are TRUE. x &lt;- c(5, 2, 22, 11, 5) any(x &lt; 10) ## [1] TRUE any(x &lt; 0) ## [1] FALSE all(x &gt; 0) ## [1] TRUE all(x &lt; 10) ## [1] FALSE Who’s greater than 10 in this vector party? x &gt; 10 # Raise your hand if you&#39;re over 10! ## [1] FALSE FALSE TRUE TRUE FALSE There are multiple methods to get a subset from a vector. Here are some examples: x &lt;- c(NA, 2, -4, NA, 9, -1, 5) x ## [1] NA 2 -4 NA 9 -1 5 y &lt;- x[x &lt; 0] # Extract the rebels (negative values) y ## [1] NA -4 NA -1 Vectors x and y contain annoying NA values, which can be removed using is.na() function. y &lt;- y[!is.na(y)] # Remove all NAs in y. Or equivalently, keep all NOT NAs in y. # The exclamation &#39;!&#39; means NOT. y ## [1] -4 -1 Now the updated variable y contains only numerical values. All NAs have been removed. To assign the “cleaned” vector to a different variable z: z &lt;- y[!is.na(y)] # Assign the new vector without NAs to z. z ## [1] -4 -1 Alternatively, you can use the subset() function to get a “clean” data without NAs. For example: x &lt;- c(NA, 2, -4, NA, 9, -1, 5) y &lt;- subset(x, x &lt; 0) y ## [1] -4 -1 Let’s peek into the function is.na(), which checks if a vector contains NA or not. Note that the result is a vector holding logical values. Do we have missing values in the vector x? is.na(x) ## [1] TRUE FALSE FALSE TRUE FALSE FALSE FALSE In R, NA represents missing or unavailable values, like a placeholder for “I don’t know.” Consider the case where a given value isn’t available in the data set. Unlike simply omitting values, NA explicitly indicates unavailability. This distinction is important for accurate data analysis. Many functions that work with vectors treat these values specially. For instance, the sum of x might not be straightforward due to NAs: sum(x) ## [1] NA The result NA is considered “not available” because one of the vector’s values is NA. R is responsible; It won’t just blithely add up the numbers without warning you about the incomplete data. However, you can explicitly tell sum (and many other functions) to remove NA values before R starts the calculations. Let’s find how sum function handles NAs: ? sum As you see in the documentation, sum has an na.rm parameter to handle NA values. The optional argument na.rm is set to FALSE by default, but if you set it to TRUE, all NAs will be removed from the vector before the calculation is performed. In other words, you can use the argument na.rm = T or na.rm = TRUE to ignore all NAs during calculations. sum(x, na.rm = TRUE) # Calculate a sum excluding NAs ## [1] 11 ::: {.exercise #unnamed-chunk-146} For vector var1 &lt;- c(NA, 334, 566, 319, NA, -307). Create a new vector var2 by removing all NAs from var1. Calculate the mean of var1, ignoring the NAs. ::: To illustrate the difference between NULL and NA, let’s experiment with vec.x: # Build up vec.x with numbers greater than 10 in the vector vec.x &lt;- c(40, 3, 11, 0, 9) z1 &lt;- NULL for (i in vec.x) { if (i &gt; 10) z1 &lt;- c(z1, i) } z1 ## [1] 40 11 length(z1) # How long is z1? ## [1] 2 # Build up vec.x with numbers greater than 10 in the vector vec.x &lt;- c(40, 3, 11, 0, 9) z2 &lt;- NA for (i in vec.x) { if (i &gt; 10) z2 &lt;- c(z2, i) } z2 ## [1] NA 40 11 length(z2) # And z2? ## [1] 3 The length of z1 and z2 reveals that the NULL is counted as nonexistent like the invisible man, while NA is a missing value, taking up space. Vectors in R are like the ingredients in a recipe – mix them right, you’ll create data-analysis delicacies. Let’s peek the operation between a vector and a scalar. # Vector meets a scalar x &lt;- c(1, 4, 8, 9, 10) y &lt;- 1 x + y ## [1] 2 5 9 10 11 As you can see, 1 is added to each element in x. The operation is equivalent adding a vector of ones to x: y &lt;- c(1, 1, 1, 1, 1) x + y ## [1] 2 5 9 10 11 The operation between vectors with the same length is element-wise. For example: # Two vectors with the same length x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2, 0, 3, 15) x + y # Element-wise addition ## [1] 2 6 8 12 25 x * y # Element-wise multiplication, each pair in sync ## [1] 1 8 0 27 150 If vectors have different length, R will automatically recycle the shorter one, until it has the same length as the longer one. For example: x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2) x + y ## [1] 2 6 9 11 11 y was recycled. Here’s what R is really doing: x &lt;- c(1, 4, 8, 9, 10) y &lt;- c(1, 2, 1, 2, 1) x + y ## [1] 2 6 9 11 11 ifelse() function allows you to choose elements conditionally. Its structure is ifelse(test, yes, no). The yes or no depends on whether the test is true or false. Let’s see it in action: x &lt;- c(2, -3, 4, -1, -5, 6) y &lt;- ifelse(x &gt; 0, &#39;Positive&#39;, &#39;Negative&#39;) y ## [1] &quot;Positive&quot; &quot;Negative&quot; &quot;Positive&quot; &quot;Negative&quot; &quot;Negative&quot; &quot;Positive&quot; In this example, the elements in y are either ‘positive’ or ‘negative’, based on whether elements in x are greater or less than 0. Another twist with ifelse(): x &lt;- c(3, 4, -6, 1, -2) y &lt;- ifelse (x &lt; 0, abs(x), 2 * x + 1) y ## [1] 7 9 6 3 2 Here in this case, if an element in x is less than 0 (if x is negative), y takes the absolute value of the element. Otherwise (if x is positive or zero), multiply the element by 2 then add 1 for y. ::: {.exercise #unnamed-chunk-156} Randomly select 10 integers from 1 to 100 using sample selection function. Create a vector y which satisfies the following conditions: if an selected integer is an even number, y is ‘even’, otherwise y returns ‘odd’. ::: To find where elements of one vector appear in another, use the match() function: x &lt;- c(5, 5, 22, 11, 3) y &lt;- c(5, 11, 8) z &lt;- match(y, x) # Where do y&#39;s elements show up in x? z # Unveil the positions ## [1] 1 4 NA The elements 5 and 8 in y locate at the first and fourth position of x respectively. The function ignores the duplicated 5 which locates at the second position of x. The last element 8 in y is not found in x, which returns an “NA”. 3.2.1.2 An example of vector usage Image a fishing Journey where Tom, Jerry, and Mickey went fishing and caught 7, 3, and 9 fishes, respectively, shown in a vector: c(7, 3, 9) ## [1] 7 3 9 The c() function creates a new vector by combining a set of values. Give it a name if we want to use it later: fishes &lt;- c(7, 3, 9) fishes ## [1] 7 3 9 fishes is a vector with 3 elements. Many functions can be used to operate on this vector. Let’s start with a bar plot: barplot(fishes) # See figure 3.3A Compute the total catch: sum(fishes) ## [1] 19 To access the individual elements by indices: fishes[3] ## [1] 9 ::: {.exercise #unnamed-chunk-163} Did Mickey catch more fishes than Tom and Jerry combined? Find out the answer using the fishes vector and return a TRUE or FALSE value. ::: Jerry protested that, per minimum size fishing rules, a ¼ inch long fish he caught and released was not counted in properly. So add one more to Jerry’s catch by changing the value in the 2nd element: fishes[2] &lt;- fishes[2] + 1 fishes ## [1] 7 4 9 Add an 1 to the current value of the 2nd element of 3, the result of 4 is assigned back to the 2nd element itself. As a result, the 2nd element is increased by 1. This is not a math equation, but a value assignment operation. More rigorously, we should write this using &lt;-. We can also directly overwrite the values. fishes[2] &lt;- 4 fishes ## [1] 7 4 9 Now they started a camp fire, and each ate 1 fish for dinner. The left fishes after dinner: fishes2 &lt;- fishes - 1 fishes2 ## [1] 6 3 8 R subtracts 1 from each individual element. Most arithmetic operations work just as well on vectors as they do on single values. If you add a scalar (a single value) to a vector, the scalar will be added to each value in the vector, returning a new vector with the results. Then, something bad happened in the fishing journey: While they are sleeping in their camping site, a fox stole 3 fishes from Jerry’s bucket, and 4 fishes from Mickey’s bucket. How many fishes were left now? stolen &lt;- c(0, 3, 4) # Fishes stolen by a fox fishes2 - stolen # Remaining fishes ## [1] 6 0 4 If you add or subtract two vectors of the same length, R will take the corresponding elements from each vector and add or subtract them. The 0 is necessary to keep the same vector length. Proud of himself, Mickey wanted to make a 5ft x 5ft poster to show himself the best fisherman. Knowing that a picture is worth a thousand words, he learned R and started plotting, trying to put his name on the plots. The data elements in a vector can have names or labels for a more personalized touch: names(fishes) &lt;- c(&quot;Tom&quot;, &quot;Jerry&quot;, &quot;Mickey&quot;) The right side is a vector, holding 3 character values. These values are assigned as the names of the 3 elements in the fishes vector. names() is a built-in function. Visualize the vector: fishes ## Tom Jerry Mickey ## 7 4 9 barplot(fishes) # Bar plot with names. See figure 3.3B Figure 3.3: Simple Bar plot Assigning names for a vector also enables you to use labels to access each element. Try getting the value for Jerry: fishes[&quot;Jerry&quot;] # Jerry&#39;s catch ## Jerry ## 4 ::: {.exercise #unnamed-chunk-171} Assign the value ‘Ten’ to Tom using his name rather than the index in the fishes vector. ::: Tom proposes ambitiously that the goal for the next fishing trip is to double their catches. 2 * fishes ## Tom Jerry Mickey ## 14 8 18 Hopelessly optimistic, Jerry proposed that next time each should “square” their catches, so that together they might feed the entire school. sum(fishes ^ 2) ## [1] 146 Note that two operations are nested. You can obviously do it in two steps. ::: {.exercise #unnamed-chunk-174} Create a vector representing the prices of groceries: bread $2.5, milk $3.1, jam $5.3, beer $9.1. And visualize it with a bat plot. ::: 3.2.1.3 Scatter Plots of two vectors The plot function takes two vectors, one for X values and one for Y values, and creates a scatter plot. Let’s explore the relationship between numbers and their square roots. x &lt;- seq(1, 20, 0.1) y &lt;- sqrt(x) Then simply call plot with the two vectors: plot(x, y) Great job! Notice that values from the first argument (x) forms the horizontal axis, and values from the second (y) takes the vertical stage. ::: {.exercise #unnamed-chunk-177} Bring up a vector “x” with 21 integers from -10 to 10, then create a scatter plot showing x^2 against x. ::: 3.2.2 Matrices A matrix is a two-dimensional vector(rows and columns) with two additional attributes: the number of rows(nrow) and the number of columns(ncol). It can only contain one type of values, i.e. numbers, characters, or logical values. We can create a matrix using rbind or cbind function. rbind combines all rows and cbind combines all columns: m &lt;- rbind(c(3, 4, 5), c(10, 13, 15)) # Combine vectors by row m ## [,1] [,2] [,3] ## [1,] 3 4 5 ## [2,] 10 13 15 n &lt;- cbind(c(3, 4, 5), c(10, 13, 15), c(3, 2, 1)) # Combine vectors by column n ## [,1] [,2] [,3] ## [1,] 3 10 3 ## [2,] 4 13 2 ## [3,] 5 15 1 s &lt;- rbind(m, n) # Combine two matrices m and n by row s ## [,1] [,2] [,3] ## [1,] 3 4 5 ## [2,] 10 13 15 ## [3,] 3 10 3 ## [4,] 4 13 2 ## [5,] 5 15 1 To use rbind() combining matrices by row, the matrices must have the same number of columns. Similar to cbind(), the matrices must have same number of rows to combine by column. The matrix() function is another way to create matrices: x &lt;- matrix(seq(1:12), nrow = 4, ncol = 3) x ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 The argument seq() create a sequence from 1 to 12, nrow() and ncol() define the number of rows and columns in the matrix, respectively. It’s not necessary to define both nrow() and ncol(). Since if one is provided, the other one is inferred from the length of the data. y &lt;- matrix(seq(1:12), nrow = 4) y ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 Notice that the matrix is filled in column-wise by default. Add the byrow = TRUE to the argument to fill a matrix by row-wise: z &lt;- matrix(seq(1:12), nrow = 4, byrow = TRUE) # Fill matrix row-wise z ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 Also, you can generate a matrix by creating an empty matrix first: w &lt;- matrix(nrow = 4, ncol = 3) w ## [,1] [,2] [,3] ## [1,] NA NA NA ## [2,] NA NA NA ## [3,] NA NA NA ## [4,] NA NA NA Then assign values to the empty matrix. For example, assign the value 3 to the position at first row and first column, and value 100 to the position of the second row and third column: w[1,1] &lt;- 3 w[2,3] &lt;- 100 w ## [,1] [,2] [,3] ## [1,] 3 NA NA ## [2,] NA NA 100 ## [3,] NA NA NA ## [4,] NA NA NA A matrix can also be created from a vector by setting its dimensions using function dim(). x &lt;- c(1, 5, 6, 9, 8, 10, 21, 15, 76) x ## [1] 1 5 6 9 8 10 21 15 76 class(x) ## [1] &quot;numeric&quot; dim(x) &lt;- c(3, 3) x ## [,1] [,2] [,3] ## [1,] 1 9 21 ## [2,] 5 8 15 ## [3,] 6 10 76 class(x) ## [1] &quot;matrix&quot; &quot;array&quot; Using as.matrix() function, you can convert a non-matrix data set to a matrix. Take the data iris as an example. subset.iris &lt;- iris[1:10, 1:4] class(subset.iris) ## [1] &quot;data.frame&quot; The data structure of subset.iris is a data frame. Now we turn it into a matrix. x &lt;- as.matrix(subset.iris) # Data frame to matrix class(x) ## [1] &quot;matrix&quot; &quot;array&quot; 3.2.2.1 Matrix operations Matrices support a variety of operations: x &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) x ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 x^2 # Square each element in x ## [,1] [,2] [,3] ## [1,] 1 9 25 ## [2,] 4 16 36 Transpose of x for the convenience of view and analysis: y &lt;- t(x) y ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 x %*% y # Matrix Multiplication ## [,1] [,2] ## [1,] 35 44 ## [2,] 44 56 Subtraction requires matching dimensions: x - y # Matrix subtraction Error in x - y : non-conformable arrays The error reminds us that the matrices for subtraction must have same dimensions. y &lt;- matrix(rep(1, 6), nrow = 2) y ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 x - y # Matrix subtraction now works! ## [,1] [,2] [,3] ## [1,] 0 2 4 ## [2,] 1 3 5 Create a new matrix from calculations by doubling each element and add 5: z &lt;- 2 * x + 5 z ## [,1] [,2] [,3] ## [1,] 7 11 15 ## [2,] 9 13 17 You can also get a logical matrix using logical code. x &lt;- matrix(c(12, 34, 51, 27, 26, 10), ncol = 2) x &gt; 20 # Logical matrix ## [,1] [,2] ## [1,] FALSE TRUE ## [2,] TRUE TRUE ## [3,] TRUE FALSE To extract all TRUE results from x: x[x &gt; 20] ## [1] 34 51 27 26 Similarly, you can define a vector with logical values, then apply it to x to get all TRUE values. log.vau &lt;- c(FALSE, TRUE, TRUE, TRUE, TRUE, FALSE) x[log.vau] # Filter by logical vector ## [1] 34 51 27 26 Remember matrix is a vector, and filled by column-wise. Therefore the vector with logical values applies to x by column-wise order. Since matrices are vectors with two dimensions, all operations for vectors also apply to matrices. For example: x[1, ] # Get the first row of x ## [1] 12 27 a &lt;- as.matrix(iris[, 1:4]) # Take out the first 4 columns of iris and convert it to matrix. c &lt;- a[5:10, 2:4] # Extract a subset c ## Sepal.Width Petal.Length Petal.Width ## [1,] 3.6 1.4 0.2 ## [2,] 3.9 1.7 0.4 ## [3,] 3.4 1.4 0.3 ## [4,] 3.4 1.5 0.2 ## [5,] 2.9 1.4 0.2 ## [6,] 3.1 1.5 0.1 x &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) c[1:2, ] &lt;- x # Replace the first two rows of c with x c ## Sepal.Width Petal.Length Petal.Width ## [1,] 1.0 3.0 5.0 ## [2,] 2.0 4.0 6.0 ## [3,] 3.4 1.4 0.3 ## [4,] 3.4 1.5 0.2 ## [5,] 2.9 1.4 0.2 ## [6,] 3.1 1.5 0.1 To know the mean and sum of these rows and columns, try rowMeans(), colMeans(), rowSums(), colSums(). x &lt;- as.matrix(iris[1:10, 1:4]) rowMeans(x) ## 1 2 3 4 5 6 7 8 9 10 ## 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 colMeans(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 4.86 3.31 1.45 0.22 rowSums(x) ## 1 2 3 4 5 6 7 8 9 10 ## 10.2 9.5 9.4 9.4 10.2 11.4 9.7 10.1 8.9 9.6 colSums(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 48.6 33.1 14.5 2.2 Now use apply() function for more complex calculations, like computing the standard deviations by columns. The second argument “1” or “2” in apply() represents rows or columns that the function applies to. apply(x, 2, sd) # Calculate the standard deviation of each column in x ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.29135698 0.30713732 0.10801234 0.07888106 Or calculate medians by rows, using 1 for rows. apply(x, 1, median) # Calculate the median of each row in x ## 1 2 3 4 5 6 7 8 9 10 ## 2.45 2.20 2.25 2.30 2.50 2.80 2.40 2.45 2.15 2.30 Visualize a large matrix data with a heat map: heatmap(x, scale = &quot;column&quot;, margins = c(10,5)) ::: {.exercise #unnamed-chunk-200} Given subset.iris &lt;- as.matrix(iris[1:10, 1:4]), calculate the column-wise mean of subset.iris using the apply function. ::: 3.2.2.2 Examples of matrices Example 1: Finding Minimal Values Let’s define a function find.min.posi first to find the positions of the minimal value in each column of a matrix, then apply it to subset.iris &lt;- as.matrix(iris[1:10, 1:4]). find.min.posi &lt;- function(x){ y &lt;- function(xcol){ return(which.min(xcol)) } return(apply(x, 2, y)) } subset.iris &lt;- as.matrix(iris[1:10, 1:4]) find.min.posi(subset.iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 9 9 3 10 This code identifies the minimum values in each column and returns their positions. For example, the minimal value for Sepal.Length is 4.4, which locates in the 9th row. The minimal values locate in the 9th, 3rd and 10th row for Sepal.Width, Petal.Length and Petal.Width, respectively. ::: {.exercise #unnamed-chunk-203} Given subset.iris.2 &lt;- as.matrix(iris[, 1:4]), complete the find.max function below to find the maximal value in each column of subset.iris.2. find.max &lt;- function(x){ y &lt;- function(xcol){ return(_________) } return(_______(x, ____, y)) } ________(subset.iris.2) ::: To enhance the readability of matrices, use functions colnames() and rownames() to assign names to the columns and rows: y &lt;- rbind(c(1, 3, 5), c(2, 4, 6)) y ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 colnames(y) &lt;- c(&#39;First.Col&#39;, &#39;Second.Col&#39;, &#39;Third.Col&#39;) row.names(y) &lt;- c(&#39;odd.number&#39;, &#39;even.number&#39;) y # Matrix with named rows and columns ## First.Col Second.Col Third.Col ## odd.number 1 3 5 ## even.number 2 4 6 Another interesting application of the matrix is the manipulation of gray-scale images. You may have heard a pixel matrix which represents “picture elements”. They are small little dots making up images. Each image is a matrix with thousands or even millions of pixels. Each pixel can only be one color at a time, its brightness is represented by a single numeric value. The range of the colors from black to white correspond to the scale varies from 0% to 100%. If the color value 0.2941 is assigned to a pixel which locates 3rd row and 4th column, then the dot at the 3rd row and the 4th column is pretty dark. Sometimes it’s necessary to blur images or add mosaic to a picture for various purposes. See the example of adding a mosaic effect to a gray-scale image. Example 2: Adding mosaic to an image of Einstein First, let’s read and display a gray-scale image of Einstein. library(pixmap) EINSTEIN &lt;- read.pnm(&quot;images/EINSTEIN.pgm&quot;, cellres = 1) plot(EINSTEIN) Then explore the structure of this image: str(EINSTEIN) ## Formal class &#39;pixmapGrey&#39; [package &quot;pixmap&quot;] with 6 slots ## ..@ grey : num [1:512, 1:512] 0.596 0.541 0.522 0.529 0.561 ... ## ..@ channels: chr &quot;grey&quot; ## ..@ size : int [1:2] 512 512 ## ..@ cellres : num [1:2] 1 1 ## ..@ bbox : num [1:4] 0 0 512 512 ## ..@ bbcent : logi FALSE Here you get a new data class of S4 type. Unlike data frames, you need to use “@” instead of “$” sign to access components of S4 objects. class(EINSTEIN@ grey) ## [1] &quot;matrix&quot; &quot;array&quot; EINSTEIN@ grey is a matrix representing pixel brightness values. The output ..@ grey : num [1:512, 1:512] reveals the dimension of matrix (512*512). The value of the pixel(0.2941176) at the 3rd row and the 4th column can be accessed: EINSTEIN@ grey[3,4] ## [1] 0.2941176 If you change the value 0.2941176 to 0 by EINSTEIN@ grey[3,4] &lt;- 0, the pixel will become pure black. If you assign a random number between 0 to 1 to the value, then the color in the pixel will be randomly assigned based on the random number. To blur an image, let’s define a mosaic.plot function: mosaic.plot &lt;- function(image, yrange, xrange){ length.y &lt;- length(yrange) length.x &lt;- length(xrange) image2 &lt;- image whitenoise &lt;- matrix(nrow = length.y, ncol = length.x, runif(length.y * length.x)) image2@grey[yrange, xrange] &lt;- whitenoise return(image2) } In this function, the argument image is the original image, yrange and xrange are the ranges of rows and columns that you want to blur, which means the xrange and yrange construct the mosaic region. Copy image to image2 to assure the original image unchanged. The whitenoise creates a matrix filled with random numbers following a uniform distribution, whose dimensions are determined by the mosaic region. Replace the values in the original image range that you want to blur image2@grey[yrange,xrange] by the whitenoise matrix. EINSTEIN.mosaic &lt;- mosaic.plot(EINSTEIN, 175:249, 118:295) plot(EINSTEIN.mosaic) Here, yrange=175:249 and xrange=118:295 select a sub-matrix from 175th to 249th row, and 118th to 295th column. This sub-matrix stores the color values of Einstein’s eyes region. The sub-matrix is replaced by whitenoise matrix. Therefore the image around the eyes region is replaced by image of random dots. The function locator() allows you to find the relevant rows and columns for specific regions in an image. Type locator() in the Console window, R will wait for you to click a point within an image. Click esc on your keyboard to exit the function, then the coordinates of that point will be in the Console window. If you click more points once, the function will return all coordinates of these points sorted by your clicking order. You must be careful about the y-coordinate. The row numbers in pixmap objects increase from the top of the image to the bottom, therefore you need to opposite the y-coordinate by subtracting them from the number of rows in the original image. For example, the y-coordinates obtained from locator() function are actually 337 and 263. After subtracting them from 512, you get 175 and 249, which are used as yrange in the mosaic function. ::: {.exercise #unnamed-chunk-211} Using the mosaic.plot() function to blur the image of mona_lisa.pgm by adding mosaic to the eyes region. Your output should look like the graph below. ::: ::: {.exercise #unnamed-chunk-213} Modify the following function so that the eyes region of Mona Lisa image is covered by a pure color but mosaic. Experiment with different color intensities by setting the degree = 0, 0.5 and 1 respectively, plot the images. mosaic.plot.2 &lt;- function(picture, yrange, xrange, degree){ length.y &lt;- _______(yrange) length.x &lt;- length(______) pic2 &lt;- picture pic2@____[_______,_______] &lt;- degree return(pic2) } ::: 3.2.3 Data Frames Data frames are fundamental in R for handling two-dimensional datasets. Unlike matrices, data frames can have different modes (types) for each column, which makes them a universal tool for data analysis. In fact, a data frame is a special case of a two-dimensional list. The data set iris that we analyzed before is a classic example of a data frame. class(iris) ## [1] &quot;data.frame&quot; head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 3.2.3.1 properties of a data frame There are some properties of a data frame. Diverse Column Modes: Data frames can contain columns of various types: numeric, factor, or character. For instance, Sepal.Length in data frame iris is numeric, while Species is a factor. class(iris$Sepal.Length) ## [1] &quot;numeric&quot; class(iris$Species) ## [1] &quot;factor&quot; Non-Empty Column Names: Typically, each column name represents a different variable. There are 5 column names or variables in the iris data set, they are Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species, respectively. colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Uniform Column Lengths: All columns in a data frame should have the same number of data points. This ensures each variable has the same number of observations. There are 150 observations for each variable in the iris data. Unique Row Names: The row names are unique. If they are not pre-named, the indices “1”, “2”, “3”, \\(\\cdots\\), “n” will be assigned to each row, where “n” is the length of rows or the number of observations. The row names of iris are shown below: rownames(iris) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; ## [13] &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; ## [25] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; ## [37] &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; ## [49] &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; ## [61] &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; ## [73] &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; ## [85] &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; ## [97] &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; ## [109] &quot;109&quot; &quot;110&quot; &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;114&quot; &quot;115&quot; &quot;116&quot; &quot;117&quot; &quot;118&quot; &quot;119&quot; &quot;120&quot; ## [121] &quot;121&quot; &quot;122&quot; &quot;123&quot; &quot;124&quot; &quot;125&quot; &quot;126&quot; &quot;127&quot; &quot;128&quot; &quot;129&quot; &quot;130&quot; &quot;131&quot; &quot;132&quot; ## [133] &quot;133&quot; &quot;134&quot; &quot;135&quot; &quot;136&quot; &quot;137&quot; &quot;138&quot; &quot;139&quot; &quot;140&quot; &quot;141&quot; &quot;142&quot; &quot;143&quot; &quot;144&quot; ## [145] &quot;145&quot; &quot;146&quot; &quot;147&quot; &quot;148&quot; &quot;149&quot; &quot;150&quot; 3.2.3.2 Basic operations A data frame can be created by combining vectors with the function data.frame() . For example: x &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) y &lt;- c(41, 32, 13, 89) z &lt;- c(&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;, &quot;TRUE&quot;) df1 &lt;- data.frame(x, y, z) df1 The &lt;fctr&gt;s underneath x and z in the output mean that character vectors x and z are automatically converted to factors. You can check the change by the class() function: class(x) ## [1] &quot;character&quot; class(df1$x) ## [1] &quot;character&quot; To prevent automatic conversion to factors, use an argument stringsAsFactors = FALSE in the data.frame() function. df2 &lt;- data.frame(x, y, z, stringsAsFactors = FALSE) df2 ## x y z ## 1 A 41 TRUE ## 2 B 32 FALSE ## 3 C 13 FALSE ## 4 D 89 TRUE class(df2$x) ## [1] &quot;character&quot; ::: {.exercise #unnamed-chunk-222} Determine the class of the R built-in data set mtcars using appropriate functions. ::: Data frame support various functions for data analysis. Read in data frame x firstly: x &lt;- iris Using summary() to get descriptive statistics of each column. summary(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## head() and tail() functions show the fist and last few rows. head(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Dimensions of the data frame (rows and columns): dim(x) ## [1] 150 5 You can just get number of rows or number of columns separately: nrow(x) ## [1] 150 ncol(x) ## [1] 5 str() is a very useful function, which shows data types for all columns. str(x) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Like in matrix, individual elements can be accessed in a data frame. Let’s select the element in the second row and third column: x[2, 3] ## [1] 1.4 Also you can subtract a subset data frame from x. For example, to extract the columns 2 to 4 and rows 1 to 10: x[1:10, 2:4] ## Sepal.Width Petal.Length Petal.Width ## 1 3.5 1.4 0.2 ## 2 3.0 1.4 0.2 ## 3 3.2 1.3 0.2 ## 4 3.1 1.5 0.2 ## 5 3.6 1.4 0.2 ## 6 3.9 1.7 0.4 ## 7 3.4 1.4 0.3 ## 8 3.4 1.5 0.2 ## 9 2.9 1.4 0.2 ## 10 3.1 1.5 0.1 Now view the first column: x[, 1] ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 Column selection can also be done using the data frame name x followed with the column name. x$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 The argument drop=FALSE will keep a one-column vector as a data frame. For example: class(x[, 1]) ## [1] &quot;numeric&quot; class(x[, 1, drop = FALSE]) ## [1] &quot;data.frame&quot; Use mean() function if you need to know the average Sepal Length. By the way, expressions can be nested. mean(x$Sepal.Length) ## [1] 5.843333 It’s very common to select a subset of data by certain columns or by certain conditions, using logical operations. The “==”, “&gt;”, and “&lt;” are logical operations and the “=” is an equality sign for assigning value. Let’s get a subset which contains only the species of setosa: y &lt;- subset(x, Species == &quot;setosa&quot;) head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa A subset that the length of sepal are greater than 7: subset(x, Sepal.Length &gt; 7) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 103 7.1 3.0 5.9 2.1 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica If more than one condition are applied to the data, use &amp; to connect the conditions. For example, to extract a subset which contains only the species of virginica, the sepals more than 7cm long, and the length of petal removed: x.3conds &lt;- x[x$Sepal.Length &gt; 7 &amp; x$Species == &quot;virginica&quot;, - 3] # -3 means removing the 3rd column from x. ::: {.exercise #unnamed-chunk-238} From data set mtcars, select a subset where cyl is 6 and mpg is greater than 21.2, excluding the variable carb. ::: 3.2.3.3 Advanced techniques and analysis Data frames can be modified by adding new columns or rows. Let’s add a new column named “id” to the data frame x, which goes from 1 to 150. Function head() is used to view the first 6 rows of the new data set. x.id &lt;- cbind(id = c(1:150), x) head(x.id) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 1 5.1 3.5 1.4 0.2 setosa ## 2 2 4.9 3.0 1.4 0.2 setosa ## 3 3 4.7 3.2 1.3 0.2 setosa ## 4 4 4.6 3.1 1.5 0.2 setosa ## 5 5 5.0 3.6 1.4 0.2 setosa ## 6 6 5.4 3.9 1.7 0.4 setosa Add another column of random numbers y to x.id. y &lt;- rnorm(150) x2 &lt;- cbind(y, x.id) head(x2) ## y id Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 0.88691825 1 5.1 3.5 1.4 0.2 setosa ## 2 -0.54760471 2 4.9 3.0 1.4 0.2 setosa ## 3 -0.89891836 3 4.7 3.2 1.3 0.2 setosa ## 4 0.08343956 4 4.6 3.1 1.5 0.2 setosa ## 5 0.61928627 5 5.0 3.6 1.4 0.2 setosa ## 6 -1.95236310 6 5.4 3.9 1.7 0.4 setosa Similar to cbind(), rbind() is for adding another exist row or rows with same length of columns. Now use tail() to examine the result. newRow &lt;- c(1, 1, 1, 1, &quot;setosa&quot;, 151) x3 &lt;- rbind(x, newRow) tail(x3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 146 6.7 3 5.2 2.3 virginica ## 147 6.3 2.5 5 1.9 virginica ## 148 6.5 3 5.2 2 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3 5.1 1.8 virginica ## 151 1 1 1 1 setosa You can sort the data frames by a specific columns, either in ascending or descending order. For example, to sort the first column in ascending, and the second column in descending: y &lt;- x[order(x[, 1]), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 14 4.3 3.0 1.1 0.1 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 43 4.4 3.2 1.3 0.2 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 4 4.6 3.1 1.5 0.2 setosa y &lt;- x[rev(order(x[, 2])), ] head(y) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 16 5.7 4.4 1.5 0.4 setosa ## 34 5.5 4.2 1.4 0.2 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 15 5.8 4.0 1.2 0.2 setosa ## 17 5.4 3.9 1.3 0.4 setosa ## 6 5.4 3.9 1.7 0.4 setosa It’s common to visualize the distribution of a variable grouped by another. For instance, use boxplot to observe sepal length distribution across species: boxplot(Sepal.Length ~ Species, x) You can perform statistical analyses by category, like mean of the variables by species: aggregate(. ~ Species, x, mean) ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.006 3.428 1.462 0.246 ## 2 versicolor 5.936 2.770 4.260 1.326 ## 3 virginica 6.588 2.974 5.552 2.026 ::: {.exercise #unnamed-chunk-245} Use aggregate() to calculate medians for variables mpg, disp, hp, and wt in data set mtcars across cyl. ::: Statistical analysis such as ANOVA and regression analysis are also possible with data frames. Let’s perform analysis of variance, named ANOVA, to test whether sepal length has significant difference among species. m &lt;- aov(Sepal.Length ~ Species, iris) summary(m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.21 31.606 119.3 &lt;2e-16 *** ## Residuals 147 38.96 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value is close to 0, therefore the null hypothesis is rejected and you can conclude that the length of sepal among three species are not same. The sapply() function applies functions across columns, so that you can do the ANOVA test for all of the 4 numeric variables across the species at once. aov.fun &lt;- function(temx){ m1 &lt;- aov(temx ~ iris$Species) summary(m1) } nvar &lt;- iris[- 5] # Remove the variable Species. da &lt;- sapply(nvar, aov.fun) # Apply aov.fun() function to each variable in the data set nvar. da ## $Sepal.Length ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 63.212 31.606 119.26 &lt; 2.2e-16 *** ## Residuals 147 38.956 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Sepal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 11.345 5.6725 49.16 &lt; 2.2e-16 *** ## Residuals 147 16.962 0.1154 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Petal.Length ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 437.10 218.551 1180.2 &lt; 2.2e-16 *** ## Residuals 147 27.22 0.185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## $Petal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## iris$Species 2 80.413 40.207 960.01 &lt; 2.2e-16 *** ## Residuals 147 6.157 0.042 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regression analysis uses linear model(lm) to analyze the relationship between these columns. Here sepal length is used as a function of sepal width and petal length, plus error. m &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length, x) summary(m) ## ## Call: ## lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96159 -0.23489 0.00077 0.21453 0.78557 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.24914 0.24797 9.07 7.04e-16 *** ## Sepal.Width 0.59552 0.06933 8.59 1.16e-14 *** ## Petal.Length 0.47192 0.01712 27.57 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3333 on 147 degrees of freedom ## Multiple R-squared: 0.8402, Adjusted R-squared: 0.838 ## F-statistic: 386.4 on 2 and 147 DF, p-value: &lt; 2.2e-16 ::: {.exercise #unnamed-chunk-249} Test if mpg, disp, hp, wt and qsec significantly differ across cyl in mtcars. Fill the blanks and interpret your conclusion based on the p-values. aov.fun.car &lt;- function(temx){ m2 &lt;- ________(________ ~ _________$______) summary(________) } sub.car &lt;- mtcars[, ___________] aov.car &lt;- ___________(_________, __________) ____________ ::: You can merge two data frames by the function merge(). dfmg1 &lt;- data.frame(V1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), V2 = c(90, 80, 70, 60), stringsAsFactors = FALSE) dfmg1 ## V1 V2 ## 1 A 90 ## 2 B 80 ## 3 C 70 ## 4 D 60 dfmg2 &lt;- data.frame(V1 = c(&quot;A&quot;, &quot;C&quot;), W2 = c(95, 85), stringsAsFactors = FALSE) dfmg2 ## V1 W2 ## 1 A 95 ## 2 C 85 merge(dfmg1, dfmg2) ## V1 V2 W2 ## 1 A 90 95 ## 2 C 70 85 The two data frames dfmg1 and dfmg2 here have the same variable V1. After merging, the rows with common elements “A” and “C” in both V1s are selected. To merge two data frames with columns contain similar information but different names, use by.x and by.y arguments to specify the variable names in the first and second data frame, respectively. dfmg3 &lt;- data.frame(W1 = c(&quot;A&quot;, &quot;F&quot;, &quot;C&quot;), W2 = c(95, 85, 65), stringsAsFactors = FALSE) dfmg3 ## W1 W2 ## 1 A 95 ## 2 F 85 ## 3 C 65 merge(dfmg1, dfmg3, by.x = &quot;V1&quot;, by.y = &quot;W1&quot;) ## V1 V2 W2 ## 1 A 90 95 ## 2 C 70 65 Now use data.frame() function to define a new data frame with 100 rows and two columns, ids and rand. y &lt;- data.frame(id = 1:100, rand = rnorm(100)) head(y) ## id rand ## 1 1 -0.8922799 ## 2 2 -1.9189952 ## 3 3 0.6032588 ## 4 4 -0.8541374 ## 5 5 -1.3943944 ## 6 6 0.1835520 tail(y) ## id rand ## 95 95 -0.4787542 ## 96 96 1.3590723 ## 97 97 1.4686959 ## 98 98 0.6267683 ## 99 99 -1.1354164 ## 100 100 -0.3957976 You can merge two data frames with different rows. Note the new data frame z has only 100 rows after merging x and y by the common column name id. x &lt;- iris x$id = c(1:150) z &lt;- merge(x, y, by = &quot;id&quot;) head(z) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species rand ## 1 1 5.1 3.5 1.4 0.2 setosa -0.8922799 ## 2 2 4.9 3.0 1.4 0.2 setosa -1.9189952 ## 3 3 4.7 3.2 1.3 0.2 setosa 0.6032588 ## 4 4 4.6 3.1 1.5 0.2 setosa -0.8541374 ## 5 5 5.0 3.6 1.4 0.2 setosa -1.3943944 ## 6 6 5.4 3.9 1.7 0.4 setosa 0.1835520 tail(z) ## id Sepal.Length Sepal.Width Petal.Length Petal.Width Species rand ## 95 95 5.6 2.7 4.2 1.3 versicolor -0.4787542 ## 96 96 5.7 3.0 4.2 1.2 versicolor 1.3590723 ## 97 97 5.7 2.9 4.2 1.3 versicolor 1.4686959 ## 98 98 6.2 2.9 4.3 1.3 versicolor 0.6267683 ## 99 99 5.1 2.5 3.0 1.1 versicolor -1.1354164 ## 100 100 5.7 2.8 4.1 1.3 versicolor -0.3957976 summary(z) ## id Sepal.Length Sepal.Width Petal.Length ## Min. : 1.00 Min. :4.300 Min. :2.000 Min. :1.000 ## 1st Qu.: 25.75 1st Qu.:5.000 1st Qu.:2.800 1st Qu.:1.500 ## Median : 50.50 Median :5.400 Median :3.050 Median :2.450 ## Mean : 50.50 Mean :5.471 Mean :3.099 Mean :2.861 ## 3rd Qu.: 75.25 3rd Qu.:5.900 3rd Qu.:3.400 3rd Qu.:4.325 ## Max. :100.00 Max. :7.000 Max. :4.400 Max. :5.100 ## Petal.Width Species rand ## Min. :0.100 setosa :50 Min. :-2.23085 ## 1st Qu.:0.200 versicolor:50 1st Qu.:-0.61558 ## Median :0.800 virginica : 0 Median : 0.17988 ## Mean :0.786 Mean : 0.01315 ## 3rd Qu.:1.300 3rd Qu.: 0.64910 ## Max. :1.800 Max. : 2.93330 You can also merge data frames by row names too, by adding row names as a new column in each data frame. # Create sample data frames df1 &lt;- data.frame(A = c(1, 2, 3), B = c(4, 5, 6)) rownames(df1) &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;) df2 &lt;- data.frame(C = c(7, 8, 9), D = c(10, 11, 12)) rownames(df2) &lt;- c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;) # Add row names as a new column in each data frame df1$RowName &lt;- rownames(df1) df2$RowName &lt;- rownames(df2) # Merge the data frames by the common column &quot;RowName&quot; merged_df &lt;- merge(df1, df2, by = &quot;RowName&quot;) 3.2.4 Strings and string vectors String manipulation is a common task in data analysis, especially when dealing with textual data. Let’s dive deeper into single-string operations, handling string vectors, and an advanced example involving DNA sequences. 3.2.4.1 Working with a Single String Define a string x: x &lt;- &quot;R is cool&quot; Count the number of characters: nchar(x) ## [1] 9 Concatenate strings(with a space by default): paste(x, &quot;!!&quot;) ## [1] &quot;R is cool !!&quot; Extract sub-string (from position of 6 and 9): substr(x, 6, 9) ## [1] &quot;cool&quot; Split string into a list separated by space: strsplit(x, &quot; &quot;) ## [[1]] ## [1] &quot;R&quot; &quot;is&quot; &quot;cool&quot; Replace “R” with “Tim”: gsub(&quot;R&quot;, &quot;Tim&quot;, x) ## [1] &quot;Tim is cool&quot; Remove everything after a space: gsub(&quot; .*&quot;, &quot;&quot;, x) ## [1] &quot;R&quot; Check for the pattern “is”: grepl(&quot;is&quot;, x) ## [1] TRUE Convert to lowercase or uppercase: tolower(x) ## [1] &quot;r is cool&quot; toupper(x) ## [1] &quot;R IS COOL&quot; 3.2.4.2 Handling String Vectors A string vector can hold many strings. This can be a column of names or IDs in a table. First let’s define a string vector: x &lt;- c(&quot;ab&quot;, &quot;cde&quot;, &quot;d&quot;, &quot;ab&quot;) You can use all commands about vector for string vector, such as accessing 2nd element in vector: x[2] ## [1] &quot;cde&quot; Determine the number of strings in the vector: length(x) ## [1] 4 Find unique elements: unique(x) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; Is there any duplicated element in the vector? duplicated(x) ## [1] FALSE FALSE FALSE TRUE The last element is duplicated. Count characters in each element: nchar(x) ## [1] 2 3 1 2 You can also unite two string vectors x and y if another vector y is defined first: y &lt;- c(&quot;ab&quot;, &quot;e&quot;) union(x, y) ## [1] &quot;ab&quot; &quot;cde&quot; &quot;d&quot; &quot;e&quot; Is there intercept among these two sets of strings? intersect(x, y) ## [1] &quot;ab&quot; Add something like “Q” to each element: paste(x, &quot;Q&quot;) ## [1] &quot;ab Q&quot; &quot;cde Q&quot; &quot;d Q&quot; &quot;ab Q&quot; To avoid the space between these elements and “Q”, try paste0(): paste0(x, &quot;Q&quot;) ## [1] &quot;abQ&quot; &quot;cdeQ&quot; &quot;dQ&quot; &quot;abQ&quot; Collapse multiple strings into one joined by space: paste(x, collapse = &quot; &quot;) ## [1] &quot;ab cde d ab&quot; 3.2.4.3 Advanced Example: Manipulating DNA Sequences Let’s experiment with a list practically. Consider a piece of DNA sequence: DNA &lt;- &quot;taaCCATTGtaaGAACATGGTTGTCcaaaCAAGATGCTAGT&quot; Note that the assignment operator “&lt;-” is used here. “=” can also function correctly in many instances but has the potential to be ambiguous. First, convert everything to upper case. DNA &lt;- toupper(DNA) Next, cut this DNA into smaller pieces by the pattern “ATG”. This type of thing happens in nature, as some enzymes cut DNA according to a certain pattern. segs &lt;- strsplit(DNA, &quot;ATG&quot;) # Split by &quot;ATG&quot; The result is contained in ‘segs’ as a list, which will be discussed later in this chapter. The unlist( ) function converts list into a string vector. segs &lt;- unlist(segs) segs # A vector of strings ## [1] &quot;TAACCATTGTAAGAAC&quot; &quot;GTTGTCCAAACAAG&quot; &quot;CTAGT&quot; segs[1] # First segment ## [1] &quot;TAACCATTGTAAGAAC&quot; ::: {.exercise #unnamed-chunk-279} In the iris dataset, define a new column FullName containing the full species name by adding “Iris” in front of each species name. For example, “setosa” becomes “Iris setosa”. ::: Note: some of the materials above have been inspired by http://tryr.codeschool.com/, which contains more in-depth information on string manipulation in R. 3.2.5 Lists Lists are adaptable structures in R that can hold objects of different types, overcoming the limitations of vectors. 3.2.5.1 List creating, modifying, and accessing Let’s start by creating a simple list: y &lt;- list(5, &quot;John Doe&quot;, c(100, 7), mean) # A list with 4 components y ## [[1]] ## [1] 5 ## ## [[2]] ## [1] &quot;John Doe&quot; ## ## [[3]] ## [1] 100 7 ## ## [[4]] ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x000001d92f930088&gt; ## &lt;environment: namespace:base&gt; You can associate each components with a tag for easy reference. y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y ## $height ## [1] 5 ## ## $name ## [1] &quot;John Doe&quot; ## ## $BP ## [1] 100 77 ## ## $fun ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x000001d92f930088&gt; ## &lt;environment: namespace:base&gt; Here is another way to create a list: x1 &lt;- matrix(c(4, 2, 5, 6, 10, 9), ncol = 3) # Matrix x2 &lt;- 8 : 1 # Vector x3 &lt;- c(2.4, 5.1, 9.0, 4.4) # Numeric vector x4 &lt;- c(&#39;u&#39;, &#39;v&#39;, &#39;w&#39;) # Character vector z &lt;- list(x1, x2, x3, x4) # Combine into a list z ## [[1]] ## [,1] [,2] [,3] ## [1,] 4 5 10 ## [2,] 2 6 9 ## ## [[2]] ## [1] 8 7 6 5 4 3 2 1 ## ## [[3]] ## [1] 2.4 5.1 9.0 4.4 ## ## [[4]] ## [1] &quot;u&quot; &quot;v&quot; &quot;w&quot; Or build a list by assigning values to an empty list: v &lt;- list() # Create an empty list v[[1]] &lt;- matrix(c(4, 2, 5, 6, 10, 9), ncol = 3) # Assign a matrix to v[[1]] v[[2]] &lt;- 8 : 1 # Assign a vector to v[[2]] v[[3]] &lt;- x3 # Assign x3 to v[[3]] where x3 is defined as above v[[4]] &lt;- x4 # Assign x4 to v[[4]] where x4 is defined as above v ## [[1]] ## [,1] [,2] [,3] ## [1,] 4 5 10 ## [2,] 2 6 9 ## ## [[2]] ## [1] 8 7 6 5 4 3 2 1 ## ## [[3]] ## [1] 2.4 5.1 9.0 4.4 ## ## [[4]] ## [1] &quot;u&quot; &quot;v&quot; &quot;w&quot; List ‘z’ and ‘v’ are identical despite being created differently, because they are assigned with the same components. There are several different ways to access the elements in a list. For example, to access the third component in y: y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y[[3]] # Specify the number in a double square bracket ## [1] 100 77 y$BP # Use $ sign and tag ## [1] 100 77 y[[&quot;BP&quot;]] # Use the tag in a double square bracket ## [1] 100 77 Note that double square bracket ‘[[]]’ is used here, which is different from using single square brake ‘[]’ in a vector. For a list, single square brackets return a sublist, while double square brackets return the actual element. For example, y[3] returns a list, y[[3]] returns a numerical vector. Use functions class() and is.list to check the type of the returned object: y[3] ## $BP ## [1] 100 77 class(y[3]) # Get the class of y[3] ## [1] &quot;list&quot; is.list(y[3]) # Check if y[3] is a list or not ## [1] TRUE y[[3]] ## [1] 100 77 class(y[[3]]) # Get the class of y[[3]] ## [1] &quot;numeric&quot; is.list(y[[3]]) # Check if y[[3]] is a list or not ## [1] FALSE Lists can store functions as elements. For example, the forth component of y is fun = mean. There are no quotation marks around mean. The class(y[[4]]) returns a function. class(y[[4]]) ## [1] &quot;function&quot; This implies that y[[4]] is the same as the mean() function. To apply it to a numeric vector: y[[4]](c(1, 2, 3, 4, 5)) # Get the mean of 1, 2, 3, 4, 5 ## [1] 3 ::: {.exercise #unnamed-chunk-288} Given list1 &lt;- list(Name = “Tom”, Hobby = “Fishing”, Num.fish = c(16, 27, 5)) Access the second component in list1 using three ways we introduced above. What are the types of list1[“Hobby”] and list1[[“Hobby”]], respectively? ::: 3.2.5.2 List operations Modify an element: y &lt;- list(height = 5, name = &quot;John Doe&quot;, BP = c(100, 77), fun = mean) y[[2]] &lt;- &quot;Mike&quot; # Change the 2nd component from &quot;John Doe&quot; to &quot;Mike&quot; y[[4]] &lt;- NULL # Delete the 4th component by setting it to NULL y ## $height ## [1] 5 ## ## $name ## [1] &quot;Mike&quot; ## ## $BP ## [1] 100 77 The output shows that the name has been changed to Mike and the mean function is deleted. So far, the length of y is 3: length(y) ## [1] 3 Add the forth components to y: y$class &lt;- c(&quot;math&quot;, &quot;art&quot;) # Add a class component y ## $height ## [1] 5 ## ## $name ## [1] &quot;Mike&quot; ## ## $BP ## [1] 100 77 ## ## $class ## [1] &quot;math&quot; &quot;art&quot; Create Lists with Tag-Value Associations: For example, associate the names and values into a list by creating an empty list and then fill it via assignment statements: sales &lt;- c(100, 105, 98, 112) seasons &lt;- c(&quot;Spring&quot;, &quot;Summer&quot;, &quot;Fall&quot;, &quot;Winter&quot;) sale.tag &lt;- list() # Create an empty list sale.tag[seasons] &lt;- sales # Associate sales data with seasons sale.tag ## $Spring ## [1] 100 ## ## $Summer ## [1] 105 ## ## $Fall ## [1] 98 ## ## $Winter ## [1] 112 Or, we can assign each component corresponding value or vector individually. sale.tag &lt;- list() # Start with an empty list # Assign values to specific seasons sale.tag[[&quot;Spring&quot;]] &lt;- 100 sale.tag[[&quot;Summer&quot;]] &lt;- 105 sale.tag[[&quot;Fall&quot;]] &lt;- 98 sale.tag[[&quot;Winter&quot;]] &lt;- 112 sale.tag ## $Spring ## [1] 100 ## ## $Summer ## [1] 105 ## ## $Fall ## [1] 98 ## ## $Winter ## [1] 112 ::: {.exercise #unnamed-chunk-294} For data set sale.tag, choose all correct answers from the following options A - F to answer the questions a, b, c: A: sale.tag[[1]] B: sale.tag[[Spring]] C: sale.tag[[“Spring”]] D: sale.tag[“Spring”] E: sale.tag[1] F: sale.tag[Spring] Which options return a vector. Answer:___________ Which options return a list. Answer:___________ Which options return an error. Answer:___________ ::: List calculation: Basic statistical functions work on vectors won’t do the same job on lists: mean(sale.tag) [1] NA Warning message: In mean.default(sale.tag) : argument is not numeric or logical: returning NA Before calculation, you need to use function unlist() to flatten the list into a vector: mean(unlist(sale.tag)) ## [1] 103.75 Similar to apply() function for vectors, lapply() (for list apply) works on each component of a list: x &lt;- list(c(10, 20, 30), c(4, 5, 6, 7, 8)) mean.x &lt;- lapply(x, mean) mean.x ## [[1]] ## [1] 20 ## ## [[2]] ## [1] 6 The mean.x returns a list consisting of 20 and 6, which are means of components of x respectively. Let’s redo the previous example by using function sapply() for a simpler output as vector or matrix. mean.x.2 &lt;- sapply(x, mean) mean.x.2 ## [1] 20 6 Check the output type by class() or is.vector() function: class(mean.x) ## [1] &quot;list&quot; class(mean.x.2) ## [1] &quot;numeric&quot; is.vector(mean.x.2) ## [1] TRUE Here is another example for sapply() function to filter out NULL elements. First apply is.null function to every element in the list, which will produce a vector with logical values. Then delete the element by setting NULL to it if the logical value comes as TRUE for the element. list.null &lt;- list(&#39;art&#39;, NULL, c(2, 4, 6, 8)) list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## NULL ## ## [[3]] ## [1] 2 4 6 8 list.null[sapply(list.null, is.null)] &lt;- NULL list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## [1] 2 4 6 8 For large datasets, sapply() is efficient to remove NULL elements. But for a smaller size dataset, it’s much easier to directly setting NULL to the NULL element in a known position. For example, we set a NULL to the second element of list.null: list.null &lt;- list(&#39;art&#39;, NULL, c(2, 4, 6, 8)) list.null[[2]] &lt;- NULL # Remove NULL elements list.null ## [[1]] ## [1] &quot;art&quot; ## ## [[2]] ## [1] 2 4 6 8 3.2.5.3 Example using list Lists in R are incredibly versatile, allowing you to efficiently manage and manipulate complex sets of data. Suppose we have a list of customers who visited a store in the last 10 days. Some customers visited it more than once. The customer names are recorded by the order of the day they came: customer &lt;- list(&quot;Alex&quot;, &quot;Brandon&quot;, &quot;Alex&quot;, &quot;Daniel&quot;, &quot;Grace&quot;, &quot;Mary&quot;, &quot;Mary&quot;, &quot;Alex&quot;, &quot;Tom&quot;, &quot;Grace&quot;) To find out the days and the total number of days that each customer visited the store, here is a function which returns all locations of the same elements in a list. loc.names &lt;- function(f){ y &lt;- unlist(f) # Flatten a list x into a vector y x &lt;- list() # Create an empty list for (i in 1:length(y)){ c.name &lt;- y[i] # Assign the ith element to c.name. x[[c.name]] &lt;- c(x[[c.name]], i) # Assign values to x[[c.name]]. c.name is the name of x. } return(x) } When you apply the function, the f will be replaced by your own list, like customer. x &lt;- list () creates an empty list. In the for loop, i starts from 1 to the length of vector y. In the customer example, the length of y is 10. c.name &lt;- y[i] assigns the i-th element in y to a new variable c.name. For example, if i = 1, y[1] is the “Alex”, then c.name = “Alex”. x[[c.name]] &lt;- c(x[[c.name]], i) : For i = 1, you have c.name &lt;- “Alex”, then x[[c.name]] = x[[“Alex”]] = NULL since x starting from an empty list. Therefore c(x[[“Alex”]], i) = c(NULL, i) = i = 1. The value 1 will be assigned to x[[“Alex”]] by the code: x[[“Alex”]] &lt;- c(x[[“Alex”]], i) = 1. Now x is not an empty list, it is a list with 1 component: x &lt;- list() x[[&quot;Alex&quot;]] &lt;- 1 x ## $Alex ## [1] 1 For i = 2, y[2] &lt;- “Brandon”, then similar to the case of i = 1, you have x[[“Brandon”]] &lt;- 1: x[[&quot;Brandon&quot;]] &lt;- 1 x ## $Alex ## [1] 1 ## ## $Brandon ## [1] 1 For i = 3, y[3] &lt;- “Alex”. You have met “Alex” once, which returned x[[“Alex”]] &lt;- 1. Now you meet “Alex” again, which means you are going to update x[[“Alex”]] by x[[“Alex”]] &lt;- c(x[[“Alex”]], i) = c(1, i) = c(1, 3). The updated x is: x[[&quot;Alex&quot;]] &lt;- c(1, 3) x ## $Alex ## [1] 1 3 ## ## $Brandon ## [1] 1 The process will stop till i = length (y) = 10. The loc.names() function will return all locations of all names which are already stored in x. Now let’s apply the function to the customer list. customer &lt;- list(&quot;Alex&quot;, &quot;Brandon&quot;, &quot;Alex&quot;, &quot;Daniel&quot;, &quot;Grace&quot;, &quot;Mary&quot;, &quot;Mary&quot;, &quot;Alex&quot;, &quot;Tom&quot;, &quot;Grace&quot;) v1 &lt;- loc.names(customer) v1 ## $Alex ## [1] 1 3 8 ## ## $Brandon ## [1] 2 ## ## $Daniel ## [1] 4 ## ## $Grace ## [1] 5 10 ## ## $Mary ## [1] 6 7 ## ## $Tom ## [1] 9 The output reveals the days each customer visited the store: Alex visited store at the first, third and eighth days, Brandon came on the second day, etc. Next, use sapply() function to calculate the total number of visits per customer. v2 &lt;- sapply(v1,length) # calculate the repeat times of each element in v1 v2 ## Alex Brandon Daniel Grace Mary Tom ## 3 1 1 2 2 1 Then, sort the visit frequency in decreasing order: sort(v2, decreasing = T) ## Alex Grace Mary Brandon Daniel Tom ## 3 2 2 1 1 1 ::: {.exercise #unnamed-chunk-309} Fill in the blanks in the function my.fun() and apply it to a list of genders. The function should return all element locations and sort the frequencies in increasing order. The geneder set includes: F F M M M M I I F I F M I F M I I M I I F Where F, M, and I represent Female, Male, and Infant, respectively. my.fun &lt;- function(f){ y &lt;- unlist(f) x &lt;- ___________ for (i in 1:____){ g1 &lt;- y[i] x[[______]] &lt;- c(x[[____]],i) } freq &lt;- _________(x, length) z &lt;- sort(________, decreasing = ____) lst &lt;- list(x, z) ___________(lst) } gender &lt;- list( “F”, ________________________________, “F”) __________(gender) ::: Many R functions, such as t.test(), return results as lists, which contain a series of components, such as p-values, vectors of residuals or coefficients, and even matrices. A list combines everything into one big object that is easy to break down and understand. To compare two sets of random numbers: r1 &lt;- rnorm(100) # Create 100 random numbers following Normal Distribution r2 &lt;- rnorm(100) t.result &lt;- t.test(r1, r2) # Run the t test. Test if the mean of r1 and r2 are same. is.list(t.result) # Check if the t.result is a list ## [1] TRUE t.result # A list holds all components of t-test ## ## Welch Two Sample t-test ## ## data: r1 and r2 ## t = 0.57704, df = 197.85, p-value = 0.5646 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2145801 0.3921035 ## sample estimates: ## mean of x mean of y ## 0.03685420 -0.05190749 names(t.result) # Return all tags of the t test result ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; t.result$p.value # Retrieve p-value for the test ## [1] 0.5645683 t.result$estimate # Return mean of r1 and r2 ## mean of x mean of y ## 0.03685420 -0.05190749 The t-test result is efficiently stored in a list, providing structured access to various test outcomes like p-values and estimates. Run the ‘?t.test’ to access the help page for more information about types of returned values and their names. Finally, let’s run a simulation with lists, using a loop. We will generate two sets of 100 random numbers from the standard normal distribution with zero mean and unit standard deviation, then perform t-test and get the p-value. By repeating this process 500 times, observe the distribution of p-values and count significant results with p &lt; 0.05. pvalues &lt;- rep(1, 500) # Define a vector containing 500 numbers, all equal to 1. for (i in 1:500) { # Loop: The values of i takes values from 1,2,3, …, 500 result = t.test(rnorm(100), rnorm(100)) pvalues[i] = result$p.value # p-values are stored in the i-th element in the vector } hist(pvalues) # Histogram of p-values summary(pvalues) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.001597 0.211807 0.442491 0.467228 0.737778 0.993367 sum(pvalues &lt; 0.05) # Total number of p-value&#39;s which are less than 0.05. ## [1] 29 This simulation demonstrates how lists can be effectively used to store and analyze results from repetitive statistical tests. "],["importing-data-and-managing-files.html", "Chapter 4 Importing data and managing files 4.1 Project-oriented workflow 4.2 Read files directly using read.table 4.3 General procedure to read data into R: 4.4 Enter data manually 4.5 Data manipulation in a data frame 4.6 Data transformation using the dplyr", " Chapter 4 Importing data and managing files Learning objectives: Create projects in Rstudio Understand the proper steps to import data Get introduced to data transformation using dplyr 4.1 Project-oriented workflow When embarking on your data journey, it’s smart to create a separate folder for each project that contains all related files. This approach is especially handy for research projects. Each of these folders, known in RStudio as a Project. For example, you can create a project for each chapter in this book. Projects are self-contained universe, making your code robust and portable across different computers and directories. If the first line of your R script is setwd(“C:\\Users\\jenny\\path\\that\\only\\I\\have”) I will come into your office and SET YOUR COMPUTER ON FIRE —@JennyBryan on Twitter Here’s a helpful video guiding you through creating a project and importing data in RStudio. And down here the process is illustrated in detail. Animated GIF for creating project and import data. 4.1.1 Create a project in a new folder First, let’s set up a new project. In RStudio, go to File-&gt;New project-&gt;New Directory-&gt;Empty Project. Then choose the directory’s house on your hard drive where the project will be created. For instance, create a directory named “Chapter4” under “D:\\RBook”. Rstudio will then whip up a project file “Chapter4.Rproj”, which contains essential information such as scripts files and working folders. Project file can be saved and reopened later from File-&gt;Open or just simply by double-clicking on it in Windows. This folder becomes your working folder, a default folder for reading files, writing outputs, etc. Now you get everything ready to go on a particular assignment or research project. Advanced users should consider create a project using version control for an extra layer of magic, so that all versions of your code is backup and easily shared by GitHub. Check out Happy Git and GitHub for the useR by Jenny Bryan for more enchantments. 4.1.2 Create a script file (and Comment, Please!) Once you have set up a new project, the first step is to start a new script file by clicking the File + button or go to File-&gt;New file and choose R script file. By default, the script file is called Untitled1.R. Rstudio awaits you to rename it at the first time you hit “Save” button. Start your R script by adding comments with background information. Comments starting with “#” are ignored by R when running, but they are helpful for your future self and others to understand the code. We re-cycle and re-use our codes over and over, so it is vital to add information about the background and purpose of your code segments. Figure 4.1 shows a recommended workflow for starting your script. Click “Save” button and name your file something like “Data Import.R”. Efficient coding in R requires a systematic approach to writing and executing scripts, also, write your scripts while saving your project files. In RStudio, executing code is straightforward. If you click on the Run button, Rstudio runs the current line of code where your cursor is located. If you select multiple lines and hit Run, you can run them all at once. You can even jump back and forth when editing and rerunning sections of code. But remember that you are operating on the data objects sequentially. It is crucial to maintain awareness of this order. Run the reset line rm(list=ls()) if you want to get a fresh start. This command lists and then deletes all data objects from R’s brain. Don’t forget to save everything once a while by hitting the Save button on the main icon bar! Even though Rstudio saves your scripts every 5 seconds, unforeseen crashes may occur. As you develop your coding skills, following these above guidelines can make you more efficient when you start a R script. Figure 4.1: A recommended workflow for starting a project in Rstudio: commenting, resetting, checking the working folder. 4.1.3 Copy data files to the new directory This step is a little field trip outside RStudio on Windows or Mac interface. Download the the necessary files, like heartatk4R.txt file from here. It is a tab-delimited text file, meaning its columns are separated by tabs. We need to get some context about the data and how it is collected. Knowing what each column represents is crucial, like decoding a treasure map. File unzipping, conversion, and context. If your data is compressed, use 7-zip, WinRAR, Winzip, or gzip to unzip it. Check if your file is text file (CSV, txt, …) or Binary file (XLS, XLSX, …), and convert binary to text file using corresponding application. Remember, CSV files separate the columns by commas, while tab-delimited files use the invisible character tab, \\(\\t\\). Checking the file with text editor and Excel. Before reading files into R, we often need to open the files to take a look. Plain text files only contain text without any formatting, links, and images. The file names can be “poems.txt”, “poems.tex”, “students.csv”, or just “data” without extension. I often save my R scripts as text files with names like “code_1-22-2017.R”. To peek inside the data, avoid the limited (and amateur) editors like Notepad or WordPad , and definitely don’t invite Microsoft Word for this purpose! I strongly recommend that you install a powerful text editor such as NotePad++ (https://notepad-plus-plus.org/), or TextPad (https://www.textpad.com/). If you are a Mac user, try TextMate, TextWrangler etc. I use NotePad++ almost every day to look into data, and also write R programs, as it can highlight R commands based on R syntax. I even use a tool called NppToR (https://sourceforge.net/projects/npptor/) to send R commands from NotePad++ directly to R, and I love it! Regardless of the extensions(.txt, .tex, .csv, etc.) in file names, all plain text files can be opened by these text editors. Excel can also be a handy sidekick. You can import text files, regardless of file names, to Microsoft Excel, which can properly parse your file into columns if the correct delimiter is specified. Comma separated values (CSV) files can also be conveniently opened by Excel. Rstudio favors CSV files and tab-delimited text files. Other types of files such as Excel, .xls, or .xlsx files are often needed to be saved as CSV files for smoother journeys. 4.1.4 Import data files In Rstudio, click File-&gt;Import Dataset-&gt;From text(readr)…, and find the file on your hard drive. You should change the Delimiter to “tab” for a correct read. The preview shows that the data is correctly parsed into multiple columns. You can rename your data object for ease. For instance, change the default “heartatk4R” to simple “df” on the lower left of the import interface. We need to check each of the columns to ensure the data types match their intended purpose. The first column is just patient id number runs from 1 to 12844. It will not be useful in our analysis. The numbers in DIAGNOSIS, DRG, and DIED are integers but they actually code for certain categories. They are not measurements. It does not make sense, for example, to add them or average them. Most of the times, there is no particular order. The same is true for SEX. So in this dialog interface, DIAGNOSIS, DRG, and DIED should be changed from “double” to “character”. As shown in Figure 4.2, you can click on the automatically guessed data type under each of the column names. By selecting “character” from the drop down list, you can successfully format the column as character. On the other hand, LOS (length of stay in days) and AGE should be numbers. But because 10 is presented as “0010”, these columns are automatically recognized as characters. We have to force R to read these columns as integers by clicking on the column title and select integer. So that we change LOS and AGE from “character” to “integer”. Rstudio, like an awesome apprentice, actually helped you generating these 3 lines of code: library(readr) df &lt;- read_delim(&quot;datasets/heartatk4R.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, col_types = cols(AGE = col_integer(), DIAGNOSIS = col_character(), DIED = col_character(), DRG = col_character(), LOS = col_integer()), trim_ws = TRUE) View(df) Before you click on the Import button, I highly recommend that you select all the codes and copy them to the script file for future use. If you already have a script window open, you can directly paste the code into a script window after clicking Import. If not, create one by clicking the File + icon on the top left, then copy and paste these codes to your script file. You will need them if you want to re-run the analysis without going through the above steps. You can view the data which now appears as a spreadsheet after importing. It can be sorted by clicking on the column names. This spreadsheet can be closed. To reopen, click on ‘df’ object in your workspace, which is the data frame named for the imported file. You data is now available as df. Figure 4.2: Adjusting data types while importing data into Rstudio. 4.1.5 Check and convert data types R usually guesses column types correctly, but you always need to verify using the str() command. If the types aren’t quite right, you can enforce data type conversion using functions like as.numeric, as.factor, or as.character. str(df) # Structure and data types for each column ## spc_tbl_ [12,844 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Patient : num [1:12844] 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: chr [1:12844] &quot;41041&quot; &quot;41041&quot; &quot;41091&quot; &quot;41081&quot; ... ## $ SEX : chr [1:12844] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... ## $ DRG : chr [1:12844] &quot;122&quot; &quot;122&quot; &quot;122&quot; &quot;122&quot; ... ## $ DIED : chr [1:12844] &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ... ## $ CHARGES : num [1:12844] 4752 3941 3657 1481 1681 ... ## $ LOS : int [1:12844] 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : int [1:12844] 79 34 76 80 55 84 84 70 76 65 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Patient = col_double(), ## .. DIAGNOSIS = col_character(), ## .. SEX = col_character(), ## .. DRG = col_character(), ## .. DIED = col_character(), ## .. CHARGES = col_double(), ## .. LOS = col_integer(), ## .. AGE = col_integer() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Categorical values can be reformatted as factors using as.factor() function. Here use df$SEX to refer to the SEX column of the data frame df: df$DIAGNOSIS &lt;- as.factor(df$DIAGNOSIS) # Convert this column to a factor df$SEX &lt;- as.factor(df$SEX) df$DRG &lt;- as.factor(df$DRG) df$DIED &lt;- as.factor(df$DIED) Factors are very similar to character vectors but with superpowers of defined levels and integer storage. nlevels(df$DIAGNOSIS) # Explore the number of levels ## [1] 9 levels(df$DIAGNOSIS) # Display the levels ## [1] &quot;41001&quot; &quot;41011&quot; &quot;41021&quot; &quot;41031&quot; &quot;41041&quot; &quot;41051&quot; &quot;41071&quot; &quot;41081&quot; &quot;41091&quot; Note “41001” is the reference level for this factor and it is coded as 1. The reference level for factors is important when we interpret results from regression, as effects are represented relative to the reference level. The reference level is defined automatically based on the order of the factors appear in the dataset. Sometimes we need to change the reference level: df$DIAGNOSIS &lt;- relevel(df$DIAGNOSIS, &quot;41091&quot;) levels(df$DIAGNOSIS) ## [1] &quot;41091&quot; &quot;41001&quot; &quot;41011&quot; &quot;41021&quot; &quot;41031&quot; &quot;41041&quot; &quot;41051&quot; &quot;41071&quot; &quot;41081&quot; str(df) # Recheck the structure ## spc_tbl_ [12,844 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Patient : num [1:12844] 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41091&quot;,&quot;41001&quot;,..: 6 6 1 9 1 1 1 1 6 6 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num [1:12844] 4752 3941 3657 1481 1681 ... ## $ LOS : int [1:12844] 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : int [1:12844] 79 34 76 80 55 84 84 70 76 65 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Patient = col_double(), ## .. DIAGNOSIS = col_character(), ## .. SEX = col_character(), ## .. DRG = col_character(), ## .. DIED = col_character(), ## .. CHARGES = col_double(), ## .. LOS = col_integer(), ## .. AGE = col_integer() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; summary(df) # Summarize the data for a quick overview ## Patient DIAGNOSIS SEX DRG DIED CHARGES ## Min. : 1 41091 :5213 F:5065 121:5387 0:11434 Min. : 3 ## 1st Qu.: 3212 41041 :2665 M:7779 122:6047 1: 1410 1st Qu.: 5422 ## Median : 6422 41011 :1824 123:1410 Median : 8445 ## Mean : 6422 41071 :1703 Mean : 9879 ## 3rd Qu.: 9633 41001 : 467 3rd Qu.:12569 ## Max. :12844 41081 : 287 Max. :47910 ## (Other): 685 NA&#39;s :699 ## LOS AGE ## Min. : 0.000 Min. : 20.00 ## 1st Qu.: 4.000 1st Qu.: 57.00 ## Median : 7.000 Median : 67.00 ## Mean : 7.569 Mean : 66.29 ## 3rd Qu.:10.000 3rd Qu.: 77.00 ## Max. :38.000 Max. :103.00 ## The summary( ) function is very useful to get basic information about data frame. It distinguishes numeric columns by statistics like mean and median, and factors by frequency counts. This reassured us that the data types are correctly recognized. It also shows missing values in CHARGES. Did some people get free treatment for heart attack? Maybe not. Missing does not mean zero. Maybe the data was not entered for some patients. Just a reminder: Except enforcing data type conversion by as.factor, as.numeric and so on, we can also reformat the columns before clicking Import as we described in 4.2.4. 4.1.6 Close a project when you are done Think of RStudio as a friendly but slightly clingy companion. So, when you’re done with a project, it’s necessary to say goodbye politely by navigating to File \\(\\rightarrow\\)Close Project. If not, RStudio assumes that you will be continue working on the same project; even after your close Rstudio, the same project and files will be open the next time you start Rstudio. It is convenient, most of times. But I’ve seen some students’ Rstudio environment cluttered with all the things they have been doing for an entire semester. This is not only confusing, but could also lead to errors. For this class, consider starting a fresh project for each chapter. To open a project, use File \\(\\rightarrow\\)Open Project and then navigate to the project. Alternatively you can double-click on the project file (like “Chapter4.Rproj”) from your file explorer in Windows or Mac. When a project file is loaded, the entire computing environment is kindly set for you, including the working directory and any script files you left open. If a script file is not open, you can easily access it by clicking on it from the Files tab in the lower right window. 4.2 Read files directly using read.table As you get more experience with R programming, you’ll find multiple ways to import data, beyond the friendly Import Dataset feature in RStudio. Here’s the code to bring in our heart attack dataset, ensuring it’s in the current working directory. To set working directory from the Rstudio main menu, go to Session -&gt; Set Working Directory. rm(list = ls()) # Erase all objects in memory getwd() # Display the current working directory df &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, sep=&quot;\\t&quot;, header = TRUE) head(df) # Peek at the first few rows # Convert several columns to factors df$DRG &lt;- as.factor(df$DRG) df$DIED &lt;- as.factor(df$DIED) df$DIAGNOSIS &lt;- as.factor(df$DIAGNOSIS) df$SEX &lt;- as.factor(df$SEX) str(df) # Check the data types of columns summary(df) # Get a summary overview of the dataset Alternatively, for the digital explorer, try this: URL &lt;- &quot;https://raw.githubusercontent.com/gexijin/learnR/master/datasets/heartatk4R.txt&quot; df &lt;- read.table(URL, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) This way, you’re directly importing data from the internet using the URL and defining column data types right off the bat. 4.3 General procedure to read data into R: Unzip files using 7-zip, WinRAR, Winzip, gzip, etc, if data is compressed. Any of these will work. Identify if it’s a text file (CSV, txt, …) or Binary file (XLS, XLSX, …). Convert binary to text file using corresponding application. Comma separated values (CSV) files use comma to separate the columns. While tab-delimited text files(txt) use the tab, or the invisible character \\(\\t\\). Preview the file using a text editor like TexPad or NotePad++. Check for rows and columns and their names. Use row.names = 1, header = T if needed. Identify the delimiters between columns (space, comma, tab…). Use sep = “\\(\\t\\)” for tabs. Missing values? Look for NA, na, NULL, blank, NaN, and 0. Specify them using missingstring = . Open as text file in Excel, choosing the appropriate delimiter while importing, or using the Text to Column under ‘Data’ in Excel. Beware of the annoying automatic conversion in Excel (e.g., “OCT4” to “4-OCT”). Edit column names by removing spaces, or shorten them for easy of reference in R. And save as CSV. Read data using read.table( ), or read.csv( ). For example, x &lt;- read.table(“somefile.txt”, sep = “\\t”, header = TRUE, missingstring = “NA”) Double check data types with str(x), make sure each column is recognized correctly as “character”, “factor” and “numeric”. Pay special attention to numbers that are actually IDs or categories; as they need specific treatments. IDs (i.e. student IDs) should be treated as character. For example, x$ids &lt;- as.character(x$ids), here x is the data frame and ids is the column name. Codes for some discrete categories (1, 2, 3, representing treatment 1, treatment 2 and treatment 3), need to be reformatted as factors. This could be done with something like x$treatment &lt;- as.factor(x$treatment). Check out cheat sheets for a quick refresher, which summarize many R functions and are available here: https://www.rstudio.com/resources/cheatsheets/. Remember, it is important to understand the different types of R objects: scalars, vectors, data frames, matrix, and lists. ::: {.exercise #unnamed-chunk-321} Create a project for chapter 4 if you haven’t yet. Download the tab-delimited text file pulse.txt from [this link] (http://statland.org/R/R/pulse.txt). Import pulse.txt into R using two methods: R menu (Show the process with necessary screenshots) and R script. a. Rename the file as chapter4Pulse. b. Change the class of ActivityL from double to integer. c. After importing pulse.txt into R, convert the class of Sex from charater to factor using R code. Don’t forget using class() function to check your answer. ::: ::: {.exercise #unnamed-chunk-322} Type the data from Table 4.1 in Excel and save as both CSV file and tab-delimited text file. Start a new Rstudio project as previously described. Copy the files to the new folder, and import the CSV file to Rstudio. Create a script file which includes the command for preparing the workspace (rm(list = ls()) and getwd()), the generated R code when importing the CSV file (similar to those shown in Figure 4.2), and the code to convert data types (Age, BloodPressure and Weight should be numeric, LastName should be character, and HeartAttack should be factor). Name the data set as patients. Submit your R script, the data structure of patients, and the output from head(patients). ::: Table 4.1: An example of a multivariate dataset. LastName Age Sex BloodPressure Weight HeartAttack Smith 19 M 100 130.2 1 Bird 55 F 86 300 0 Wilson 23 M 200 212.7 0 4.4 Enter data manually There are many different ways to invite data into R. You can enter data manually (see below), or semi-manually (see below), or read data from a local file or a file on the internet. R can also retrieve data from databases, local or remote. The most important thing is to read data set correctly into R. A dataset misread by R is like a mischievous elf - it will never be analyzed or visualized correctly. x &lt;- c(2.1, 3.1, 3.2, 5.4) sum(x) ## [1] 13.8 A &lt;- matrix( c(2, 4, 3, 1, 5, 7), # Data elements nrow = 2, # Number of rows ncol = 3) # Number of columns A # Display the matrix ## [,1] [,2] [,3] ## [1,] 2 3 5 ## [2,] 4 1 7 And for a twist, you can even use the scan() function to manually input values, or simply copy and paste a column of numbers from Excel. x &lt;- scan() # Manually enter values from keyboard, separated by Return key. End by empty line. 2.1 ## [1] 2.1 3.1 ## [1] 3.1 4.1 ## [1] 4.1 4.5 Data manipulation in a data frame Let’s take our heart attack dataset df and give it a spin. Here’s a golden rule: always type commands in the Script window, not directly into the Console. And Make it a habit to save your work frequently – there’s nothing sadder than losing data! Now imagine we want to sort our patients by age: df2 &lt;- df[order(df$AGE), ] # Sort by ascending order by AGE Global Environment window is like a digital pantry, storing the names and sizes of all the variables or objects in the computer memory. R programming is essentially a culinary art of creating and modifying these objects with crystal-clear, step-by-step instructions. And yes, you can also sort the data by clicking on the column names in spreadsheet view in the Global Environment, just like in Excel. Speaking of Excel, you can add a new column with computed results in R too: df2$pdc &lt;- df2$CHARGES / df2$LOS Here we created a new column pdc to represent per day cost. You can also create a column to represent age groups using the floor function which just returns the integer part. df2$ag &lt;- floor(df2$AGE/10) * 10 You can now do nifty things like this: boxplot(df2$CHARGES ~ df2$ag) # Boxplot of charges by age groups Each box represents an age group’s hospital stay during the post-heart attack. Older patients tend to stay longer in the hospital after heart attack. For a more focused analysis, extract a specific subset: df3 &lt;- subset(df2, SEX == &quot;F&quot;) # Only females. “==” is for comparison and “=” is for assign value. df4 &lt;- subset(df3, AGE &gt; 80) # Only people older than 80 summary(df4) # What&#39;s the story here? ## Patient DIAGNOSIS SEX DRG DIED CHARGES ## Min. : 7 41091 :564 F:1263 121:679 0:929 Min. : 92 ## 1st Qu.: 3416 41071 :224 M: 0 122:250 1:334 1st Qu.: 5179 ## Median : 6816 41041 :181 123:334 Median : 8580 ## Mean : 6624 41011 :166 Mean :10143 ## 3rd Qu.: 9814 41001 : 41 3rd Qu.:13347 ## Max. :12841 41081 : 38 Max. :46915 ## (Other): 49 NA&#39;s :74 ## LOS AGE pdc ag ## Min. : 1.000 Min. : 81.00 Min. : 18.4 Min. : 80.00 ## 1st Qu.: 5.000 1st Qu.: 83.00 1st Qu.: 796.7 1st Qu.: 80.00 ## Median : 8.000 Median : 85.00 Median : 1113.7 Median : 80.00 ## Mean : 9.457 Mean : 85.92 Mean : 1331.4 Mean : 81.96 ## 3rd Qu.:13.000 3rd Qu.: 89.00 3rd Qu.: 1523.8 3rd Qu.: 80.00 ## Max. :38.000 Max. :102.00 Max. :11246.1 Max. :100.00 ## NA&#39;s :74 Remember, it’s best to avoid attaching data during such manipulations. 4.6 Data transformation using the dplyr Following the same style of ggplot, the dplyr package, a part of the Tidyverse, makes data transformation more intuitive. First let’s install the dplyr package. install.packages(&quot;dplyr&quot;) With dplyr, data transformation becomes more of a dance than a chore. library(dplyr) df2 &lt;- df %&gt;% # Pipe operator; data is send to the next step arrange(AGE) # Sort AGE in ascending order; desc(AGE) for descending order head(df2) ## Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE ## 1 5411 41041 M 122 0 6214.00 4 20 ## 2 10853 41091 F 122 0 6726.27 4 21 ## 3 4126 41041 M 122 0 10781.00 8 23 ## 4 10738 41011 M 121 0 NA 8 23 ## 5 4247 41091 F 122 0 10672.00 6 24 ## 6 5199 41041 M 121 0 7596.00 8 24 df2 &lt;- df2 %&gt;% mutate( pdc = CHARGES / LOS) # New column for per day cost head(df2) ## Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE pdc ## 1 5411 41041 M 122 0 6214.00 4 20 1553.500 ## 2 10853 41091 F 122 0 6726.27 4 21 1681.568 ## 3 4126 41041 M 122 0 10781.00 8 23 1347.625 ## 4 10738 41011 M 121 0 NA 8 23 NA ## 5 4247 41091 F 122 0 10672.00 6 24 1778.667 ## 6 5199 41041 M 121 0 7596.00 8 24 949.500 df2 &lt;- df2 %&gt;% mutate( ag = floor(AGE/10) * 10) # New column for age groups head(df2) ## Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE pdc ag ## 1 5411 41041 M 122 0 6214.00 4 20 1553.500 20 ## 2 10853 41091 F 122 0 6726.27 4 21 1681.568 20 ## 3 4126 41041 M 122 0 10781.00 8 23 1347.625 20 ## 4 10738 41011 M 121 0 NA 8 23 NA 20 ## 5 4247 41091 F 122 0 10672.00 6 24 1778.667 20 ## 6 5199 41041 M 121 0 7596.00 8 24 949.500 20 df3 &lt;- df %&gt;% filter(SEX == &quot;F&quot;, AGE &gt; 80) # Filtering for females over 80 head(df3) ## Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE ## 1 7 41091 F 121 0 10958.52 15 84 ## 2 13 41091 F 122 0 NA 9 83 ## 3 27 41011 F 123 1 3214.90 4 83 ## 4 36 41041 F 121 0 2584.10 9 81 ## 5 51 41011 F 121 0 7589.00 11 81 ## 6 67 41011 F 123 1 16428.80 8 81 The real power and efficiency of dplyr comes when you connect the pipes to do data transformation in multiple steps, like a chain of dominoes. df2 &lt;- df %&gt;% arrange(AGE) %&gt;% mutate( pdc = CHARGES / LOS) %&gt;% mutate( ag = floor(AGE/10) * 10) %&gt;% filter(SEX == &quot;F&quot;, AGE &gt; 80) head(df2) ## Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE pdc ag ## 1 36 41041 F 121 0 2584.10 9 81 287.1222 80 ## 2 51 41011 F 121 0 7589.00 11 81 689.9091 80 ## 3 67 41011 F 123 1 16428.80 8 81 2053.6000 80 ## 4 225 41041 F 123 1 4674.50 7 81 667.7857 80 ## 5 265 41091 F 122 0 9401.05 9 81 1044.5611 80 ## 6 284 41041 F 121 0 7240.30 7 81 1034.3286 80 arrange, mutate, filter are called action verbs in the dplyr realm. For more action verbs, check out the dplyr cheat sheet from the Rstudio main menu: Help \\(\\rightarrow\\) Cheatsheets \\(\\rightarrow\\) R Markdown Cheat Sheet. It is also available online at dplyr cheat Sheet. So there you have it! With tools like dplyr, you’re transforming data analysis into a harmonious symphony of insights. Enjoy the rhythm of data exploration! "],["the-heart-attack-data-set-i.html", "Chapter 5 The heart attack data set (I) 5.1 Begin your analysis by examining each column individually 5.2 Possible correlation between two numeric columns 5.3 Associations between categorical variables 5.4 Associations between a categorical and a numeric variables 5.5 Associations between multiple columns", " Chapter 5 The heart attack data set (I) The heart attack data set, accessible at (http://statland.org/R/RC/tables4R.htm) and included in the ActivStats1 CD, contains all 12,844 cases of hospital discharges in New York State in 1993. These are patients admitted for heart attack but who did not have surgery. This information is essential for the interpretation of our results, as this is a purely observational study, not a random sample or controlled experiment. The data set is neatly organized in a table format (see Table 5.1), with rows representing cases and columns representing characteristics, a standard format for many datasets. If you download and open the file in NotePad++ or Excel, you will find that the columns are separated by tabs, with missing values marked by “NA”. Four columns (DIAGNOSIS, SEX, DRG, DIED) contain nominal values, representing labels of categories. A detailed explanation of data types can be found here. DIAGNOSIS column contains codes defined in the International Classification of Diseases (IDC), 9th Edition. These are the codes that your doctors send to your insurance company for billing. The numbers, such as 41041, are actually codes for specific parts of the heart affected. Although these are numbers, but are not math-friendly. It does not make any sense to add or subtract or compute the mean. You wouldn’t average student IDs, would you? Such categorical data needs to be recognized as factors in R. Similarly, DRG column has three possible numbers, 121 for survivors with cardiovascular complications, 122 for survivors without complications, and 123 for deceased patients. Moreover, DIED are codes using 1 for deceased, and 0 for survived. Here’s a snippet of code to get you started: URL &lt;- &quot;https://raw.githubusercontent.com/gexijin/learnR/master/datasets/heartatk4R.txt&quot; heartatk4R &lt;- read.table(URL, header = TRUE, sep = &quot;\\t&quot;, colClasses = c(&quot;character&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) Table 5.1: First 15 rows of the heart attack dataset Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE 1 41041 F 122 0 4752.00 10 79 2 41041 F 122 0 3941.00 6 34 3 41091 F 122 0 3657.00 5 76 4 41081 F 122 0 1481.00 2 80 5 41091 M 122 0 1681.00 1 55 6 41091 M 121 0 6378.64 9 84 7 41091 F 121 0 10958.52 15 84 8 41091 F 121 0 16583.93 15 70 9 41041 M 121 0 4015.33 2 76 10 41041 F 123 1 1989.44 1 65 11 41041 F 121 0 7471.63 6 52 12 41091 M 121 0 3930.63 5 72 13 41091 F 122 0 NA 9 83 14 41091 F 122 0 4433.93 4 61 15 41041 M 122 0 3318.21 2 53 Ponder over these questions as you explore the dataset in Excel. What type of people are more likely to suffer from heart attacks? Who is more likely to survive a heart attack? Suppose your friend, a 65 years old lady with DIAGNOSIS code of 41081, was just admitted to hospital for heart attack. What is the odds that she survives without complication? Also, consider yourself as a CEO of an insurance company, would you like to know what types of patients rack up higher charges? Should a particular subgroup, such as men or those over 70, pay a higher premium? To answer these questions, we will: • Import data files into R • Conduct Exploratory Data Analysis (EDA) • Implement statistical modeling (regression) x &lt;- heartatk4R # Clone the data for manipulation, call it x. str(x) # View data structure and data types ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : num 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : num 79 34 76 80 55 84 84 70 76 65 ... 5.1 Begin your analysis by examining each column individually Think of data like a first date. If you are single and meet someone in a bar, you typically start with small talks and learn some basic information about him/her. Similarly, before diving into hypothesis testing or model building, get to know your data. We are going to start by examining each column individually, for numeric column, including very basic things like mean, median, ranges, distributions, and normality. This is important because sometimes the data is so skewed or far from normal distribution. In such cases, before conducting further analyses, we need to use non-parametric tests, or to transform the raw data using log transformation, or more general box-cox transformation. ::: {.exercise #unnamed-chunk-336} Perform the following analysis for the heartatk4R dataset. If you forgot the R commands, revisit our previous chapters, or ask the wise Dr. Google. Graphical EDA: Plot the distribution of charges using box plot, histogram, qqplot, lag plot, and sequence plot. Interpret your results in PLAIN English. Note that there are missing values in this column that may cause problems for some plots. You can remove missing values by defining a new variable with temp = CHARGES [ ! is.na (CHARGES) ] and then plot with temp. Quantitative EDA: Conduct normality tests and calculate confidence intervals. If the Shapiro-Wilk normality test cannot handle data points over 12,000, try other tests in the nortest library or sample randomly with temp = sample( CHARGES, 4000). ::: For categorical columns like SEX and DIAGNOSIS, we want to count different levels, and their frequencies. In addition to quantitative analysis, we’ll visualize our findings with pie charts and bar plots, using the trusty table( ) function followed by pie( ) and barplot(). barplot(table(x$DIAGNOSIS)) This generates a bar plot of counts. It could be further refined with percentages: counts &lt;- sort(table(x$DIAGNOSIS), decreasing = TRUE) # Tabulate &amp; sort percentages &lt;- 100 * counts / length(x$DIAGNOSIS) # Convert to % barplot(percentages, las = 3, ylab = &quot;Percentage&quot;, col = &quot;green&quot;) # Figure 5.1 Figure 5.1: Barplot by percentage Note that “las = 3” changes the orientation of the labels for x-axis to vertical. Try plot without it or set it to 2. You can even do all these in single line. barplot(100* sort(table(x$DIAGNOSIS), decreasing=T) / length(x$DIAGNOSIS), las = 3, ylab = &quot;Percentage&quot;, col = &quot;green&quot;) table(x$SEX) # Tally up the Ms and Fs ## ## F M ## 5065 7779 pie(table(x$SEX)) # And turn it into a pie! Yum! Figure 5.2: Pie chart of patients by SEX Exercise 5.1 Compute the count and percentage of each level of DRG by filling the blanks below. Use bar plot and pie chart similar to Figure 5.1 and Figure 5.2 to visualize. Briefly discuss your results like you’re explaining it to a five-year-old. drg &lt;- __________ # Import values of DRG counts &lt;- sort(________(drg), decreasing = TRUE) # Tabulate &amp; sort percentages &lt;- 100 * __________ # Compute the percentage and convert to % barplot(percentages, ylab = “Percentage”, col = “blue”) # Barplot _________(counts) # Pie chart 5.2 Possible correlation between two numeric columns To explore potential correlations between two numeric columns, various correlation coefficients can be employed, such as Pearson’s correlation coefficient (PPC). This is expressed by the formula: \\[r=∑_{i=1}^{n}(\\frac{x_i-\\bar{x}}{s_x}) (\\frac{y_i-\\bar{y}}{s_y})\\] where \\(x_i\\) and \\(y_i\\) represent the \\(i^{th}\\) values, \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means, and \\(s_x\\) and \\(s_y\\) are the sample standard deviations. Pearson’s correlation ranges from -1 to 1, indicating perfect negative and positive correlation respectively. Remember, negative correlations are just as important and informative as positive ones. Figure 5.3: Interpretation of Pearson’s correlation coefficient Figure 5.3 provides visual examples of Pearson’s correlation with various scatter plots. The numbers are Pearson’s correlation coefficient r. The second row illustrates that Pearson’s correlation, despite differing slopes, can be uniformly 1. This highlights that Pearson’s correlation only signifies the correlation degree, not the slope. The third row exposes a limitation: Pearson’s correlation cannot detect nonlinear correlations. Table 5.2 below gives some guidelines for interpreting Pearson’s correlation coefficient. Table 5.2: Interpretation of correlation coefficient Correlation Negative Positive - -0.09 to 0.0 0.0 to 0.09 Small -0.3 to -0.1 0.1 to 0.3 Medium -0.5 to -0.3 0.3 to 0.5 Large -1.0 to -0.5 0.5 to 1.0 There is a small, but statistically significant correlation between age and length of stay in the hospital after a heart attack. The interpretation in plain English(the explanation that could be understood by your grandmother): Older people tend to stay slightly longer in the hospital after a heart attack. cor.test(x$AGE, x$LOS) ## ## Pearson&#39;s product-moment correlation ## ## data: x$AGE and x$LOS ## t = 21.006, df = 12842, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1654881 0.1989282 ## sample estimates: ## cor ## 0.1822609 Note that the correlation coefficient r and the p value serve two different purposes: r indicates the size of effect, while p value tells statistical significance. Based on the statistic sample, p value tells how certain we are about the difference being real, namely not due to random fluctuation. A large sample size allows us to detect even minor correlations as significant. Conversely, if we only have a few observations, a large r could have large p value, hence not significant. Distinguishing between effect size and significance is vital in statistical analyses. Like many commonly-used parametric statistical methods which rely on means and standard deviations, the Pearson’s correlation coefficient is not robust, meaning its value are sensitive to outliers and can be misleading. It is also very sensitive to distribution. Non-parametric approaches typically rank original data and do calculations on the ranks instead of raw data. They are often more robust but might lack sensitivity. There are corresponding non-parametric versions for most of the parametric tests. Spearman’s rank correlation coefficient \\(\\rho\\) is a non-parametric correlation measure. The Spearman correlation coefficient ρ is often thought as the Pearson correlation coefficient for ranked variables. However, ρ is calculated more straightforwardly in practice. The n raw scores \\(X_i\\), \\(Y_i\\) are converted to ranks \\(R_{x_i}\\), \\(R_{y_i}\\), and the differences \\(d_i\\) = \\(R_{x_i}\\)-\\(R_{y_i}\\) between the ranks of each observation on the two variables are calculated. If there are no tied ranks, \\(\\rho\\) is given by:\\[ρ=1-\\frac{6∑d_{i}^{2}}{n(n_{}^{2}-1)}\\] In R, Spearman’s \\(\\rho\\) and its significance can be tested by customizing the cor.test() function: cor.test(x$AGE, x$LOS, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x$AGE and x$LOS ## S = 2.9448e+11, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.1661032 Interpretation of Spearman’s \\(\\rho\\) parallels that of Pearson’s r. The statistical significance can also be determined similarly as demonstrated above. Kendall tau rank correlation coefficient is an alternative non-parametric statistic for correlation. We already know that scatter plots are great for visualizing correlation between two numeric columns. However, with many data points, such as our dataset with over 12,000 points, understanding these plots can be challenging, especially when the data is integers and there are a lot of overlapping data points. Yes, graphics can be misleading in such situation. This is where smoothed scatter plot, which uses color densities to represent overlapping data points, become useful. plot(x$AGE, x$LOS) # Standard scatter plot smoothScatter(x$AGE, x$LOS) # Smoothed color density representation of a scatterplot Figure 5.4: Smoothed Scatter plots use colors to code for the density of data points ::: {.exercise #unnamed-chunk-345} Investigate the correlation between length of stay(LOS) and charges by filling in the blanks below. Remember to include plain English interpretation of your results even your grandpa could understand. LOS &lt;- ______________ # Load LOS data from the heart attack dataset CHARGES &lt;- _____________ # Load CHARGES data cor.test(LOS, CHARGES, method = ___________) # Conduct Pearson correlation cor.test(LOS, CHARGES, method = __________) # Conduct Spearman correlation plot(LOS, CHARGES) # Create a standard scatter plot __________(LOS, CHARGES) # Create a smoothed color density representation of a scatterplot ::: 5.3 Associations between categorical variables In the heart attack dataset, four columns—DIAGNOSIS, DRG, SEX, and DIED—contain categorical values. These columns could be associated with each other. For example, we might wonder if men and women are equally likely to survive a heart attack. Let’s start by tabulating SEX and DIED variables: counts &lt;- table(x$SEX, x$DIED) # Create a two-dimensional array of SEX and DIED counts From these counts, we create a contingency table as seen in Table 5.3: Table 5.3: A 2x2 contingency table summarizing the distribution of DIED frequency by SEX Sex DIED_0 DIED_1 F 4298 767 M 7136 643 To explore percentages of survival, we calculate: counts / rowSums(counts) ## ## 0 1 ## F 0.84856861 0.15143139 ## M 0.91734156 0.08265844 It turns out that 15.1% of females, compared to 8.27% of males, died in the hospital. This gender difference is quite a surprise to me. But could this happen just by chance? To answer this question, we turn to a statistical test. Chi-square test can be used for testing the correlation of two categorical variables. The null hypothesis here is that men and women are equally likely to die from a heart attack. chisq.test(counts) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: counts ## X-squared = 147.76, df = 1, p-value &lt; 2.2e-16 Have you seen such a tiny p-value before? Probably! It is one of the smallest non-zero values R displays for many tests. This p is definitely small! Therefore, we reject the hypothesis that the mortality rate is the same for men and women. Looking at the data, it is higher for women. The chi-square test gives accurate p-values if expected observation is greater than 5. If not, the Fisher Exact test, more computationally intense but accurate for small datasets, is advisable. The chi-square test is an approximation to the Fisher Exact test. Karl Pearson developed the chi-square approximation before we had computers. Today’s computational power makes the Fisher Exact test feasible for small datasets. You can also use the chi-square test for contingency tables that have more than two rows or two columns. For more complex contingency tables, the Chi-square approximation remains accurate if each cell’s expected observation is over 1, and less than 20% of cells have expectations under 5. Again, the Fisher Exact test can handle quite large data sets with today’s computers, without the Chi-square limitations. fisher.test(counts) # Fisher’s Exact test ## ## Fisher&#39;s Exact Test for Count Data ## ## data: counts ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4509331 0.5653197 ## sample estimates: ## odds ratio ## 0.5049435 In this case, the result of fisher’s test is the same as chi-square test. To present findings to a boss who is either stupid or too busy, you need a chart. Below, we display two types of bar plots: stacked and side by side. counts &lt;- table(x$DIED, x$SEX) barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, beside = F) Figure 5.5: Barplot illustrating correlation of two categorical variables. A. Stacked; B. Side by side To change the position of legend, add an argument such as “args.legend = list(x = ”topleft”)”, in the function barplot(). For example, we can move the legend to the top-left of Figure 5.5 A. barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = &quot;DIED&quot;, beside = F, args.legend = list(x = &quot;topleft&quot;)) Mosaic plots are another effective way to visualize proportions, where the size and color of blocks represent different combinations of categories. mosaicplot(table(x$SEX, x$DIED), color = T, main = &quot;&quot;) Figure 5.6: Mosaic plot of DIED by SEX The mosaic plot in Figure 5.6 is similar to the barplot in Figure 5.5, but the bars are stretched to the same height, and the widths are defined by proportion of Male vs. Female. The size of the four blocks represents the counts of the corresponding combination. The blocks are also color-coded for different combination. Horizontally, the blocks are divided by SEX; There are more men than women in this dataset. Vertically, the blocks are divided by DIED (1 for died in hospital); Regardless of gender, only a small proportion of patients died in hospital. We also observe higher percentage of women died in hospital — a trend that raises questions and concerns. Mosaic plots are also useful for visualizing relationships involving multiple factors. mosaicplot(table(x$SEX, x$DIED, x$DRG), color = rainbow(3), main = &quot;&quot;) Figure 5.7: Mosaic plot of three factors Here we nested the tabulate command inside the mosaic plot. As shown in Figure 5.7, we further divided each of the 4 quadrants of Figure 5.6 into three parts in red, green and blue, according to DRG codes. Interestingly,a smaller proportion of surviving males developed complications, compared with females. The Titanic dataset, built into R, offers another opportunity for mosaic plot to explore survival patterns. ? Titanic # Access information on the famous Titanic dataset mosaicplot(~ Sex + Age + Survived, data = Titanic, color = rainbow(2)) Inspect the full dataset via a mosaic plot: mosaicplot(Titanic, color = rainbow(2)) Think about the questions: Were survival rates different between men and women? Did girls and women survive by equal proportion? ::: {.exercise #unnamed-chunk-355} The DIAGNOSIS column contains IDC codes that specify the parts of the affected hearts. Use a stacked bar plot and a mosaic plot to compare DIAGNOSIS frequency differences between men and women. Based on your observation, do you think men and women are equal in the diagnoses frequencies? DIAGNOSIS &lt;- heartatk4R$DIAGNOSIS SEX &lt;- heartatk4R$SEX counts &lt;- _____________(DIAGNOSIS, SEX) # Take SEX as columns barplot(_____________________________________________________) legend(“topleft”, legend = rownames(counts), fill = rainbow(9), ncol = 3, cex = 0.75) mosaicplot(_____________________________________________) ::: 5.4 Associations between a categorical and a numeric variables Exploring associations between different types of variables, like categorical and numeric, can offer valuable insights. Questions such as “Do women stay longer in hospitals?” or “Do charges vary with different diagnoses?” can be addressed with techniques like T-tests and ANOVA, akin to our analysis of the Iris flower dataset. For visualization, boxplots are straightforward, but the ggplot2 package allows a more detailed examination of variable distributions across groups. library(ggplot2) ggplot(heartatk4R, aes(x = AGE)) + geom_histogram(aes(y = after_stat(count / sum(count) * 100), fill = factor(SEX)), binwidth = 6, colour = &quot;black&quot;) + facet_grid(SEX ~ .) + # Split a graph according to panels defined by SEX labs(y = &quot;Percent of Total&quot;, x = &quot;Age&quot;) Figure 5.8: Histogram of AGE by SEX The histograms reveal that women’s ages are more skewed to the right, indicating that women in the dataset are considerably older than men. I am surprised at first by this huge difference, as the average age of women is bigger by 11. Further research aligns with this finding that heart attack symptoms in women are often milder and less noticeable. We can further divide the population according to survival status by adding another factor: # Creating a new grouping variable &#39;GROUP&#39; as a combination of &#39;DIED&#39; and &#39;SEX&#39; heartatk4R$GROUP &lt;- paste(heartatk4R$DIED, heartatk4R$SEX, sep = &quot;-&quot;) # Plot the histogram ggplot(heartatk4R, aes(x = AGE, group = GROUP)) + geom_histogram( aes(y = after_stat(count / tapply(count, PANEL, sum)[PANEL] * 100), fill = GROUP), binwidth = 5, color = &quot;black&quot; ) + facet_grid(DIED ~ SEX) + # Split a graph according to a matrix of panels defined by DIED and SEX labs(y = &quot;Percent of Total&quot;, x = &quot;Age&quot;) Figure 5.9: Histogram of AGE by SEX and DIED This analysis shows that older patients, regardless of sex, are less likely to survive a heart attack. This trend is even more apparent in a density plot. Instead of splitting into multiple panels, the curves are overlaid. ggplot(heartatk4R, aes(x = AGE, fill = SEX)) + geom_density(alpha = .3) Figure 5.10: Density plot of AGE by SEX The density plot reaffirms the age skew observed in the histograms 5.8. Now for each gender, we further divide the patients by their survival status. ggplot(heartatk4R, aes(x = AGE, fill = DIED)) + geom_density(alpha = .3) + facet_grid(SEX ~ .) Figure 5.11: Density plot of AGE by SEX and DIED Overlaying survival curves indicates a clear age-related trend in survival rates: younger patients tend to survive for both men and women. ::: {.exercise #unnamed-chunk-356} Use the ggplot2 package to compare the length of hospital stay distributions between survivors and non-survivors. Include both histogram plot and density plot. Interpret your results. ::: ::: {.exercise #unnamed-chunk-357} Separate the above analysis by gender, as demonstrated in Figure 5.11). ::: ::: {.exercise #unnamed-chunk-358} Examine age distribution differences between survivors and non-survivors using t-tests, boxplots, histograms, and density plots. ::: ::: {.exercise #unnamed-chunk-359} Compare charge variations among different DRG codes using ANOVA, boxplots, histograms, and density plots. ::: 5.5 Associations between multiple columns The ggplot2 package can also shed light on correlations between multiple variables through multi-panel figures. ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_boxplot(color = &quot;blue&quot;) + facet_grid(SEX ~ .) Figure 5.12: Multiple boxplots of AGE by DRG and SEX Recall that 121 indicates survivors with complication, 122 for survivors with no complication, and 123 for died. This visualization suggests older patients are more likely to die in hospital and those with complications are generally older. It raises the question: Does this mean they also had longer hospital stays? ::: {.exercise #unnamed-chunk-360} Are the surviving women younger than the women who died? Similar question can be asked for men. Produce a figure to compare age distributions between spatients who died in the hospital and those who survived, separated by gender. ::: ::: {.exercise #unnamed-chunk-361} Use ggplot2 to create boxplots comparing the lengths of stay across different DRG categories, differentiated by gender. The produced plots should be similar to Figure 5.13. Offer interpretation. ::: Figure 5.13: Multiple boxplots Scatterplots offer another dimension of analysis: # Scatterplot of LOS vs. AGE ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() # Scatterplot of LOS vs. AGE, divided by SEX ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) # Scatterplot colored by DIED ggplot(heartatk4R, aes(x = AGE, y = LOS, color = DIED)) + geom_point() + facet_grid(SEX ~ .) Here we only show the figure for the third code. Note that ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) generates multiple scatter plots of LOS ~ AGE according to different values of SEX, while color = DIED adds these two color-coded scatter plots into the same figure. Figure 5.14: A scatter plot of LOS vs. AGE, using SEX and DIED as factors Figure 5.14 suggests the positive association between age and length of stay in hospital for the survivors, regardless of sex. This is a statistician’s language. Try this instead that could be understood by both the statistician and his/her grandmother: Older patients tend to stay longer in the hospital after surviving a heart attack. This is true for both men and women. Another way to visualize complex correlation is bubble plot. It’s an enhanced scatter plot, using an additional dimension of data to determine the size of the symbols. Check out the interesting video using bubble plot: http://youtu.be/jbkSRLYSojo y &lt;- x[sample(1:12844, 200), ] # Randomly sample 200 patients plot(y$AGE, y$LOS, cex = y$CHARGES / 6000, col = rainbow(2)[y$SEX], xlab = &quot;AGE&quot;, ylab = &quot;LOS&quot;) legend(&quot;topleft&quot;, levels(y$SEX), col = rainbow(2), pch = 1) Figure 5.15: Bubble plot example Figure 5.15 is a busy plot. Female patients are shown in red while males in blue. Size of the symbol is proportional to charges. So this plot offers a multi-dimensional view! Additional common methods to detect complex correlations and structures include principal component analysis (PCA), Multidimensional scaling (MDS), hierarchical clustering etc. All of these techniques we introduced so far enable us to LEARN about the dataset without any of priory hypothesis, ideas, and judgments. Many companies claim that they want to know their customers first as individuals and then do business. Same thing applies to data mining. You need to know you dataset as it is before making predictions, classifications, etc. Remember, data analysis is a conversation with your dataset: INTERACT with your data by asking questions based on domain knowledge and common sense. Then generate lots and lots of plots to support or challenge the hypothesis you may have, and be open to surprising discoveries. This approach, exemplified with the heart attack dataset, is universally applicable. Sometimes, the insights you uncover are more important than the initial objectives. Velleman, P. F.; Data Description Inc. ActivStats, 2000-2001 release.; A.W. Longman,: Glenview, IL, 2001. "],["the-heart-attack-dataset-ii.html", "Chapter 6 The heart attack dataset (II) 6.1 Scatter plot in ggplot2 6.2 Histograms and density plots 6.3 Box plots and Violin plots 6.4 Bar plot with error bars 6.5 Statistical models: Easy to Run, tricky to interpret!", " Chapter 6 The heart attack dataset (II) In this chapter, we continue to investigate the heart attack dataset. First, let’s read in the data again and transform several variables into factors like in the previous chapter. Remember, you must set the working directory to where the data file is stored on your computer. Better yet, define a project associated with a folder, it’s like giving your data a cozy home. # Set working directory. Skip if using a project. #setwd(&quot;C:/YourPathHere&quot;) df &lt;- read.table(&quot;datasets/heartatk4R.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) # Ta-da! Data read # Convert DRG, DIED, DIAGNOSIS, and SEX into factor df$DRG &lt;- as.factor(df$DRG) df$DIED &lt;- as.factor(df$DIED) df$DIAGNOSIS &lt;- as.factor(df$DIAGNOSIS) df$SEX &lt;- as.factor(df$SEX) str(df) # Double check to make sure everything looks good ## &#39;data.frame&#39;: 12844 obs. of 8 variables: ## $ Patient : int 1 2 3 4 5 6 7 8 9 10 ... ## $ DIAGNOSIS: Factor w/ 9 levels &quot;41001&quot;,&quot;41011&quot;,..: 5 5 9 8 9 9 9 9 5 5 ... ## $ SEX : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 2 2 1 1 2 1 ... ## $ DRG : Factor w/ 3 levels &quot;121&quot;,&quot;122&quot;,&quot;123&quot;: 2 2 2 2 2 1 1 1 1 3 ... ## $ DIED : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ CHARGES : num 4752 3941 3657 1481 1681 ... ## $ LOS : int 10 6 5 2 1 9 15 15 2 1 ... ## $ AGE : int 79 34 76 80 55 84 84 70 76 65 ... 6.1 Scatter plot in ggplot2 Did you know Hadley Wickham, our ggplot2 superhero, inspired by Leland Wilkinson’s “grammar of graphics”, wrote the ggplot2 package in 2005 providing a formal, structured way of visualizing data? Just like how R was originally written by Robert Gentleman and Ross Ihaka in the 1990s; Linux was developed by Linus Torvalds, a student, at the same period. A few superheroes can make computing much easier for millions of people. ggplot2 revolutionized data visualization in R. library(ggplot2) # Load the package ggplot(df, aes(x = LOS, y = CHARGES)) # Specify data frame and aesthetic mapping This line is just the opening act, setting the scene. It specifies the name of the data frame, which is the required input data type, and defines the so-called aesthetic mapping: LOS maps to x-axis while CHARGES maps to the y-axis. To bring the plot to life, we use the geom_point function to add data points, which is called geometric objects. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # Scatter plot Thus, it is a two-step process to generate a scatter plot. Initially, it may feel cumbersome with too many steps, but it is super handy to add additional features or customize the plot step by step. Let’s add a trend line with a confidence interval for good measure. ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() + stat_smooth(method = lm) As we keep adding elements to the plot, this line of code gets longer. To keep it reader-friendly, let’s split it into multiple lines as below. The “+” at the end of the line signifies that the plot is not finished yet and tell R to keep reading the next line. I often use the tab key in the second line to remind myself that it is continued from the above line. ggplot(df, aes(x = LOS, y = CHARGES)) + # Set aesthetic mapping geom_point() + # Add data points stat_smooth(method = lm) # Add trend line As you have seen from this code, it not only makes the code easier reading but also allows room for side comments for each step - a lifesaver when recycling code. Let’s customize the plot by adding additional lines of code (Figure 6.1): ggplot(df, aes(x = LOS, y = CHARGES)) + # Set aesthetic mapping geom_point() + # Add data points stat_smooth(method = lm) + # Add trend line xlim(0, 25) + # Adjust plot limits of x-axis labs(x = &quot;Length of stay&quot;, y = &quot;Charges ($)&quot;) + # Change x and y labels annotate(&quot;text&quot;, x = 3, y = 45000, label = (&quot;R = 0.74&quot;)) # Add text to plot coordinates Figure 6.1: Scatter plot using ggplot2. For more customization ideas, a simple Google search can be your guide. For example, add title to the plot by using keyword “ggplot2 add title to plot”. It is easy for ggplot2 to represent other characteristics using additional aesthetic mappings, such as linetype, color, size, fill (“inside” color). ggplot(df, aes(x = LOS, y = CHARGES)) + geom_point() # Create basic scatter plot ggplot(df, aes(x = LOS, y = CHARGES, color = DRG)) + geom_point() # Map DRG to color ggplot(df, aes(x = LOS, y = CHARGES, color = DRG, shape = SEX)) + # Map SEX to shape geom_point() Figure 6.2: Changing color and shape to represent multivariate data in ggplot2. Each step lets us reveal additional information about these patients (Figure 6.2). With ggplot2, we can visualize complex, multivariate data by mapping different columns into diverse types of aesthetics. Figure 6.1 shows a strong positive correlation between charges and length of hospital stay, which is expected as many itemized costs are billed daily. Note there are over 12,000 data points, many are plotted near or exactly at the same places, especially at the lower left corner. To decode this density, try stat_density2d(). When saving these masterpieces, remember that vector formats (like metafile) offer high resolution but can be hefty in size. If you have many such plots in a paper or thesis, your file size can be big. Consider the bitmap format for a lightweight alternative. ::: {.exercise #unnamed-chunk-370} Generate a scatter plot of age vs. charges in the heart attack dataset using ggplot2. Represent DRG with different shapes of data points and color-code data points based on diagnosis. heartatk4R &lt;- read.table(_______________________________) ggplot(heartatk4R, aes(x = _______, y = ________, color = _________, shape = _________)) + geom_point() ::: 6.2 Histograms and density plots In addition to data points(geom_points), there are many other geometric objects, such as histogram(geom_histogram), lines (geom_line), bars (geom_bar), and so on. We can use these geometric objects to generate different types of plots. Let’s start with a histogram: ggplot(df, aes(x = AGE)) + geom_histogram() Next, let’s refine it by adding a density curve (Figure 6.3): ggplot(df, aes(x = AGE, y = ..density..)) + geom_histogram(fill = &quot;cornsilk&quot;, colour = &quot;grey60&quot;, size = .2) + geom_density() Figure 6.3: Histogram with density curve. Combined density plots are useful in comparing the distribution of subgroups of data points. Start with a plot for all: ggplot(df, aes(x = AGE)) + geom_density(alpha = 0.3) # A density plot for all Add a splash of color to differentiate sex: ggplot(df, aes(x = AGE, color = SEX)) + geom_density(alpha = 0.3) # Figure 6.4A Here, we split the dataset by gender, and plotted the density distribution on the same plot. Figure 6.4A shows that age distributions are very different between men and women. Women skew rightward, averaging over a decade older than men. This is surprising, given that this dataset contains all heart attack admissions in New York state in 1993. The fill mapping changes the “inside” color: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) # Figure 6.4B This plot shows the same information. But it looks nicer, at least to me. We can also split the plot into multiple facets for even more depth: ggplot(df, aes(x = AGE, fill = SEX)) + geom_density(alpha = 0.3) + facet_grid(DRG ~.) # Figure 6.4C Figure 6.4: The art of density plots. Recall: DRG=121 represents patients who survived but developed complications; DRG=122 denotes those without complications and DRG=123 are patients who did not survive. Figure 6.4C indicates that women in all three groups are older than men. A closer look at the male patients’ age distribution across DRG facets suggests that the distribution skews to the right in deceased patients (DRG=123), indicating that older patients are less likely to survive a heart attack. Also, survivors without complications (DRG= 122) tend to be younger than those with complications. ::: {.exercise #unnamed-chunk-376} Create density plots like Figure 6.5 to compare the distribution of length of hospital stay (LOS) for patients with different DRG groups, separated by gender. Offer interpretation in the context of the dataset. Limit the x-axis to 0 to 20. ggplot(heartatk4R, aes(_________, _________)) + ______________(alpha = 0.3) + _________(________ ~ .) + xlim(________, _____) ::: Figure 6.5: Distribution of LOS by DRG categories grouped by SEX 6.3 Box plots and Violin plots In the symphony of ggplot2, boxplots and violin plots are like the string instruments - each offering a unique way to play out the data. We can follow the same rule of geometric objects to generate boxplots using geom_boxplot(), and violin plots with geom_violin(). Let’s start with a basic version. ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() # Create basic boxplot ggplot(df, aes(x = SEX, y = AGE)) + geom_boxplot() + facet_grid(DRG ~ .) # Figure 6.6A ggplot(df, aes(x = SEX, y = AGE)) + geom_violin() + facet_grid(DRG ~ .) # Figure 6.6B The violin plot reveals more details about the distribution. Essentially, violin plot is like a boxplot showing density plot’s elegance on either side of the “violin”. Time to compose your own visual masterpiece: ::: {.exercise #unnamed-chunk-379} Generate a violin plot like Figure 6.6C to compare the distribution of length of hospital stay among patients with different prognosis outcomes (DRG), categorized by gender. Interpret your result. Note the axes labels are customized. ggplot(heartatk4R, ___________________) + _____________ + ______________ + labs(x = “DRG groups”, y = “Length of Stay(days)”) + ylim(0, 20) ::: Figure 6.6: Boxplot and violin plots using ggplot2 6.4 Bar plot with error bars Curious whether patients with certain diagnosis codes stay longer or shorter in the hospital after a heart attack? We could, of course, use the aggregate function to generate a table with mean LOS by diagnosis category. aggregate(df, by = list(df$DIAGNOSIS), FUN = mean, na.rm = TRUE) ## Group.1 Patient DIAGNOSIS SEX DRG DIED CHARGES LOS AGE ## 1 41001 7130.229 NA NA NA NA 10868.030 7.775161 65.56317 ## 2 41011 6413.323 NA NA NA NA 10631.803 7.728618 65.81305 ## 3 41021 6482.808 NA NA NA NA 10666.687 7.556000 63.96000 ## 4 41031 6356.028 NA NA NA NA 9908.118 6.679715 62.93950 ## 5 41041 6259.752 NA NA NA NA 9985.722 7.307692 63.73884 ## 6 41051 6438.974 NA NA NA NA 9062.423 7.298701 67.22078 ## 7 41071 6949.627 NA NA NA NA 9750.309 7.834997 69.01996 ## 8 41081 8384.725 NA NA NA NA 9633.123 7.048780 68.04181 ## 9 41091 6165.481 NA NA NA NA 9511.093 7.625743 67.10359 We may be happy with this table and call it quits. However, tables are often not easy to interpret compared with well-crafted graphs. Instead of the aggregate function, we can use the powerful dplyr package to summarize the data and then to generate a bar plot showing both the means and standard errors. I’ve borrowed some nifty codes and ideas from R graphics cookbook and this informative website: http://environmentalcomputing.net/plotting-with-ggplot-bar-plots-with-error-bars/. library(dplyr) # Load the package To summarize data by groups/factors, the dplyr package uses a ggplot2-like grammar, where operations flow step by step. Similar to pipes in Unix, commands separated by “%&gt;%” are executed sequentially where the output of one step becomes the input of the next. The follow 6 lines form one command, consisting of three major steps. stats &lt;- df %&gt;% # Name the new data frame and source data frame group_by(DIAGNOSIS) %&gt;% # Group by diagnosis summarise(mean = mean(LOS), # Calculate group means sd = sd(LOS), # And standard deviation of each group n = n(), # Count sample size per group se = sd(LOS) / sqrt(n())) # Compute standard error of each group Here is a detailed summary by DIAGNOSIS: stats ## # A tibble: 9 × 5 ## DIAGNOSIS mean sd n se ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 7.78 5.69 467 0.263 ## 2 41011 7.73 5.34 1824 0.125 ## 3 41021 7.56 5.26 250 0.333 ## 4 41031 6.68 4.15 281 0.247 ## 5 41041 7.31 4.75 2665 0.0920 ## 6 41051 7.30 4.54 154 0.366 ## 7 41071 7.83 5.28 1703 0.128 ## 8 41081 7.05 5.32 287 0.314 ## 9 41091 7.63 5.14 5213 0.0712 Now let’s bring these statistics to life with ggplot2 (Figure 6.7): ggplot(stats, aes(x = DIAGNOSIS, y = mean)) + # Data &amp; aesthetic mapping geom_bar(stat = &quot;identity&quot;) + # Bars represent average geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) # Add error bars Figure 6.7: Average LOS by DIAGNOSIS group with error bars representing standard error Notice how we define error bars with the mean-se and mean+se referring to the two columns in the “stats” data frame. Because we have an extremely large sample size, the standard errors are relatively small. Interestingly, patients with a diagnosis of 41031 stay shorter in the hospital. A peek at the IDC code in http://www.nuemd.com tells us this code represents “Acute myocardial infarction of inferoposterior wall, initial episode of care”, which probably makes sense. As we did previously, now let’s define the LOS bars for men and women separately. First, we need to fresh stats and generate different summary statistics. stats2 &lt;- df %&gt;% group_by(DIAGNOSIS, SEX) %&gt;% # Group by both diagnosis and sex summarise(mean = mean(LOS), sd = sd(LOS), n = n(), se = sd(LOS) / sqrt(n())) The entire dataset is now divided into 18 groups according to all possible combinations of DIAGNOSIS and SEX. Each group’s LOS is summarized in terms of mean, standard deviation (sd), observations (n), and standard errors(se). Here’s the summarized data frame: stats2 ## # A tibble: 18 × 6 ## # Groups: DIAGNOSIS [9] ## DIAGNOSIS SEX mean sd n se ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 41001 F 8.74 6.03 175 0.456 ## 2 41001 M 7.20 5.41 292 0.316 ## 3 41011 F 8.41 6.23 692 0.237 ## 4 41011 M 7.31 4.66 1132 0.139 ## 5 41021 F 8.54 5.82 100 0.582 ## 6 41021 M 6.9 4.76 150 0.389 ## 7 41031 F 6.41 4.67 96 0.477 ## 8 41031 M 6.82 3.86 185 0.283 ## 9 41041 F 7.79 5.18 998 0.164 ## 10 41041 M 7.02 4.45 1667 0.109 ## 11 41051 F 7.93 5.64 69 0.679 ## 12 41051 M 6.79 3.34 85 0.362 ## 13 41071 F 8.65 5.76 727 0.214 ## 14 41071 M 7.23 4.80 976 0.154 ## 15 41081 F 7.58 5.60 130 0.492 ## 16 41081 M 6.61 5.04 157 0.402 ## 17 41091 F 8.34 5.76 2078 0.126 ## 18 41091 M 7.15 4.63 3135 0.0826 Now we are ready to generate bar plot for men and women separately using the fill aesthetic mapping: ggplot(stats2, aes(x = DIAGNOSIS, y = mean, fill = SEX)) + # Map the aesthetics geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + # Plot bars for mean of LOS labs(x = &quot;Diagnosis codes&quot;, y = &quot;Length of Stay&quot;) + # Label axes geom_errorbar(aes(ymin = mean - se, ymax = mean + se), # Add error bars position = position_dodge(.9), width = 0.2) Figure 6.8: The length of stay summarized by diagnosis and sex. Error bars represent standard error. Figure 6.9: Mean ages for patients with different diagnosis and treatment outcome. ::: {.exercise #unnamed-chunk-388} Generate Figure 6.9 and offer your interpretation. Modify both the summarizing script and the plotting script. df &lt;- heartatk4R %&gt;% _____________(DIAGNOSIS, DRG) %&gt;% # Group by both diagnosis and sex _____________(mean = mean(AGE), sd = sd(AGE), n = n(), se = sd(AGE) / sqrt(n())) ggplot(df, aes(_________________________) + # Map the aesthetics _______________(stat = “identity”, position = “dodge”) + # Plot bars _______________(x = “Diagnosis codes”, y = “Age(yrs)”) + # Label axes _______________(aes(__________________, ________________), # Add error bars position = position_dodge(.9), width = 0.2) ::: Overall, our investigation uncovers that women are much older than men when admitted to hospital for heart attack, facing higher mortality and complication rates. It is not because they develop heart attack later in life, but their symptoms are subtler and often more easily get ignored. Remember, a heart attack might not always scream through chest pain; it could sometimes masquerade as back pain, numbness in the arm, or pain in the jaw or teeth! (Warning: Statistics professor is talking about medicine!) As you could see from these plots, ggplot2 generates attractive, publication-ready graphics. Moreover, it is a relatively structured way of customizing plots. That’s why it is becoming popular among R users. Once again, there are many example codes and answered R coding questions online. Whatever you want to do, you can google it, try the example code, and modify it to fit your needs. Remember, Google is your friend for codes and solutions. And it’s all free! Enjoy coding! 6.5 Statistical models: Easy to Run, tricky to interpret! Navigating the complex world of statistical models is like solving a mystery – running the code is straightforward, but interpreting and verifying the results requires a detective’s eye! By using pair-wised correlation analysis, here is a key finding: Women experience a higher mortality rate than men from heart attack, and tend to be older than men. However, this straightforward observation masks underlying confounding effects. To unravel the mystery, we need to delineate the effects of multiple factors using multiple linear regression. Here’s how we model the hospital charges as a function of all other factors: fit &lt;- lm(CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) summary(fit) ## ## Call: ## lm(formula = CHARGES ~ SEX + LOS + AGE + DRG + DIAGNOSIS, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31486 -2453 -674 1766 32979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6863.753 314.515 21.823 &lt; 2e-16 *** ## SEXM 183.085 84.131 2.176 0.02956 * ## LOS 989.717 8.268 119.700 &lt; 2e-16 *** ## AGE -55.255 3.216 -17.180 &lt; 2e-16 *** ## DRG122 -916.531 86.851 -10.553 &lt; 2e-16 *** ## DRG123 1488.187 141.771 10.497 &lt; 2e-16 *** ## DIAGNOSIS41011 -136.097 230.312 -0.591 0.55458 ## DIAGNOSIS41021 137.118 349.948 0.392 0.69520 ## DIAGNOSIS41031 201.021 334.056 0.602 0.54735 ## DIAGNOSIS41041 -349.932 223.352 -1.567 0.11720 ## DIAGNOSIS41051 -1162.651 408.621 -2.845 0.00444 ** ## DIAGNOSIS41071 -755.549 233.214 -3.240 0.00120 ** ## DIAGNOSIS41081 -353.290 332.747 -1.062 0.28838 ## DIAGNOSIS41091 -1042.968 215.038 -4.850 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4308 on 12131 degrees of freedom ## (699 observations deleted due to missingness) ## Multiple R-squared: 0.569, Adjusted R-squared: 0.5685 ## F-statistic: 1232 on 13 and 12131 DF, p-value: &lt; 2.2e-16 In this model, one reference level for each factor(such as female) is used as a baseline for comparison, so it does not display. Interestingly, being male (SEXM), shows a marginally significant effect on charges; In detail, if everything else being equal, a male patient will incur $183 dollars more cost than female for the hospital stay. It is really small number compared to overall charges and also the p value is just marginally significant for this large sample(0.02956). However, this finding is in contrast to the t.test(CHARGES ~ SEX) shown below, where we got the opposite conclusion with much smaller p value of 4.573e-07. It is because the t-test did not account for other factors, while the regression model did. t.test(CHARGES ~ SEX, data = df) ## ## Welch Two Sample t-test ## ## data: CHARGES by SEX ## t = 5.047, df = 9355.3, p-value = 4.573e-07 ## alternative hypothesis: true difference in means between group F and group M is not equal to 0 ## 95 percent confidence interval: ## 384.9901 873.9597 ## sample estimates: ## mean in group F mean in group M ## 10260.660 9631.185 Returning to our multiple linear regression model we can see that the most pronounced effect is LOS. This is not surprising, as many hospitals have charges on daily basis. Since the p values are cutoff at 2e-16, the t value is an indicator of significance. LOS has a t value of 119.7, which is way bigger than that for all other variables. If I am an insurance company CEO, I will do anything I can to push the hospitals to discharge patients as early as possible. The coefficient tells us, for the average patient, one extra day of stay in the hospital increases the charges by 989.7 dollars. Surprisingly, age has a negative effect. Which means, when other factors are controlled, older patients are actually charged less. Good news for the older demographic - there is no reason to charge older patients more in terms of insurance premiums. Also, there is a little evidence in gender differences in charges. Compared with patients who had complications (DRG = 121, the baseline), those have no complications (DRG = 122) incurred less charges on average by the amount of $916. Sadly, patients who died(DRG = 123) are likely to be charged more. Diagnosis codes also play a role: patients with diagnosis codes 41091, 41051 and 41071 incurred less charges compared with those with 41001. Now let’s investigate mortality. It is a binary outcome, so we turn to logistic regression. fit &lt;- glm(DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial( ), data = df) summary(fit) ## ## Call: ## glm(formula = DIED ~ SEX + LOS + AGE + DIAGNOSIS, family = binomial(), ## data = df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.359769 0.263001 -20.379 &lt; 2e-16 *** ## SEXM -0.345084 0.064978 -5.311 1.09e-07 *** ## LOS -0.248553 0.009197 -27.025 &lt; 2e-16 *** ## AGE 0.081122 0.002995 27.082 &lt; 2e-16 *** ## DIAGNOSIS41011 -0.404406 0.157454 -2.568 0.01022 * ## DIAGNOSIS41021 -0.336452 0.254026 -1.324 0.18534 ## DIAGNOSIS41031 -0.784819 0.262089 -2.994 0.00275 ** ## DIAGNOSIS41041 -0.991677 0.158059 -6.274 3.52e-10 *** ## DIAGNOSIS41051 -0.235784 0.279752 -0.843 0.39932 ## DIAGNOSIS41071 -1.719416 0.178522 -9.631 &lt; 2e-16 *** ## DIAGNOSIS41081 -0.484542 0.226713 -2.137 0.03258 * ## DIAGNOSIS41091 -0.677723 0.145463 -4.659 3.18e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8889.4 on 12843 degrees of freedom ## Residual deviance: 6844.2 on 12832 degrees of freedom ## AIC: 6868.2 ## ## Number of Fisher Scoring iterations: 6 Note that DIED is coded 1 for patients died, and 0 for survivors. A negative coefficient indicates less likely to die, hence more likely to survive. Males are more likely to survive heart attack, compared with females with the same age and diagnosis. patients with longer hospital stay have better survival chances, while older patients are more likely to die. Compared with patients with diagnosis code of 41001, those diagnosed with 41071, 41041, and 41091, are more likely to survive. Now, it’s your turn to explore the factors influencing length of stay: ::: {.exercise #unnamed-chunk-392} Use multiple linear regression to investigate the factors affecting length of stay(LOS), excluding charges. Interpret your results. fit &lt;- ___________(LOS ~ SEX + AGE + DRG + DIAGNOSIS, data = ___________) ___________(fit) ::: While not part of the exercise, another query we may interested in is identifying factors contributing to complications in surviving patients. Her, focusing only on the survivors and using logistic regression would be the way to go. Remember, pair-wise examinations are crucial; including two highly correlated factors in the same model is a statistical faux pas. Happy data deciphering! "],["advanced-topics.html", "Chapter 7 Advanced topics 7.1 Introduction to R Markdown 7.2 Tidyverse 7.3 Interactive plots made easy with Plotly 7.4 Shiny Apps 7.5 Define your own function", " Chapter 7 Advanced topics 7.1 Introduction to R Markdown Animated GIF for R Markdown Let’s start learning R Markdown from its cheat sheet. Here’s a fun and simple way to start: Click the RStudio IDE and navigate to Help \\(\\rightarrow\\) Cheatsheets \\(\\rightarrow\\) R Markdown Cheat Sheet. As the cheat sheet explained, the R Markdown (.Rmd) file can combine your code and the output in one file, and produce documents in various formats, such as html, pdf, MS Word, or RTF. Figure 7.1: R Markdown Cheat Sheet Here is how to create an R Markdown file: Firstly, you need to open a new .Rmd file at File \\(\\rightarrow\\) New File \\(\\rightarrow\\). Then, in the open window, feel free to change the Title. For example, you can change the Untitled to MyFirstRmd. Third, choose the default output format from the three options: HTML, PDF or WORD. Then click “OK” . Figure 7.2: Create a new RMarkdown file Congratulations, you now have a new file named MyFirstRmd.Rmd. The first part of the file is YAML (Yet Another Markup Language) header, which is surrounded by dashes “-” . As shown in Figure 7.3, it specifies the key arguments for the document, such as title, author, date, and output format. If it says output: pdf_document, congratulations, you have chosen PDF as the default format for your create file. Figure 7.3: YAML Header The second part of an .Rmd file is called chunks of R, surrounded by three back ticks “`”. Figure 7.4 is an example of a chunk. The “nm_cars” after r in the { } is the name of the chunk. Figure 7.4: Chunk of R A chunk is a collection of r code. You can run each chunk by clicking the right arrow on the top-right or using the Ctrl+Shift+Enter shortcut. R executes the code and displays the outputs along with the code，as illustrated in Figure 7.4. ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Many chunk options are available in the R Markdown Cheat Sheet. let’s explore some common chunk options that you’ll use frequently. 1, eval = FALSE behaves like a display case, showing the code but not evaluating it. The final document will only show the code, sans results. 2, include = FALSE is like a ninja, running the code, without displaying the code or result in the final document. 3, echo = FALSE acts like a stagehand, hiding the code from the final document, and revealing only the output. Another chunk below displays only the plot in the final document. The code does not show up since the echo = FALSE parameter was added to the code chunk. 4, message = FALSE prevents messages from appearing in the final document. 5, warning = FALSE is like a “Do Not Disturb” sign, preventing warnings from showing up in the final document. 6, results = ‘hide’ is a bit of a tease, Concealing printed output. 7, fig.show = ‘hide’ is like a magician, hiding plots in sleeve and out of sight. 8, error = TRUE acts as a brave soul, allowing the render to continue even if code returns an error. The third part of an .Rmd file included the text mixed with symbols like # or *. You can type your text like how you do in MS Word. The pound symbol # implies a heading. The number of pounds indicates the heading level; It runs from 1 to 6. One # presents the first level header, two #s imply the second level header, and etc. Here is an output displaying various levels. To italicize a word, surround it with a single asterisk: *Italic* becomes Italic. For bolding, use two asterisks: **bold** turns into bold. The final step is to obtain the whole output by clicking the Knit button on the top of the .Rmd file. You will be asked to name the file and save it to your PC if it is the first time you knit the file. Follow these simple steps as instructed, your output file will magically appear on your screen. 7.2 Tidyverse Tidyverse is a collection of powerful R packages, including superheros like ggplot2, dplyr, readr, purr, tidyr, and tibble. They were all written by Hadley Wickham, a true hero in the open-source R world. Following the same design philosophy and grammar, these mighty packages are designed to make R code more readable, intuitive, and just plain fun. As they are so user-friendly, some people argue that beginners should start by learning them, instead of the base R. Now let’s explain the dplyr package in a little detail using the classic iris data set. install.packages(&quot;dplyr&quot;) library(dplyr) In dplyr, we use the pipe operator ‘%&gt;%’ like a magic wand, sending data to the next stage. This is similar to the “+” operator we used in ggplot2 but for data manipulation. For example: to create a new data frame for setosa with sepals longer than 4.0: iris %&gt;% filter(Species == &quot;setosa&quot;, Sepal.Length &gt; 4) Add a new column for the ratios of sepal length to sepal width: iris %&gt;% mutate(ratio = Sepal.Length / Sepal.Width) Sort by sepal length in ascending order: iris %&gt;% arrange(Sepal.Length) The power of dplyr is that we can connect these pipe operators to define a work flow. Imagine we’re looking for the Iris setosa flowers with the largest sepal ratio. iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% # Filter rows select(Sepal.Length, Sepal.Width) %&gt;% # Select two columns mutate(ratio = Sepal.Length / Sepal.Width) %&gt;% # Add a new column arrange(desc(ratio)) %&gt;% # Sort in descending order head() # Show top rows. No more pipes, end of sequence. ## Sepal.Length Sepal.Width ratio ## 1 4.5 2.3 1.956522 ## 2 5.0 3.0 1.666667 ## 3 4.9 3.0 1.633333 ## 4 4.8 3.0 1.600000 ## 5 4.8 3.0 1.600000 ## 6 5.4 3.4 1.588235 Here, filter( ), mutate( ) and arrange( ) are 3 magical “verbs” that operate on the data frame sequentially. head( ) is the function that only shows the top rows. Notice the pipe operator ‘%&gt;%’ at the end of each line, leading us through the enchantment sequence, making the code a delight to read. Two other useful verbs are group_by( ) and summarise( ), perfect for generating summary statistics. Below, we use group_by to split the dataset into 3 groups by the species, compute the means of sepal length and width, and then recombine. It is a “split-apply-combine” magic. iris %&gt;% group_by(Species) %&gt;% # Split by Species summarise(avgSL = mean(Sepal.Length), avgSW = mean(Sepal.Width)) %&gt;% arrange(avgSL) ## # A tibble: 3 × 3 ## Species avgSL avgSW ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 ## 2 versicolor 5.94 2.77 ## 3 virginica 6.59 2.97 Here we’ve created a bouquet of new data frames, each displaying the mean sepal length and sepal width for each species. Obviously, we can change mean( ) to many other functions. This makes it very easy to summarize large data sets. Exercise Instructions: For all the following exercises, you need to use R Markdown. Please submit both your .Rmd file and the final document in pdf format. (No answer key is required.) ::: {.exercise #unnamed-chunk-406} Fill in the blanks. Hint: Build your code piece by piece, testing each step. Use head or summary to check your work. Read in the heart attack data from chapter 4. Insert Your own Path there. Calculate the average cost per day for patients with different DIAGNOSIS codes. Focus on females older than 20 and younger than 70 who stayed in the hospital at least one day. Sort the results in descending order. Use one command with multiple steps. heartatk4R &lt;- read.csv(“______________/heartatk4R.txt”, header = TRUE, sep = “, colClasses = c(“character”, “factor”, “factor”, “factor”, “factor”, “numeric”, “numeric”, “numeric”) head(heartatk4R) library(dplyr) df &lt;- heartatk4R %&gt;% filter(SEX == “______” &amp; AGE &gt; 20 _____ AGE &lt; 70 &amp; ________) %&gt;% group_by(___________) %&gt;% ___________(CostPerDay = ___________) %&gt;% summarise(AvgCostPerDay = __________(CostPerDay, na.rm = _______)) %&gt;% ___________(-AvgCostPerDay) df ::: 7.3 Interactive plots made easy with Plotly Ever wished your ggplot2 creations could come alive? Let’s dive into ‘plotly’. install.packages(&quot;plotly&quot;) library(plotly) g &lt;- ggplot(iris, aes(Petal.Width, Petal.Length , color = Species)) + geom_point() ggplotly(g) We first generated the plot using ggplot2 and stored it in an object ‘g’, which was rendered interactively with plotly. If you mouse over the plot, the values are highlighted. Want a closer look? You can also select an area on the chart to zoom in. 7.4 Shiny Apps Recent developments in R made it easy to create interactive charts and even complex websites. Without any web development experience, I created a site entirely in R (iDEP http://ge-lab.org/idep/) to enable biologists to analyze genomic data on their own. My blog (http://gex.netlify.com) is another RStudio creation. 7.4.1 Install the Shiny package To start your own magical journey, invoke Shiny by typing this in the console: install.packages(&quot;shiny&quot;) 7.4.2 Create a Shiny web app is a piece of cake Start a new Shiny app by either using the shortcut above or navigating through File -&gt; New File -&gt; Shiny Web App from the RStudio main menu. Give your app a name, such as test1, and save it to your local PC. RStudio, being the helpful elf it is, provides a small, but functional sample app in a file named test1.app.R. Click on Run App on the top-right of the script window, a histogram will pop up. We can customize the histogram bins by moving the sliding bar, just like a DJ tweaking the soundboard. 7.4.3 Let’s play! We can further customize the outputs by editing the code. In the app.R file, there are two functions: ui() and server() . The ui() defines the user interface, and server() specifies the logic. Personalize the color by changing the col = ‘darkgray’ in line 44 to your favorite color, such as ‘green’, ‘red’, etc. For a rainbow effect, set it to col = rainbow(10) so that we can use ten colors on a rolling basis. After saving the correction, run the app again. Switch the dataset. To replace the original dataset Old Faithful Geyser to a new dataset iris, we need to modify the columns and dataset specified on line 40. Meanwhile, by updating the content after titlePanel on line 16, we get a new title. See the hignlighted code in the following graphs. Save the changes and run the app again. Figure 7.5 is the histogram of iris’s Sepal length. Figure 7.5: Output of the Sepal length in the iris Adjust default bins. The default number of bins of 30, specified at line 25, is probably too big. Let’s reduce it to 12 by setting value = 12. Save and rerun the app, now we have the histogram with 12 bins by default. Add column selection. Would it be cool if the user can choose the variable from the output? Yes, it’s possible! We can change ‘Sepal.Length’ to other columns by assigning x to other variables. To make it, add a control widget by inserting this line of code after line 20: “selectInput(”cid”, “Column”, choices = colnames(iris)),“. Do not forget the comma at the very end! Remember, it is crucial! Save and run the app. A Column control widget presents above the Number of bins. You are now able to select any of the variable in iris dataset. But wait, the histogram does not change correspondingly. This is because we have not changed the code with respect to the histogram. Take a look at the server function, on line 44, the histogram function is given as hist(x,…), where x &lt;- iris[, ‘Sepal.Length’], specified on line 40. That’s why only the histogram of Sepal length is produced. To let the histogram change according to the selection of column, we replace x &lt;- iris[, ‘Sepal.Length’] with x &lt;- iris[, input$cid ] on the line 40. Note that the cid is consistent with the inserted code selectInput( “cid”,\\(\\cdots\\)) on line 20: “sidebarPanel(selectInput(”cid”, ”Column”, choices = colnames(iris)),”. Save and rerun the code. Then select a variable from the Column, say Petal.Width, now, the histogram of Petal width shows up accordingly. ::: {.exercise #unnamed-chunk-421} What happens when you choose the last Column of “Species” in the app? Solve the issue by deleting the variable ‘Species’ from the data iris. Hint: Modify the code on line 20: selectInput(“cid”, “Column”, choices = colnames(iris)). ::: To modify the title, let’s define a main title as titl &lt;- , and then assign it to the option “main =___” in the hist() function. ::: {.exercise #unnamed-chunk-422} Change the title from “Histogram of x” to “Histogram of __” where _ is the name of the selected variable like “Sepal.Length”. Complete the question by filling in the blanks. Your output should look like the Figure 7.6. titl &lt;- paste(“Histogram of”, input$cid, sep = ” “) hist(x, breaks = bins, col = ‘green’, border = ‘white’, main = _______) ::: Figure 7.6: Histogram output Enhance your app with a more complex feature. Let’s add an approximation normal distribution line by replacing the function hist(x, breaks = bins, col = ‘green’, border = ‘white’) with the code below: h &lt;- hist(x, breaks = bins, col = rainbow(10), border = &#39;white&#39;) yfit &lt;- dnorm(bins, mean = mean(x), sd = sd(x)) yfit &lt;- yfit * diff( h$mids[1:2]) * length(x) lines(bins, yfit, col = &quot;blue&quot;) The output should mirror Figure 7.7. Figure 7.7: Output of adding an approximation normal distribution curve ::: {.exercise #unnamed-chunk-425} Fill in the blanks to solve the error message when selecting Species in the Column in this app by plotting a pie chart instead. Your pie chart should be similar to Figure 7.8. Hint steps: 1, Ensure the selectInput() line the same as its original with all columns selected. 2, Separate the variables into two groups, numeric variables (columns 1 through 4) for histogram and categorical variable (5th column) for pie plot. 3, Use ‘if()’ and ‘}else{’ to differentiate the groups. ::: Figure 7.8: Pie chart output library(shiny) # Define UI for application that draws a histogram ui &lt;- fluidPage( # Application title titlePanel(&quot;Interactive page for iris data&quot;), # Sidebar with a slider input for number of bins sidebarLayout( sidebarPanel( selectInput(&quot;cid&quot;, &quot;Select the column&quot;, choices = ___________), sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 12) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;Plot&quot;) ) ) ) # Define server logic required to draw a histogram and pie plot server &lt;- function(input, output) { output$Plot &lt;- renderPlot({ x &lt;- iris[, input$cid] # Draw the histogram with the specified number of bins for variable columns ________ (input$cid %in% colnames(iris)[______]){ bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) titl &lt;- paste(&quot;Histogram of&quot;, input$cid, sep = &quot; &quot;) hist(x, breaks = bins, col = &#39;green&#39;, border = &#39;white&#39;, main = titl) # Draw the pie plot for categorical column Species __________ count &lt;- table(x) labl &lt;- paste(names(count), &quot;\\n&quot;, count, sep = &quot;&quot;) titl &lt;- paste(&quot;Pie plot of&quot;, input$cid, sep = &quot; &quot;) pie(count, labels = labl, main = titl) } }) } 7.4.4 Publish your app Ready to share your magical Shiny app with the world? Just follow these steps to publish it online. If it’s your first time, fear not; It’s easier than baking a pie(and less messy)! Step 1: Click the “Publish” button at the top-right of the window. Then choose “ShinyApps.io” in the Connect Account window as shown in the Figure 7.9. Figure 7.9: Connect Account Step 2: Click “your account on ShinyApps” in the Connect ShinyApps.io Account window as shown in Figure 7.10. Figure 7.10: Connect ShinyApps.io Account Step 3: In the Shinyapps.io by RStudio window, hit “Sign Up”. Figure 7.11: Sign up Step 4: Type your email, password and confirm password in the window as shown in 7.12, then click “Sign Up”. Figure 7.12: Registration Step 5: In the Account Setup window (Figure 7.13), type an account name that you would like to use and click “Save”. Remember, everyone will have a unique account name. If the name is already taken (like “MyFirstShiny”), the border of the box will turn to red. You need to pick up another name and keep it cool. I named my account “stat415or515”. Figure 7.13: Account Setup Step 6: A new window will pop up, showing you how to finish the final steps. Start by installing the package rsconnect. Then click the “Copy to clipboard” button in Figure 7.14, and paste it into your console to authorize your account. Lastly, replace the path in Figure 7.15 with the path where your Shiny app is saved. Remember, the slash is forward not backward in the path. For example, my shiny app IrisData.R saved at ‘C:/Users/Documents/FirstShiny/’ is published with the following code: library(rsconnect) rsconnect::deployApp(&#39;C:/Users/Documents/FirstShiny/IrisData&#39;) Figure 7.14: Authorization steps Figure 7.15: Specify the path of your app Step 7: Find your app’s URL link at the bottom of Figure 7.16. The link to my iris data is https://stat415or515.shinyapps.io/IrisData/. Figure 7.16: Generate the link Show off your creation by sending the URL of your data to your friends or your adviser. For more detailed instructions, check out these excellent tutorials: https://docs.rstudio.com/shinyapps.io/getting-started.html https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/ Looking for solutions to these exercises? They’re hiding out at GitHub: https://github.com/gexijin/teach/blob/master/app.R 7.5 Define your own function Creating your own functions in R is like having a personal chef for your data – it cooks up exactly what you need, whenever you need it! Take the following arithmetic function as a starter: \\[f(x)=1.57345 x^3+ x^2-2x+1\\] Sure, you can compute it manually: x &lt;- 5 1.57345 * x ^ 3 + x ^ 2 - 2 * x + 1 ## [1] 212.6813 This will work, but why go through the hassle every time? Let’s define our own function without repeating same code every time: myf &lt;- function(x) { # Name the function myf y = 1.57345 * x^3 + x^2 - 2 * x + 1 # Define the function return(y) # Return the function result } Note that “{” and “}” signify the beginning and end of a block of code. “function” is the magic word, telling R that a function is going to be defined. At the end, the “return” statement reveals the desired result. Now you can call the function easily: myf(5) # Obtain the y value for x=5 ## [1] 212.6813 or myf(x = 5) ## [1] 212.6813 It even works with vectors! For example: x &lt;- - 10 : 10 # A vector with integer values from -10 to 10 myf(x) # Apply the function myf() to x ## [1] -1452.45000 -1047.04505 -724.60640 -475.69335 -290.86520 -160.68125 ## [7] -75.70080 -26.48315 -3.58760 2.42655 1.00000 1.57345 ## [13] 13.58760 46.48315 109.70080 212.68125 364.86520 575.69335 ## [19] 854.60640 1211.04505 1654.45000 plot(x, myf(x)) # Plot the function Function can handle many different calculations beyond arithmetic functions. More than just numbers, it can take in one or more various data types and return a list of complex data objects. ::: {.exercise #unnamed-chunk-435} Suppose f(x)= |x|+5x-6, where |x| means the absolute value of x. Write an R function to implement this arithmetic function. Use this function to calculate f(-4.534). Plot the function’s graph. ::: Now, let’s define a function to count even numbers in a vector. # Count the number of even integers in vec evencount &lt;- function(vec) { k &lt;- 0 # Assign 0 to a count variable k for (i in vec) { # Loop through all elements (represented by i) in vec # Values of i are actually vec[1],vec[2],... if (i %% 2 == 0) # Test if i is an even or odd number # %% is the modulo operator k &lt;- k + 1 # Add to count if the number is even } return(k) # Return the count of even numbers } x &lt;- c(2, 5, 7, 8, 14, 12, 8, 3) # Define a new vector x evencount(x) # Count even numbers in &#39;x&#39; ## [1] 5 A variable is called local variable if it is only visible within a function. For instance, k and vec are local variables to the function evencount(). They disappear after the function returns. Trying to access them outside will leave you empty-handed: k # Poof! Error: object ‘k’ not found vec # Gone with the wind! Error: object ‘vec’ not found A variable is called global variable if it is defined outside of a function and is accessible within the function. Here is an example: myfun.globle &lt;- function (x){ y &lt;- 1 return(x - 2 * y) } myfun.globle(8) # Calculate 8-2*y = 8-2*(1) = 6 ## [1] 6 y is a global variable here. The function myfun.globle2() defined below returns the same value as above. But both x and y within the parentheses (x, y=1) in function() are local variables. myfun.globle2 &lt;- function (x, y = 1){ # y is set as 1 within the function by default return(x - 2 * y) } myfun.globle2(8) # Calculate 8-2*y = 8-2*(1) = 6 ## [1] 6 ::: {.exercise #unnamed-chunk-442} Define a function which returns the count of all odd numbers in the vector x &lt;- 1:99. ::: ::: {.exercise #unnamed-chunk-443} Create a function to count the values that are less than 0 for two vectors x=rnorm(50) and y=rnorm(5000), respectively. Then craft another function to calculate the proportion of such values for x and y respectively. Compare the calculated proportions with theoretical proportion 0.5, what conclusions can you make? ::: So, except basic R skills, now you go through advance topics like R Markdown, Tidyverse, Plotly, ShinyApp, and function. Each topic is like a magical spell; use them wisely and watch your data dance to your tunes! "],["state-dataset.html", "Chapter 8 State dataset 8.1 Data Import and Manipulation 8.2 Visualizing data 8.3 Analyzing the relationship among variables 8.4 The whole picture of the data set 8.5 Linear model analysis 8.6 Conclusion", " Chapter 8 State dataset 8.1 Data Import and Manipulation Welcome to the wild world of state datasets! We’re time-traveling back to the groovy 1970s in the USA. Our mission? To explore three datasets: state.abb, state.x77, and state.region. Here’s a snappy overview: state.abb: A vector of 2-letter state name abbreviations. “Who’s CA? Oh, California!” state.x77: A 50x8 matrix containing the following statistics in its columns: Population: population estimate as of July 1, 1975 Income: per capita income (1974) Illiteracy: illiteracy (1970, percent of population) Life Exp: life expectancy in years (1969-71) Murder: murder and non-negligent manslaughter rate per 100,000 population (1976) HS Grad: percent high-school graduates (1970) Frost: mean number of days with minimum temperature below freezing (1931-1960) in capital or large city Area: land area in square miles state.region: A factor containing the regions (Northeast, South, North Central, West) that each state belongs to. To avoid a data scavenger hunt, let’s merge these three datasets into one mega data frame “sta” with 10 columns and 50 rows! tem &lt;- data.frame(state.x77) # Transform matrix into data frame sta &lt;- cbind(state.abb, tem, state.region) # Combine the three data sets colnames(sta)[1] &lt;- &quot;State&quot; # Rename first column colnames(sta)[10] &lt;- &quot;Region&quot; # Rename the 10th column head(sta) ## State Population Income Illiteracy Life.Exp Murder HS.Grad Frost ## Alabama AL 3615 3624 2.1 69.05 15.1 41.3 20 ## Alaska AK 365 6315 1.5 69.31 11.3 66.7 152 ## Arizona AZ 2212 4530 1.8 70.55 7.8 58.1 15 ## Arkansas AR 2110 3378 1.9 70.66 10.1 39.9 65 ## California CA 21198 5114 1.1 71.71 10.3 62.6 20 ## Colorado CO 2541 4884 0.7 72.06 6.8 63.9 166 ## Area Region ## Alabama 50708 South ## Alaska 566432 West ## Arizona 113417 West ## Arkansas 51945 South ## California 156361 West ## Colorado 103766 West str(sta) ## &#39;data.frame&#39;: 50 obs. of 10 variables: ## $ State : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... ## $ Population: num 3615 365 2212 2110 21198 ... ## $ Income : num 3624 6315 4530 3378 5114 ... ## $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... ## $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... ## $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... ## $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... ## $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... ## $ Area : num 50708 566432 113417 51945 156361 ... ## $ Region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 2 ... summary(sta) ## State Population Income Illiteracy ## Length:50 Min. : 365 Min. :3098 Min. :0.500 ## Class :character 1st Qu.: 1080 1st Qu.:3993 1st Qu.:0.625 ## Mode :character Median : 2838 Median :4519 Median :0.950 ## Mean : 4246 Mean :4436 Mean :1.170 ## 3rd Qu.: 4968 3rd Qu.:4814 3rd Qu.:1.575 ## Max. :21198 Max. :6315 Max. :2.800 ## Life.Exp Murder HS.Grad Frost ## Min. :67.96 Min. : 1.400 Min. :37.80 Min. : 0.00 ## 1st Qu.:70.12 1st Qu.: 4.350 1st Qu.:48.05 1st Qu.: 66.25 ## Median :70.67 Median : 6.850 Median :53.25 Median :114.50 ## Mean :70.88 Mean : 7.378 Mean :53.11 Mean :104.46 ## 3rd Qu.:71.89 3rd Qu.:10.675 3rd Qu.:59.15 3rd Qu.:139.75 ## Max. :73.60 Max. :15.100 Max. :67.30 Max. :188.00 ## Area Region ## Min. : 1049 Northeast : 9 ## 1st Qu.: 36985 South :16 ## Median : 54277 North Central:12 ## Mean : 70736 West :13 ## 3rd Qu.: 81163 ## Max. :566432 8.2 Visualizing data Let’s start by visualizing the distributions of numeric variables, where numbers transform into stories! In many cases, we want to know if our data follows a normal distribution or not so that we can decide whether some analysis methods are suitable or not. Let’s dive into the world of histograms, Q-Q plots, and the intriguing Shapiro-Wilk test. Ready? Here we go! Histogram: Does the histogram approach a normal density curve? If yes, then the variable more likely follows a normal distribution. Q-Q plot: Do the sample quantiles almost fall into a straight line? A yes here also hints a normal distribution. Shapiro-Wilk test: This is a widely used normality test. The null hypothesis is that a variable follows a normal distribution. A tiny p-value indicates a non-normality of the variable. a &lt;- colnames(sta)[2:9] # Pick up all numeric columns/variables according to the names par(mfrow = c(4, 4)) # Layout outputs in 4 rows and 4 columns for (i in 1:length(a)){ sub = sta[a[i]][, 1] # Extract corresponding variable a[i] in sta hist(sub, main = paste(&quot;Hist. of&quot;, a[i], sep = &quot; &quot;), xlab = a[i]) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, a[i], sep = &quot; &quot;)) # qqline(sub) # Add a QQ plot line. if (i == 1) { s.t &lt;- shapiro.test(sub) # Normality test for population } else { s.t &lt;- rbind(s.t, shapiro.test(sub)) # Bind a new test result to previous row } } s.t &lt;- s.t[, 1:2] # Take the first two columns of shapiro.test result s.t &lt;- cbind(a, s.t) # Add variable name for the result s.t ## a statistic p.value ## s.t &quot;Population&quot; 0.769992 1.906393e-07 ## &quot;Income&quot; 0.9769037 0.4300105 ## &quot;Illiteracy&quot; 0.8831491 0.0001396258 ## &quot;Life.Exp&quot; 0.97724 0.4423285 ## &quot;Murder&quot; 0.9534691 0.04744626 ## &quot;HS.Grad&quot; 0.9531029 0.04581562 ## &quot;Frost&quot; 0.9545618 0.05267472 ## &quot;Area&quot; 0.5717872 7.591835e-11 From the histograms and QQ plots we can see that the distributions of Population, Illiteracy, and Area are doing a leftward lean. Income and Life.Exp, however, are playing it close to normal. The shapiro tests reveal Income, Life.Exp and Frost as normal party-goers with p-values greater than 0.05, while Murder and HS.Grad are almost normal, but not quite there. Population, Illiteracy, and Area, unfortunately, didn’t get the normality invite. Now, what about the categorical variable, region? Let’s peek into how states are spread across regions. counts &lt;- sort(table(sta$Region), decreasing = TRUE) # Number of states in each region percentages &lt;- 100 * counts / length(sta$Region) barplot(percentages, ylab = &quot;Percentage&quot;, col = &quot;lightblue&quot;) text(x=seq(0.7, 5, 1.2), 2, paste(&quot;n=&quot;, counts)) # Add count to each bar Figure 8.1: State count in each region. The barplot tells us that we have relatively more states in the South(16) and fewer states in the Northeast(9). North Central and West are almost twins with 12 and 13 states respectively. Ever heard of a lollipop plot? It’s a hybrid of a scatter plot and a barplot. Here is the lollipop chart showcases the relationship between states and their population. library(ggplot2) ggplot(sta, aes(x = State, y = Population)) + geom_point(size = 3, color = &quot;red&quot;) + geom_segment(aes(x = State, xend = State, y = 0, yend = Population)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) # Rotate axis label Figure 8.2: Loppipop plot of the population in each state. Even back in the early days, California and New York were the hotspots, while South Dakota was more of a quiet retreat. More Questions to Ponder: How did the murder rate vary across states and regions in those days? What influenced it? Could other factors explain the murder rate? A choropleth map may give us an overall view. library(maps) library(ggplot2) sta$region &lt;- tolower(state.name) # Lowercase states&#39; names states &lt;- map_data(&quot;state&quot;) # Extract state data map &lt;- merge(states, sta, by = &quot;region&quot;, all.x = T) # Merge states and state.x77 data map &lt;- map[order(map$order), ] # Must order first ggplot(map, aes(x = long, y = lat, group = group)) + geom_polygon(aes(fill = Murder)) + geom_path() + scale_fill_gradientn(colours = rev(heat.colors(10))) + coord_map() + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + guides(fill = guide_legend(title = &quot;Murder Rate&quot;)) Figure 8.3: Map of murder rate distribution. A glance at the map reveals a gradient of concern: redder shapes in the South and East, suggesting higher danger, while the North Central, Northwest, and Northeast appear calmer in yellow. library(ggridges) ggplot(sta, aes(x = Murder, y = Region, fill = Region)) + geom_density_ridges() + theme_ridges() + # No color on backgroud theme(legend.position = &quot;none&quot;, # No show legend axis.title.x = element_text(hjust = 0.5), # x axis title in the center axis.title.y = element_text(hjust = 0.5)) # y axis title in the center Figure 8.4: Ridgeline plot for murder rate in each region. The ridgeline plot confirms our map’s story. South is leaning right with higher murder rates, while the other regions are more left-leaning. ::: {.exercise #unnamed-chunk-447} Similar to Figure 8.3, weave your magic to craft an Illiteracy map using state.x77 data set and give a brief interpretation. Hint: Combine state.abb and state.x77 first or use the row names of state.x77 data set directly. You may start from importing the data: tem &lt;- data.frame(state.x77) sta &lt;- cbind(state.abb, tem, state.region) colnames(sta)[10] &lt;- “Region” ::: ::: {.exercise #unnamed-chunk-448} Similar to Figure 8.4, conjure up a ridgeline plot to explore the regional distribution of Illiteracy using state.x77 and state.region data sets and interpret your figure. ::: 8.3 Analyzing the relationship among variables A scatter matrix is a pair-wise scatter plot of multiple variables presented in a matrix format. It’s perfect for eyeballing linear relationships among variables in a single plot. The correlation coefficient, ranging from -1 to 1, is the gossip scale. A -1 coefficient means two variables are in a bitter feud (think y=-x), while a 1 indicates a blossoming friendship (like y=2x+1). Now, let’s peek into the lives of our state data variables. st &lt;- sta[, 2:9] # Take numeric variables as goal matrix library(ellipse) library(corrplot) corMatrix &lt;- cor(as.matrix(st)) # Calculate correlation matrix col &lt;- colorRampPalette(c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) # 3 colors to represent coefficients -1 to 1. corrplot.mixed(corMatrix, order = &quot;AOE&quot;, lower = &quot;number&quot;, lower.col = &quot;black&quot;, number.cex = .8, upper = &quot;ellipse&quot;, upper.col = col(10), diag = &quot;u&quot;, tl.pos = &quot;lt&quot;, tl.col = &quot;black&quot;) # Mix plots of &quot;number&quot; and &quot;ellipse&quot; Figure 8.5: Corrplot for numeric variables. Look at the top-right of the correlation figure. A stark red, narrow ellipse between Murder and Life.Exp signalizes a high negative correlation. Conversely, the cozy blue showing between Murder and Illiteracy implies a high positive correlation. The red-orange hues between Murder and Frost, HS.Grad suggest a mild negative correlation. The orange shape between Murder and Income shows slight negative correlation. While light-blue tones between Murder and both Area and Population indicate a small positive correlation. The pearson and spearman correlation matrix on the bottom-left gives us the r-values between each pair of the variables, which confirm the correlation details on the top-right. Positive correlation between Murder and Illiteracy (r-value: 0.70) is like saying, “Less school, more trouble”. Negative correlations between Murder and Life.Exp, Frost (r-values: -0.78, -0.54) can be interpreted as “More mayhem, shorter lifespans”, and “Murderers dislike winter”. Fascinating, isn’t it? ::: {.exercise #unnamed-chunk-449} Similar to Figure 8.5, creat a scatter matrix among 7 variables: mpg, cyl, disp, hp, drat, wt and qsec in the data set mtcars. Give a brief interpretation of the plot. ::: Now let’s see the cluster situation of these variables. plot(hclust(as.dist(1 - cor(as.matrix(st))))) # Hierarchical clustering Figure 8.6: Cluster dendrogram for state numeric variables. The cluster Dendrogram reveals two clusters for these variables. Murder cozies up to Illiteracy mostly, and then to Population and Area. Meanwhile, HS.Grad prefers hanging out with Income mostly, then with Life.Exp and Frost. Though illiteracy and HS.Grad are in the different clusters, they share a bond: for the same state, the lower the illiteracy, the higher the high school graduation rate. An r-value of -0.66 between Illiteracy and HS.Grad in the corrplot confirms this clandestine relationship. ::: {.exercise #unnamed-chunk-450} Similar to Figure 8.6, creat a cluster dendrogram of the 7 variables: mpg, cyl, disp, hp, drat, wt and qsec in the data set mtcars. What does your output reveal? ::: Now, who’s ready to talk about illiteracy? Let’s see a density plot about the distribution of Illiteracy by region. ggplot(sta, aes(x = Illiteracy, fill = Region)) + geom_density(alpha = 0.3) Figure 8.7: Illiteracy distribution by region. Here’s a juicy tidbit: The North-Central region seems to have its illiteracy act together, mostly under 1%. While the south region has an open and bell-shaped distribution with illiteracy ranging from 0.5 to 3. Though region west has a spread out distribution too, it’s left-skewed; Most west states keep illiteracy of less than 1% of their populations. The Northeast, mostly under 1.5%, is also keeping illiteracy on a short leash. ::: {.exercise #unnamed-chunk-451} Similar to Figure 8.7, use density plot to explore the distribution of mpg by cyl in the data set mtcars. ::: Because of the relationship of Murder with both Population and Area, now We add one more column of Pop.Density in the mix, which is the population per square miles of area. sta$Pop.Density &lt;- sta$Population/sta$Area boxplot(sta$Pop.Density ~ sta$Region, xlab = &quot;Region&quot;, ylab = &quot;Population Density&quot;) Figure 8.8: Box plot of population density by region. model &lt;- aov(sta$Pop.Density ~ sta$Region, sta) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sta$Region 3 1.051 0.3502 12 6.3e-06 *** ## Residuals 46 1.343 0.0292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The box plot shows that the mean Pop.Density of Northeast is much more than the other regions, while West has the lowest mean Pop.Density. The ANOVA test with a p-value of 6.3e-06 gives us the evidence to reject the null hypothesis that the mean Pop.Densities are the same for different regions, which means at least one of the regional mean population densities is different from the others. Not all regional Pop.Densities are created equal. Scandalous! But wait, there’s more! Let’s check out Illiteracy and Murder with a dash of Population per area, and the region too. ggplot(sta, aes(x = Illiteracy, y = Murder)) + geom_point(aes(size = Pop.Density, color = Region)) + geom_smooth(method = &#39;lm&#39;,formula = y ~ x) # Add regression line Figure 8.9: Scatterplot for illiteracy and murder sized by population density and colored by region. The scatter plot shows that many Northeast states have big population densities but middle-of-the-road illiteracy rates compared with the states in the other three regions. Murder and illiteracy are positively correlated. Southern states take the spotlight with higher murder rates and illiteracy levels. Because of the high correlation of murder and Life.Exp, we will take a look at the distribution of Life.Exp. ggplot(sta, aes(x = Region, y = Life.Exp, fill = Region)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.1) Figure 8.10: Regional life expectancy. On average, the south is singing the blues with a lower life expectancy than the other three regions. North Central, on the other hand, is living its best life with the highest Life.Exp. West has extending distribution with two long tails on each end, which means some west states have very long life expectancy, while some expect short life expectancy though they are in the same region. Now, let’s talk about murder’s relationship drama with Life.Exp, HS.Grad, and Income. First we categorize the income into 5 IncomeTypes. library(dplyr) # Group income into IncomeType first sta.income &lt;- sta %&gt;% mutate(IncomeType = factor(ifelse(Income &lt; 3500, &quot;Under3500&quot;, ifelse(Income &lt; 4000 &amp; Income &gt;= 3500, &quot;3500-4000&quot;, ifelse(Income &lt; 4500 &amp; Income &gt;= 4000, &quot;4000-4500&quot;, ifelse(Income &lt; 5000 &amp; Income &gt;= 4500, &quot;4500-5000&quot;, &quot;Above5000&quot;)))))) ggplot(sta.income, aes(x = Murder, y = Life.Exp)) + geom_point(aes(shape = IncomeType, color = Region, size = HS.Grad)) + geom_smooth(method = &#39;lm&#39;, formula = y ~ x) Figure 8.11: Relationship between murder rate and life expectancy, high school graduation, and income. Murder rate is negatively correlated with Life.Exp. Those small symbols with high murder rates? They’re from the South with lower HS.Grad rates and lower Life.Exp. Left-skewing of most square and triangle symbols hints that wealthier states are often less murderous. But it’s a complex factor, some high income states with incomes above 5000 and cross symbols had high Murder rate too(close to 12). ::: {.exercise #unnamed-chunk-452} Now it’s your turn to be the director. Use a scatter plot to analyze the correlation between Illiteracy and those variables in the other cluster shown in Figure 8.6. Interpret your plot. ::: 8.4 The whole picture of the data set We’ve been analyzing the connections between murder rate and the various other variables in both clusters. It turns out that all variables are correlated, more or less. Now, let’s take a helicopter view and see the whole landscape of these variables. Ready for some heat maps and starry diagrams? library(gplots) st.matrix &lt;- as.matrix(st) # Transfer the data frame to matrix s &lt;- apply(st.matrix, 2, function(y)(y - mean(y)) / sd(y)) # Standardize data a &lt;- heatmap.2(s, col = greenred(75), # Color green red density.info = &quot;none&quot;, trace = &quot;none&quot;, scale = &quot;none&quot;, RowSideColors = rainbow(4)[sta$Region], srtCol = 45, # Column labels at 45 degree margins = c(5, 8), # Bottom and right margins lhei = c(5, 15) # Relative heights of the rows ) legend(&quot;topright&quot;, levels(sta$Region), fill = rainbow(4), cex = 0.8) # Add legend Figure 8.12: Heat map for whole state data set. Much like the cluster Dendrogram plot, Heatmap shows two clusters: Life.Exp, Income, HS.Grad, together with Frost build one cluster, while Illiteracy, Murder, Population, and Area build another cluster. Compared with other states, lots of southern states with lower Life.Exp, Income, and HS.Grad have higher Murder rates and Illiteracy(Look at Mississippi and Alabama!). On the flip side, some northern and western states, like Nebraska and South Dakota, are living a higher Life.Exp, Income, and HS.Grad but lower Area, Population, Murder, and Illiteracy. South Dakota’s income, though, is a bit green with envy. row.names(st) &lt;- sta$State stars(st, key.loc = c(13, 1.5), draw.segments = T) Figure 8.13: Segment diagram for all states. The segment Diagram reveals different facets of each state. Take South Dakota, for instance. It boasts big Frost(yellow), high Life Expectancy(blue), a solid high school graduation rate(pink) and good income(red). On the other side, it has a small area and population. Thankfully, the illiteracy and murder rate are very tiny compared with the other states. Let’s dive deeper with principal components analysis! pca = prcomp(st, scale = T) # scale = T to normalize the data pca ## Standard deviations (1, .., p=8): ## [1] 1.8970755 1.2774659 1.0544862 0.8411327 0.6201949 0.5544923 0.3800642 ## [8] 0.3364338 ## ## Rotation (n x k) = (8 x 8): ## PC1 PC2 PC3 PC4 PC5 ## Population 0.12642809 0.41087417 -0.65632546 -0.40938555 -0.405946365 ## Income -0.29882991 0.51897884 -0.10035919 -0.08844658 0.637586953 ## Illiteracy 0.46766917 0.05296872 0.07089849 0.35282802 -0.003525994 ## Life.Exp -0.41161037 -0.08165611 -0.35993297 0.44256334 -0.326599685 ## Murder 0.44425672 0.30694934 0.10846751 -0.16560017 0.128068739 ## HS.Grad -0.42468442 0.29876662 0.04970850 0.23157412 0.099264551 ## Frost -0.35741244 -0.15358409 0.38711447 -0.61865119 -0.217363791 ## Area -0.03338461 0.58762446 0.51038499 0.20112550 -0.498506338 ## PC6 PC7 PC8 ## Population 0.01065617 0.062158658 0.21924645 ## Income -0.46177023 -0.009104712 -0.06029200 ## Illiteracy -0.38741578 0.619800310 0.33868838 ## Life.Exp -0.21908161 0.256213054 -0.52743331 ## Murder 0.32519611 0.295043151 -0.67825134 ## HS.Grad 0.64464647 0.393019181 0.30724183 ## Frost -0.21268413 0.472013140 -0.02834442 ## Area -0.14836054 -0.286260213 -0.01320320 plot(pca) # Plot the amount of variance each principal components captures summary(pca) # Show the importance of the components ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.8971 1.2775 1.0545 0.84113 0.62019 0.55449 0.38006 ## Proportion of Variance 0.4499 0.2040 0.1390 0.08844 0.04808 0.03843 0.01806 ## Cumulative Proportion 0.4499 0.6539 0.7928 0.88128 0.92936 0.96780 0.98585 ## PC8 ## Standard deviation 0.33643 ## Proportion of Variance 0.01415 ## Cumulative Proportion 1.00000 percentVar &lt;- round(100 * summary(pca)$importance[2, 1:7], 0) # Compute % variances percentVar ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## 45 20 14 9 5 4 2 The first two components are the big cheeses, contributing 45% and 20% of the variance, respectively, collectively account for 65% of the overall variance. The third component, slightly less impactful,still holds 10% of the total variance. The bar plot illustrates the dominance of each individual component. library(ggfortify) row.names(sta) &lt;- sta$State autoplot(prcomp(st, scale = T), data = sta, colour = &#39;Region&#39;, shape = FALSE, label = TRUE, label.size = 3.5, loadings = TRUE, loadings.colour = &#39;blue&#39;, loadings.label = TRUE, loadings.label.size = 4, loadings.label.colour = &#39;blue&#39;) Figure 8.14: Biplot for PCA. The biplot is like our data’s own reality show, illustrating the special roles of these variables to the component of the variance. Illiteracy positively contributes to variance PC1, while Life.Exp and Frost play a contrasting negative role with PC1. Additionally, Area significantly influences the positive variance in PC2. The other four variables exhibit both sides role, contributing positively or negatively to PC1 and PC2. Drama ensues as Southern states like Louisiana (LA) and Mississippi (MS) showcase their Illiteracy and Murder rates, while North-Central states like Minnesota (MN) and North Dakota (ND) boast their high Life Expectancy and Frost. Alaska (AK) and California (CA) from the West region flaunt their Big Area feature. 8.5 Linear model analysis Ready to play detective with numbers? Let’s explore the fascinating realm of linear models to unravel the mystery behind the murder rate. We’re dropping HS.Grad and Frost out of our suspect list due to their high correlation with other variables. lm.data &lt;- sta[, c(2:6, 9:10)] lm.data &lt;- within(lm.data, Region &lt;- relevel(Region, ref = &quot;South&quot;)) # Set region South as reference model &lt;- lm(Murder ~ ., data = lm.data) summary(model) ## ## Call: ## lm(formula = Murder ~ ., data = lm.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8178 -0.9446 -0.1485 1.0406 3.5501 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.059e+02 1.601e+01 6.611 5.85e-08 *** ## Population 2.591e-04 5.439e-05 4.764 2.39e-05 *** ## Income 2.619e-04 4.870e-04 0.538 0.59362 ## Illiteracy 1.861e+00 5.567e-01 3.343 0.00178 ** ## Life.Exp -1.445e+00 2.275e-01 -6.351 1.37e-07 *** ## Area 1.133e-06 3.407e-06 0.333 0.74117 ## RegionNortheast -2.673e+00 8.020e-01 -3.333 0.00183 ** ## RegionNorth Central -7.182e-01 8.712e-01 -0.824 0.41451 ## RegionWest 2.358e-01 8.096e-01 0.291 0.77229 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.552 on 41 degrees of freedom ## Multiple R-squared: 0.852, Adjusted R-squared: 0.8232 ## F-statistic: 29.51 on 8 and 41 DF, p-value: 1.168e-14 The prime suspects in influencing the murder rate turn out to be Life Expectancy, Population, and Illiteracy, with the region playing the role of an accessory. Here’s the lowdown: for every step up in Life Expectancy, the murder rate drops by 1.445 units. Conversely, Population and Illiteracy are playing the villains, pushing up the murder rate by 0.000259 and 1.861 units, respectively. Being in the Northeast gets you a 2.673-unit markdown on murder rates. Our model is a rival Sherlock, explaining 82% of the variance in murder rates. Here’s the formula to predict the murder rate: Murder=105.9-1.445\\(*Life.Exp+0.000259*\\)Population+1.861\\(*Illiteracy-2.673*\\)RegionNortheast ::: {.exercise #unnamed-chunk-455} Your turn to wear the detective hat! Analyze a linear model for Illiteracy and interpret your result. Hint: Check the corrplot figure 8.5 and pay attention to the high correlation between murder rate and life expectancy. ::: 8.6 Conclusion -The southern region shows a higher murder rate with lower life expectancy, income, and high school graduation rate but a higher illiteracy rate. Conversely, the northern region is all about lower murder rate with higher population density, life expectancy, income, and high school graduation rate accompanied by lower illiteracy. -Fun fact: Data on life expectancy, population, illiteracy of the state in the 1970s, along with regional affiliations, can help you guess its murder rate from those days. Quite the narrative power vested in numbers, right? That’s a wrap! Remember, in the world of data, there’s always a story waiting to be told. Happy analyzing! "],["the-game-sales-dataset.html", "Chapter 9 The game sales dataset 9.1 Reading in data and managing the dataset 9.2 Visualization of categorical variables 9.3 Correlation among numeric variables 9.4 Analysis of score and count 9.5 Analysis of sales 9.6 Effect of platform type on principle components 9.7 Models for global sales 9.8 Conclusion", " Chapter 9 The game sales dataset Welcome to the fascinating spectacle of video game sales data, which was heroically downloaded from Kaggle: https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings#Video_Games_Sales_as_at_22_Dec_2016.csv. We will witness the performance of the sales across North America, Europe, Japan, and other regions, which all add up to the global sales. Our dataset is also enchanted with the critic scores, user scores, and the number of brave souls who dared to contribute to these scores. Here’s a brief overview of the dataset’s contents: Name: Name of the game. Platform: Console on which the game is running. Year_of_Release: Year of the game released. Genre: The category of the game. Publisher: The game’s champion and distributor. NA_Sales: Game sales in North America (in millions of units). EU_Sales: Game sales in the European Union (in millions of units). JP_Sales: Game sales in Japan (in millions of units). Other_Sales: Game sales in the rest of the world, i.e. Africa, Asia excluding Japan, Australia, Europe excluding the E.U. and South America (in millions of units). Global_Sales: Total sales in the world (in millions of units). Critic_Score: Aggregate score compiled by Meta critic staff. Critic_Count: The number of critics who’ve lent their voices to the Critic Score. User_Score: Score by Metacritic’s subscribers. User_Count: The number of users who weighed in with their opinions. Developer: Party responsible for creating the game. Rating: The ESRB ratings: E for “Everyone”; E10+ for “Everyone 10+”; T for “Teen”; M for “Mature”; AO for “Adults Only”; RP for “Rating Pending”; and the nostalgic K-A for “kids to adults”. After downloading the data, we transmuted N/A into NA first using Excel and saved it as a .csv file, then read it into R. We banished observations with empty strings of Rating, leaving us with 6825 observations in our “game” dataset. We observed a curious phenomenon: an abundance of sales values dared to be zero. We then transformed the sales into basic units and plus 1 to prepare for the logarithmic transformation (a wizard’s trick to normalize distributions). We also scaled the Critic Scores down by a factor of 10 to harmonize with the User Scores’ scale. 9.1 Reading in data and managing the dataset # Load the dataset tem &lt;- read.csv(&quot;datasets/video-game-sales-at-22-Dec-2016.csv&quot;) tem &lt;- na.omit(tem) # Remove NA # Keep games with ratings and remove redundant levels library(dplyr) game &lt;- tem %&gt;% filter(Rating != &quot;&quot;) %&gt;% droplevels() # Convert sales to actual figures by multiplying 1000000 # Plus one to avoid the curse of negative log (sneaky!) game$Year_of_Release &lt;- as.factor(as.character(game$Year_of_Release)) game$NA_Sales &lt;- game$NA_Sales * 1000000 + 1 game$EU_Sales &lt;- game$EU_Sales * 1000000 + 1 game$JP_Sales &lt;- game$JP_Sales * 1000000 + 1 game$Other_Sales &lt;- game$Other_Sales * 1000000 + 1 game$Global_Sales &lt;- game$Global_Sales * 1000000 + 1 # Harmonizing Critic Scores to match the User Score scale by dividing by 10 game$Critic_Score &lt;- as.numeric(as.character(game$Critic_Score)) / 10 game$User_Score &lt;- as.numeric(as.character(game$User_Score)) game$Critic_Count &lt;- as.numeric(game$Critic_Count) game$User_Count &lt;- as.numeric(game$User_Count) # Set category variables as factor variables game$Name &lt;- as.factor(game$Name) game$Platform &lt;- as.factor(game$Platform) game$Genre &lt;- as.factor(game$Genre) game$Publisher &lt;- as.factor(game$Publisher) game$Developer &lt;- as.factor(game$Developer) game$Rating &lt;- as.factor(game$Rating) # Format column names for clarity colnames(game) &lt;- c(&quot;Name&quot;, &quot;Platform&quot;, &quot;Year.Release&quot;, &quot;Genre&quot;, &quot;Publisher&quot;, &quot;NA.Sales&quot;, &quot;EU.Sales&quot;, &quot;JP.Sales&quot;, &quot;Other.Sales&quot;, &quot;Global.Sales&quot;, &quot;Critic.Score&quot;, &quot;Critic.Count&quot;, &quot;User.Score&quot;, &quot;User.Count&quot;, &quot;Developer&quot;, &quot;Rating&quot;) # Display the structure and a summary of the dataset str(game) ## &#39;data.frame&#39;: 6825 obs. of 16 variables: ## $ Name : Factor w/ 4377 levels &quot; Tales of Xillia 2&quot;,..: 4203 2055 4205 2530 4201 2533 2054 4195 1820 4196 ... ## $ Platform : Factor w/ 17 levels &quot;3DS&quot;,&quot;DC&quot;,&quot;DS&quot;,..: 13 13 13 3 13 13 3 13 15 13 ... ## $ Year.Release: Factor w/ 25 levels &quot;1985&quot;,&quot;1988&quot;,..: 15 17 18 15 15 18 14 16 19 18 ... ## $ Genre : Factor w/ 12 levels &quot;Action&quot;,&quot;Adventure&quot;,..: 11 7 11 5 4 5 7 11 4 11 ... ## $ Publisher : Factor w/ 262 levels &quot;10TACLE Studios&quot;,..: 164 164 164 164 164 164 164 164 147 164 ... ## $ NA.Sales : num 41360001 15680001 15610001 11280001 13960001 ... ## $ EU.Sales : num 28960001 12760001 10930001 9140001 9180001 ... ## $ JP.Sales : num 3770001 3790001 3280001 6500001 2930001 ... ## $ Other.Sales : num 8450001 3290001 2950001 2880001 2840001 ... ## $ Global.Sales: num 82530001 35520001 32770001 29800001 28920001 ... ## $ Critic.Score: num 7.6 8.2 8 8.9 5.8 8.7 9.1 8 6.1 8 ... ## $ Critic.Count: num 51 73 73 65 41 80 64 63 45 33 ... ## $ User.Score : num 8 8.3 8 8.5 6.6 8.4 8.6 7.7 6.3 7.4 ... ## $ User.Count : num 322 709 192 431 129 594 464 146 106 52 ... ## $ Developer : Factor w/ 1289 levels &quot;10tacle Studios, Fusionsphere Systems&quot;,..: 779 779 779 779 779 779 779 779 468 779 ... ## $ Rating : Factor w/ 7 levels &quot;AO&quot;,&quot;E&quot;,&quot;E10+&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:9826] 2 5 6 10 11 13 19 21 22 23 ... ## ..- attr(*, &quot;names&quot;)= chr [1:9826] &quot;2&quot; &quot;5&quot; &quot;6&quot; &quot;10&quot; ... summary(game) ## Name Platform Year.Release ## LEGO Star Wars II: The Original Trilogy : 8 PS2 :1140 2008 : 592 ## Madden NFL 07 : 8 X360 : 858 2007 : 590 ## Need for Speed: Most Wanted : 8 PS3 : 769 2005 : 562 ## Harry Potter and the Order of the Phoenix: 7 PC : 651 2009 : 550 ## Madden NFL 08 : 7 XB : 565 2006 : 528 ## Need for Speed Carbon : 7 Wii : 479 2003 : 498 ## (Other) :6780 (Other):2363 (Other):3505 ## Genre Publisher NA.Sales ## Action :1630 Electronic Arts : 944 Min. : 1 ## Sports : 943 Ubisoft : 496 1st Qu.: 60001 ## Shooter : 864 Activision : 492 Median : 150001 ## Role-Playing: 712 Sony Computer Entertainment: 316 Mean : 394485 ## Racing : 581 THQ : 307 3rd Qu.: 390001 ## Platform : 403 Nintendo : 291 Max. :41360001 ## (Other) :1692 (Other) :3979 ## EU.Sales JP.Sales Other.Sales Global.Sales ## Min. : 1 Min. : 1 Min. : 1 Min. : 10001 ## 1st Qu.: 20001 1st Qu.: 1 1st Qu.: 10001 1st Qu.: 110001 ## Median : 60001 Median : 1 Median : 20001 Median : 290001 ## Mean : 236090 Mean : 64159 Mean : 82678 Mean : 777591 ## 3rd Qu.: 210001 3rd Qu.: 10001 3rd Qu.: 70001 3rd Qu.: 750001 ## Max. :28960001 Max. :6500001 Max. :10570001 Max. :82530001 ## ## Critic.Score Critic.Count User.Score User.Count ## Min. :1.300 Min. : 3.00 Min. :0.500 Min. : 4.0 ## 1st Qu.:6.200 1st Qu.: 14.00 1st Qu.:6.500 1st Qu.: 11.0 ## Median :7.200 Median : 25.00 Median :7.500 Median : 27.0 ## Mean :7.027 Mean : 28.93 Mean :7.186 Mean : 174.7 ## 3rd Qu.:8.000 3rd Qu.: 39.00 3rd Qu.:8.200 3rd Qu.: 89.0 ## Max. :9.800 Max. :113.00 Max. :9.600 Max. :10665.0 ## ## Developer Rating ## EA Canada : 149 AO : 1 ## EA Sports : 142 E :2082 ## Capcom : 126 E10+: 930 ## Ubisoft : 103 K-A : 1 ## Konami : 95 M :1433 ## Ubisoft Montreal: 87 RP : 1 ## (Other) :6123 T :2377 The summary reveals the existence of games published under the same banner (or name) but across various platforms; PlayStation 2(PS2) is the Goliath among platform; Action claims the throne as the genre of choice; Electronic Arts boasts the highest frequency among publishers; Rating T and E lead the rating pack; The sales data display small minimums, quantiles, and medians but boast heroic maximums, indicating the presence of superstar games; Exceedingly big maximum User Count hints at a cult-like following for certain titles. Our pre-analysis shows that these variables are shy away from normality, notably the sales and score counts. To remedy this, we shall wield the power of logarithms! NA.Sales.Log &lt;- log(game$NA.Sales) EU.Sales.Log &lt;- log(game$EU.Sales) JP.Sales.Log &lt;- log(game$JP.Sales) Other.Sales.Log &lt;- log(game$Other.Sales) Global.Sales.Log &lt;- log(game$Global.Sales) Critic.Count.Log &lt;- log(game$Critic.Count) User.Count.Log &lt;- log(game$User.Count) Now let’s bind the log-transformed variables with their original counterparts. game.log &lt;- cbind.data.frame(NA.Sales.Log, EU.Sales.Log, JP.Sales.Log, Other.Sales.Log, Global.Sales.Log, Critic.Count.Log, User.Count.Log) game &lt;- cbind.data.frame(game, game.log) # The data we use for analysis Time to conjure histograms and QQ plots for our transformed dataset. # Select numeric columns for visualization name &lt;- colnames(game)[c(11, 13, 17:23)] par(mfrow = c(5, 4)) # Layout in 5 rows and 4 columns # Loop through each numeric variable and create a histogram and a Q-Q plot for (i in 1:length(name)){ sub &lt;- sample(game[name[i]][, 1], 5000) submean &lt;- mean(sub) hist(sub, main = paste(&quot;Hist. of&quot;, name[i], sep = &quot; &quot;), xlab = name[i]) abline(v = submean, col = &quot;blue&quot;, lwd = 1) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, name[i], sep = &quot; &quot;)) qqline(sub) if (i == 1) {s.t &lt;- shapiro.test(sub) } else {s.t &lt;- rbind(s.t, shapiro.test(sub)) } } # Extract the essence of the Shapiro-Wilk test results s.t &lt;- s.t[, 1:2] # Take first two columns of shapiro.test result s.t &lt;- cbind(name, s.t) # Add variable name for the result s.t ## name statistic p.value ## s.t &quot;Critic.Score&quot; 0.9639289 1.340987e-33 ## &quot;User.Score&quot; 0.9142633 1.176085e-46 ## &quot;NA.Sales.Log&quot; 0.6501188 1.278253e-72 ## &quot;EU.Sales.Log&quot; 0.7139223 1.531191e-68 ## &quot;JP.Sales.Log&quot; 0.6194571 2.339768e-74 ## &quot;Other.Sales.Log&quot; 0.7094548 7.506174e-69 ## &quot;Global.Sales.Log&quot; 0.9970272 2.198802e-08 ## &quot;Critic.Count.Log&quot; 0.9772805 1.537995e-27 ## &quot;User.Count.Log&quot; 0.9411244 1.029132e-40 The histograms and QQ plots of these original sales are not abnormally distributed, but the log-transformed sales data now resemble much clearer and close to normal distributions, especially the log value of global sales. Even though the Shapiro test, with a p-value less than 0.05, seems to dispel the illusion of normality, it’s nearly a portrait of normality. Perhaps the absentee values are the reason for the abnormality of the raw sales data. Our analytic crystal ball will gaze more intently upon the log of global sales. The histograms and QQ plots also suggest that the two scores and log-transformed counts are doing a tantalizing tango with normal distribution. Despite the Shapiro-Wilk test’s naysaying, we’ll proceed under the assumption that they are normally distributed for our analysis. In this realm of data, there lie intriguing enigmas: the distribution dance of global versus regional sales; the web of relationships between them; the mystical connection between critic and user scores along with their respective counts; and whether these scores and counts cast the most potent spells on sales, or if other arcane elements such as genre, rating, platform, and publisher have a part to play in this grand sales ritual. Now let’s wave our wands for some visual magic! 9.2 Visualization of categorical variables To simplify platform analysis, we regroup platform into a new variable called Platform.type. # Regroup platform into Platform.type categories pc &lt;- c(&quot;PC&quot;) xbox &lt;- c(&quot;X360&quot;, &quot;XB&quot;, &quot;XOne&quot;) nintendo &lt;- c(&quot;Wii&quot;, &quot;WiiU&quot;, &quot;N64&quot;, &quot;GC&quot;, &quot;NES&quot;, &quot;3DS&quot;, &quot;DS&quot;) playstation &lt;- c(&quot;PS&quot;, &quot;PS2&quot;, &quot;PS3&quot;, &quot;PS4&quot;, &quot;PSP&quot;, &quot;PSV&quot;) game &lt;- game %&gt;% mutate(Platform.type = ifelse(Platform %in% pc, &quot;PC&quot;, ifelse(Platform %in% xbox, &quot;Xbox&quot;, ifelse(Platform %in% nintendo, &quot;Nintendo&quot;, ifelse(Platform %in% playstation, &quot;Playstation&quot;, &quot;Others&quot;))))) library(ggplot2) ggplot(game, aes(x = Platform.type)) + geom_bar(fill = &quot;blue&quot;) Figure 9.1: Bar plot of platform type. The bar plot regales us with a tale of platform dominance: Playstation leads the charge, followed by Xbox and Nintendo, with the “Others” trailing behind. The realm of genres is a diverse landscape, but let’s see who really rules the land. dat &lt;- data.frame(table(game$Genre)) dat$fraction &lt;- dat$Freq / sum(dat$Freq) dat &lt;- dat[order(dat$fraction), ] dat$ymax &lt;- cumsum(dat$fraction) dat$ymin &lt;- c(0, head(dat$ymax, n = -1)) names(dat)[1] &lt;- &quot;Genre&quot; library(ggplot2) ggplot(dat, aes(fill = Genre, ymax = ymax, ymin = ymin, xmax = 4, xmin = 3)) + geom_rect(colour = &quot;grey30&quot;) + # Background color coord_polar(theta = &quot;y&quot;) + # Coordinate system to polar xlim(c(0, 4)) + labs(title = &quot;Ring plot for Genre&quot;, fill = &quot;Genre&quot;) + theme(plot.title = element_text(hjust = 0.5)) The Genre ring plot crowns Action, Sports, and Shooter as the mightiest triumvirate, with Action occupies nearly a quarter of genre kingdom. Combined, these three genres account for more than half the realm. Puzzle, Adventure, and Stratege have relatively less count. Now let’s consolidate the rare ratings AO, RP, and K-A under “Others” due to their sparse data points. # Consolidate Ratings into Rating.type rating &lt;- c(&quot;E&quot;, &quot;T&quot;, &quot;M&quot;, &quot;E10+&quot;) game &lt;- game %&gt;% mutate(Rating.type = ifelse(Rating %in% rating, as.character(Rating), &quot;Others&quot;)) Let’s slice up the ratings pie chart of gamers’ preferred flavors. counts &lt;- sort(table(game$Rating.type), decreasing = TRUE) # Rename the names of counts for detail information names(counts) &lt;- c(&quot;T - Teen&quot;, &quot;E - Everyone&quot;, &quot;M - Mature&quot;, &quot;E10+ - Everyone 10+&quot;, &quot;Others&quot;) pct &lt;- paste(round(counts/sum(counts) * 100), &quot;%&quot;, sep = &quot; &quot;) lbls &lt;- paste(names(counts), &quot;\\n&quot;, pct, sep = &quot; &quot;) # Rating information and percentage pie(counts, labels = lbls, col = rainbow(length(lbls)), main=&quot;Pie Chart of Ratings with sample sizes&quot;) The pie chart reveals that the popular ratings are T, E, M, and E10+, according to the order. While “Others” claims but a sliver of the pie. library(ggmosaic) library(plotly) p &lt;- ggplot(game) + geom_mosaic(aes(x = product(Rating.type), fill = Platform.type), na.rm = TRUE) + labs(x = &quot;Rating Type&quot;, y = &quot;Platform Type&quot;, title=&quot;Mosaic Plot&quot;) + theme(axis.text.y = element_blank()) ggplotly(p) Figure 9.2: Mosaic plot between platform type and rating type. Same as we noticed previously, the mosaic plot reiterates that ‘Others’ is barely visible due to its minuscule size. For all platform and rating combinations, Playstation dominates the platform ratings for all other three different rating types except for Everyone 10 plus. Nintendo captures the heart of the “Everyone 10+” category. It’s the second most popular platform for rating Everyone. Xbox holds steady as the runner-up in Mature and Teenage categories, and holds the third favorite platform for rating Everyone and Everyone 10 plus. In the category of ‘Others’, games rated ‘Everyone’ are the chosen ones. ::: {.exercise #unnamed-chunk-464} Download the game sales dataset and clean the data as previously outlined at the beginning of this chapter. Then, produce a mosaic plot between genre and rating. Briefly interpret your mystical creation. ::: 9.3 Correlation among numeric variables st &lt;- game[, c(11, 13, 17:23)] # Select the numeric variables of interest st &lt;- na.omit(st) library(ellipse) library(corrplot) corMatrix &lt;- cor(as.matrix(st)) # Correlation matrix col &lt;- colorRampPalette(c(&quot;#7F0000&quot;, &quot;red&quot;, &quot;#FF7F00&quot;, &quot;yellow&quot;, &quot;#7FFF7F&quot;, &quot;cyan&quot;, &quot;#007FFF&quot;, &quot;blue&quot;, &quot;#00007F&quot;)) corrplot.mixed(corMatrix, order = &quot;AOE&quot;, lower = &quot;number&quot;, lower.col = &quot;black&quot;, number.cex = .8, upper = &quot;ellipse&quot;, upper.col = col(10), diag = &quot;u&quot;, tl.pos = &quot;lt&quot;, tl.col = &quot;black&quot;) Figure 9.3: Corrplot among numeric variables. With high r-values of 0.75, 0.65, 0.52, and 0.42 between the log-transformed values of Global.Sales and regional sales, We’ll earmark Global.Sales.Log as our target for later romance with other variables. Also, regional sales are positively flirting with one another. User Score is sweet to Critic Score with an r of 0.58. The correlation between User Count log and User Score stays relatively low. plot(hclust(as.dist(1 - cor(as.matrix(st))))) # Hierarchical clustering Figure 9.4: Exercise dendrogram for numeric variables. In this dendrogram, all log values of sales except JP.Sales.Log build one cluster like best friends; Scores, log values of counts, along with JP.Sales form their own clique. In the first cluster, Other.Sales.Log is whispering closest to Global.Sales.Log, followed by NA.Sales.Log, with EU.Sales.Log joining the fray. 9.4 Analysis of score and count library(ggpmisc) # Package for function stat_poly_eq(), adding equations to plots formula &lt;- y ~ x # A scatter plot that reveals the hidden dance between User Score and Critic Score p1 &lt;- ggplot(game, aes(x = User.Score, y = Critic.Score)) + geom_point(aes(color = Platform), alpha = .8) + geom_smooth(method = &#39;lm&#39;, se = FALSE, formula = formula) + # Add regression line theme(legend.position = &quot;none&quot;) + stat_poly_eq(formula = formula, # Add regression equation and R square value eq.with.lhs = &quot;italic(hat(y))~`=`~&quot;, # Add ^ on y aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;*plain(\\&quot;,\\&quot;)~&quot;)), label.x.npc = &quot;left&quot;, label.y.npc = 0.9, # Position of the equation label parse = TRUE) # Output.type as &quot;expression&quot; # Density plot - where scores show off their curves p2 &lt;- ggplot() + geom_density(data = game, aes(x = Critic.Score), color = &quot;darkblue&quot;, fill = &quot;lightblue&quot;) + geom_density(data = game, aes(x = User.Score), color = &quot;darkgreen&quot;, fill = &quot;lightgreen&quot;, alpha=.5) + labs(x = &quot;Critic.Score-blue, User.Score-green&quot;) # Combine the plots for a double feature library(gridExtra) grid.arrange(p1, p2, nrow = 1, ncol = 2) Figure 9.5: Scatter and density plot for critic score and user score. A positive correlation waltzes between Critic.Score and User.Score. It seems the critics are a tough crowd, scoring on average lower than users. t.test(game$Critic.Score, game$User.Score) ## ## Welch Two Sample t-test ## ## data: game$Critic.Score and game$User.Score ## t = -6.5463, df = 13629, p-value = 6.108e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2058518 -0.1109834 ## sample estimates: ## mean of x mean of y ## 7.027209 7.185626 T-test with a p-value of much less than 0.05 suggests to accept the alternative hypothesis with 95% confidence: the difference between critic and user scores is not by mere chance. The mean critic score prances at 7.03, while the mean user score frolics slightly higher at 7.19. library(hexbin) # For hexagonal wizardry # Binhex plot for Critic Count Log p1 &lt;- ggplot(game, aes(x = Critic.Count.Log, y = Critic.Score)) + stat_binhex() + # Bin 2d plane into hexagons scale_fill_gradientn(colours = c(&quot;black&quot;, &quot;red&quot;), name = &quot;Frequency&quot;) # Adding a custom continuous color palette # Binhex plot for User Count Log p2 &lt;- ggplot(game, aes(x = User.Count.Log, y = User.Score)) + stat_binhex() + scale_fill_gradientn(colours = c(&quot;black&quot;, &quot;red&quot;), name = &quot;Frequency&quot;) # Color legend # Show both hexes side by side grid.arrange(p1, p2, nrow = 1, ncol = 2) Figure 9.6: Binhex plot for critic count and user count. Critic.Score and Critic.Count.Log share a decent correlation, with an r-value of 0.41 noted earlier, though Critic.Count.Log doesn’t have an overall impact on Critic.Score. On the other canvas, User.Score appears to maintain its independence from User.Count.Log. ::: {.exercise #unnamed-chunk-466} Use ggplot2 package to conjure up a scatter plot with a smooth line between Global_Sales and NA_Sales. In plain, mortal language, explain what you find in the plot. ::: ::: {.exercise #unnamed-chunk-467} Employ a density plot to illustrate the relationship among Global_Sales, NA_Sales, EU_Sales, JP_Sales, and Other_Sales. Interpret your plot. ::: 9.5 Analysis of sales With an eclectic collection of plots and analysis, we delve into the intricacies of sales through the prisms of year of release, region, rating, genre, and more. 9.5.1 By Year.Release # Prepare the data Year.Release &lt;- game$Year.Release counts &lt;- data.frame(table(Year.Release)) p &lt;- game %&gt;% select(Year.Release, Global.Sales) %&gt;% group_by(Year.Release) %&gt;% summarise(Total.Sales = sum(Global.Sales)) q &lt;- cbind.data.frame(p, counts[2]) # Add counts to data frame names(q)[3] &lt;- &quot;count&quot; q$count &lt;- as.numeric(q$count) # Plot the data ggplot(q, aes(x = Year.Release, y = Total.Sales, label = q$count)) + geom_col(fill = &quot;green&quot;) + geom_point(y = q$count * 500000, size = 3, shape = 21, fill = &quot;Yellow&quot; ) + geom_text(y = (q$count + 50) * 500000) + # Position of the text: count of games each year theme(axis.text.x = element_text(angle = 90), panel.background = element_rect(fill = &quot;purple&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + scale_x_discrete(&quot;Year of Release&quot;, labels = as.character(Year.Release), breaks = Year.Release) Figure 9.7: Total global sales and games released by year. The histogram of total sales illustrates that before 1996, game sales were akin to a lonely desert, only one game was released for each year. Between 1996 and 2000, like a slow-blooming flower, sales and game releases gradually climbed. After that, we witnessed a gold rush in sales and game releases, peaking in 2008. Following this golden year, both total sales and count of games went downhill. 9.5.2 By Region library(reshape2) game %&gt;% select(Year.Release, NA.Sales.Log, EU.Sales.Log, JP.Sales.Log, Other.Sales.Log, Global.Sales.Log) %&gt;% melt(id.vars = &quot;Year.Release&quot;) %&gt;% # Stacks other columns into &quot;Year.Release&quot; group_by(Year.Release, variable) %&gt;% summarise(total.sales = sum(value)) %&gt;% ggplot(aes(x = Year.Release, y = total.sales, color = variable, group = variable)) + geom_point() + geom_line() + labs(x = &quot;Year Release&quot;, y = &quot;Total Sales Log Value&quot;, color = &quot;Region&quot;) + theme(axis.text.x = element_text(angle = 90), panel.background = element_rect(fill=&quot;pink&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 9.8: Year wise log global sales by region. The trajectories of log sales in North America, Europe, and Other regions seem to be singing in harmony, while Japan performs its own unique symphony, reaffirming the conclusions drawn from our earlier cluster analysis. 9.5.3 By Rating game$Rating.type &lt;- as.factor(game$Rating.type) x &lt;- game[, c(6:10)] # Sales across different regions matplot(t(x), type = &quot;l&quot;, col = rainbow(5)[game$Rating.type]) legend(&quot;center&quot;, levels(game$Rating.type), fill = rainbow(5), cex = 0.8, pt.cex = 1) text(c(1.2, 2, 3, 3.9, 4.8), 80000000, colnames(x)) Figure 9.9: Sales by rating type. The line graph regales us with the tale of an “E for Everyone” title, predominantly purchased in North America and Europe, soaring to the lofty heights of 80 million in global sales—with North America accounting for half. A quick perusal of the data reveals the star of the show to be “Wii Sports” from 2006. Additionally, Mature games are popular in North America(green), Everyone games(red) have good sales in Europe, while Japan holds a torch for Teen(purple) and Everyone(red) games. The “Other” region samples a medley of ratings, not playing favorites. 9.5.4 By Genre game %&gt;% select(Year.Release, Global.Sales.Log, Genre) %&gt;% group_by(Year.Release, Genre) %&gt;% summarise(Total.Sales.Log = sum(Global.Sales.Log)) %&gt;% ggplot(aes(x = Year.Release, y = Total.Sales.Log, group = Genre, fill = Genre)) + geom_area() + theme(legend.position = &quot;right&quot;, axis.text.x = element_text(angle = 90), panel.background = element_rect(fill = &quot;blue&quot;), panel.grid.major = element_blank(), panel.grid.minor=element_blank()) Figure 9.10: Year wise log global sales by Genre. Behold the tapestry of sales across genres, with 2007-2009 as the golden era—years when the sales towered above 7000 (log value) annually. Action and sports genres have reigned supreme over the two-decade span, while adventure, puzzle, and strategy genres remained relatively niche. 9.5.5 by Score p1 &lt;- ggplot(game, aes(x = Critic.Score, y = Global.Sales.Log)) + geom_point(aes(color = Genre)) + geom_smooth() # Add a regression line p2 &lt;- ggplot(game, aes(x = User.Score, y = Global.Sales.Log)) + geom_point(aes(color = Rating)) + geom_smooth() grid.arrange(p1, p2, nrow = 1, ncol = 2) Figure 9.11: Global sales in relation to critic and user scores. Regardless of Genre and Rating, a positive trend is noticeable between the Critic Score and the logarithm of Global Sales. Especially, when the Critic Score surpasses 9, Global Sales logarithm escalates sharply. In contrast, the logarithm of Global Sales increases at a snail’s pace in response to User Scores. # Abbreviate a particularly lengthy game title game$Name &lt;- gsub(&quot;Brain Age: Train Your Brain in Minutes a Day&quot;, &quot;Brain Age: Train Your Brain&quot;, game$Name) # Analyze and plot the top 20 games by total sales p1 &lt;- game %&gt;% select(Name, User.Score, Critic.Score, Global.Sales) %&gt;% group_by(Name) %&gt;% summarise(Total.Sales = sum(Global.Sales), Avg.User.Score = mean(User.Score), Avg.Critic.Score = mean(Critic.Score)) %&gt;% arrange(desc(Total.Sales)) %&gt;% head(20) # Create a bar chart with accompanying score lines ggplot(p1, aes(x = factor(Name, levels = Name))) + geom_bar(aes(y = Total.Sales/10000000), stat = &quot;identity&quot;, fill = &quot;green&quot;) + geom_line(aes(y = Avg.User.Score, group = 1, colour = &quot;Avg.User.Score&quot;), size = 1.5) + geom_point( aes(y = Avg.User.Score), size = 3, shape = 21, fill = &quot;Yellow&quot; ) + geom_line(aes(y = Avg.Critic.Score, group = 1, colour = &quot;Avg.Critic.Score&quot;), size = 1.5) + geom_point(aes(y = Avg.Critic.Score), size = 3, shape = 21, fill = &quot;white&quot;) + theme(axis.text.x = element_text(angle = 90, size = 8)) + labs(title = &quot;Top Global Sales Games with Scores&quot;, x = &quot;Top-selling games&quot; ) + theme(plot.title = element_text(hjust = 0.5)) Among these 20 top-selling games, the first two games, ‘Wii Sports’ and ‘Grand Theft Auto V’ tower over their competitors in sales. For a majority of games, the average critic scores are higher than the average user scores, echoing the findings of our density plot in Figure 9.5. “Call of Duty” titles, however, seem to be in the users’ bad books, fetching notably lower average user scores compared to other top-selling games. 9.5.6 By Rating &amp; Genre &amp; Critic score # Aggregate and summarize data for plotting p1 &lt;- game %&gt;% select(Rating.type, Global.Sales, Genre, Critic.Score) %&gt;% group_by(Rating.type, Genre) %&gt;% summarise(Total.Sales = sum(Global.Sales) / 10^8, Avg.Score = mean(Critic.Score)) p2 &lt;- p1 %&gt;% group_by(Genre) %&gt;% summarise(Avg.Critic.Score = mean(Avg.Score)) # Create a bar chart with overlaid score lines ggplot() + geom_bar(data = p1, aes(x = Genre, y = Total.Sales, fill = Rating.type), stat = &quot;Identity&quot;, position = &quot;dodge&quot;) + geom_line(data = p2, aes(x = Genre, y = Avg.Critic.Score, group = 1, color = &quot;Avg.Critic.Score&quot;), size = 2) + geom_point(data = p2, aes(x = Genre, y = Avg.Critic.Score, shape = &quot;Avg.Critic.Score&quot;), size = 3, color = &quot;Blue&quot;) + scale_colour_manual(&quot;Score&quot;, breaks = &quot;Avg.Critic.Score&quot;, values = &quot;yellow&quot;) + scale_shape_manual(&quot;Score&quot;, values = 19) + # Add scale for line and point plot theme(axis.text.x = element_text(angle = 90), legend.position = &quot;bottom&quot;, panel.background = element_rect(fill = &quot;black&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 9.12: Total sales for genre and rating in accordance with critic score. Everyone*sports games are so popular, knocking it out of the sales park, with the critics’ nod of approval. Not to be outdone, ‘Mature’ rated Action and Shooter games follow closely, with a respectable critic score to match. An overlay of average critic scores on the bar chart reveals that Action and Role-Playing games lead the critic scores, while Fighting, adventure, and racing games get relatively lower average critic scores. However, higher critic scores do not always translate to higher sales; for example, Role-Playing games have higher critic scores but do not outsell Action games. Puzzle and strategy do sell less compared with other genres, though their critic scores are above the average. 9.5.7 By Platform library(viridis) library(scales) p &lt;- game %&gt;% group_by(Platform.type, Year.Release) %&gt;% summarise(total = sum(Global.Sales)) p$Year.Release. &lt;- as.numeric(as.character(p$Year.Release)) ggplot(p, aes(x = Year.Release., fill = Platform.type)) + geom_density(position = &quot;fill&quot;) + labs(y = &quot;Market Share&quot;) + scale_fill_viridis(discrete = TRUE) + scale_y_continuous(labels = percent_format()) Figure 9.13: Yearly market share by platform type. It was a lonely landscape for PC and PlayStation before 1990, with PC reigning unchallenged in the gaming kingdom. As the clock ticked past 1995, newcomers Nintendo and Xbox burst onto the scene, hungrily carving out hefty slices of the market pie. Other platforms popped up like mushrooms in the early ’90s, only to vanish into the annals of gaming history after a brief 20-year cameo. Fast forward to 2010, and it’s a four-way dance-off with Nintendo, PC, PlayStation, and Xbox maintaining a harmonious market share equilibrium. # Compute 1-way ANOVA test for log value of global sales by Platform Type model &lt;- aov(Global.Sales.Log ~ Platform.type, data = game) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Platform.type 4 1283 320.8 181.2 &lt;2e-16 *** ## Residuals 6820 12077 1.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 tukey &lt;- TukeyHSD(model) par(mar = c(4, 10, 2, 1)) plot(tukey, las = 1) The ANOVA test confirms significant sales differences among platform types. Delving deeper, The Tukey post-hoc dance shows everyone stepping on its own toes, indicating notable differences across the board—except for Xbox versus Nintendo, and others versus Nintendo, who seem to be in step with one another, sharing a similar sales rhythm. game$Platform.type &lt;- as.factor(game$Platform.type) ggplot(game, aes(x = Platform.type, y = Global.Sales.Log, fill = Rating.type)) + geom_boxplot() Figure 9.14: Global sales log by platform and rating type. In the global sales log arena, PCs tend to play the underdog. In contrast, PlayStation and Xbox punch above their weight, landing higher median sales across various rating categories. Titles rated ‘Everyone’ seem to be crowd-pleasers on all platforms, while the ‘Mature’ category particularly flourishes on PC, PlayStation, and Xbox. ggplot(game, aes(Critic.Score, Global.Sales.Log, color = Platform.type)) + geom_point() + facet_wrap(~ Genre) Figure 9.15: Global sales log correlated with critic score across different platforms and genres. Most genre plots in Figure 9.15 illustrate a universal truth: there is a positive correlation between Global.Sales.Log and Critic Score, the higher the critic score, the better the global sales log value. Most puzzle games were from Nintendo, while lots of stratege games are PC. For other genres, all platforms are distributed relatively evenly. PC sat in the corner of lower market share in different genres, while Nintendo’s sporty, racing, platform, and misc games, Playstation’s action, fighting, and racing games, Xbox’s misc, action, and shooter games shined with knockout global sales logs. 9.6 Effect of platform type on principle components To examine the influence of various gaming platforms on the variance in game sales, a principal component analysis (PCA) was conducted. When performing a PCA, it’s essential to scale the data, especially if the variables are measured on different scales. st &lt;- game[, c(11, 13, 17:23)] pca &lt;- prcomp(st, scale = T) # Normalization activated with scale = TRUE percentVar &lt;- round(100 * summary(pca)$importance[2, 1:9], 0) # Compute % variances pcaData &lt;- as.data.frame(pca$x[, 1:2]) # Extract 1st and 2nd PC values pcaData &lt;- cbind(pcaData, game$Platform.type) # Add platform type for clustering colnames(pcaData) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Platform&quot;) ggplot(pcaData, aes(PC1, PC2, color = Platform, shape = Platform)) + geom_point(size = 0.8) + xlab(paste0(&quot;PC1: &quot;, percentVar[1], &quot;% variance&quot;)) + # x-axis label ylab(paste0(&quot;PC2: &quot;, percentVar[2], &quot;% variance&quot;)) + # y-axis label theme(aspect.ratio = 1) # Width and height ratio Figure 9.16: PCA plot colored by platform type. Each platform—PC, Xbox, Playstation, and Nintendo—claims its territory on the PCA chart, demonstrating a unique role in the variance components of PC1 and PC2, showcasing its distinct influence on the gaming market’s variability. library(ggfortify) set.seed(1) autoplot(kmeans(st, 3), data = st, label = FALSE, label.size = 0.1) Figure 9.17: K-means clustering on PCA components using ggfortify. Linking this with the PCA Figure 9.16, we discover the 4 main actors: Playstation, Xbox, Nintendo, and PC. The first cluster is formed by all these four gaming giants. The second cluster is mostly comprised of Xbox, Nintendo and Playstation, while PC and Playstation dominate the third cluster. 9.7 Models for global sales To manage the complexity of publisher and developer data, and to sidestep collinearity, we’ve streamlined the number of publisher levels to 12, relegating the rest to “Others.” We rely on critic scores over user scores, given their stronger correlation, and utilize the log value of user score count for its better relationship with global sales. Other sales log variables are omitted to avoid redundancy. Here’s how we prepare our data: # Re-categorize publishers into 13 groups Publisher. &lt;- head(names(sort(table(game$Publisher), decreasing = TRUE)), 12) game &lt;- game %&gt;% mutate(Publisher.type = ifelse(Publisher %in% Publisher., as.character(Publisher), &quot;Others&quot;)) game.lm &lt;- game[, c(3:4, 11, 21, 23:26)] # Select relevant columns for linear model Now, let’s build a linear model to understand the factors affecting global sales: model &lt;- lm(Global.Sales.Log ~ ., data = game.lm) summary(model) ## ## Call: ## lm(formula = Global.Sales.Log ~ ., data = game.lm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9221 -0.5558 0.0254 0.5789 4.1009 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 10.187752 0.922104 11.048 ## Year.Release1988 -3.088268 1.300919 -2.374 ## Year.Release1992 -1.796072 1.300851 -1.381 ## Year.Release1994 2.930510 1.302000 2.251 ## Year.Release1996 1.488582 0.987461 1.507 ## Year.Release1997 1.188658 0.955613 1.244 ## Year.Release1998 0.744742 0.939532 0.793 ## Year.Release1999 0.653088 0.936406 0.697 ## Year.Release2000 0.785921 0.925467 0.849 ## Year.Release2001 0.772488 0.922728 0.837 ## Year.Release2002 0.570865 0.922008 0.619 ## Year.Release2003 0.340110 0.921843 0.369 ## Year.Release2004 0.550812 0.921901 0.597 ## Year.Release2005 0.210534 0.921694 0.228 ## Year.Release2006 0.141533 0.921703 0.154 ## Year.Release2007 0.373361 0.921629 0.405 ## Year.Release2008 0.529829 0.921677 0.575 ## Year.Release2009 0.446508 0.921648 0.484 ## Year.Release2010 0.596658 0.921889 0.647 ## Year.Release2011 0.389514 0.921944 0.422 ## Year.Release2012 -0.031458 0.922540 -0.034 ## Year.Release2013 -0.170252 0.922985 -0.184 ## Year.Release2014 -0.383671 0.923096 -0.416 ## Year.Release2015 -0.478979 0.923302 -0.519 ## Year.Release2016 -0.826950 0.923192 -0.896 ## GenreAdventure -0.335989 0.063305 -5.307 ## GenreFighting 0.072835 0.055338 1.316 ## GenreMisc 0.357196 0.054323 6.575 ## GenrePlatform -0.184139 0.054854 -3.357 ## GenrePuzzle -0.345253 0.091582 -3.770 ## GenreRacing -0.097943 0.048451 -2.021 ## GenreRole-Playing -0.357837 0.043093 -8.304 ## GenreShooter -0.142624 0.040372 -3.533 ## GenreSimulation 0.405629 0.060887 6.662 ## GenreSports 0.011217 0.046986 0.239 ## GenreStrategy -0.483702 0.063838 -7.577 ## Critic.Score 0.114744 0.009865 11.632 ## User.Count.Log 0.615776 0.011589 53.134 ## Platform.typeOthers -0.188799 0.066011 -2.860 ## Platform.typePC -2.442802 0.051704 -47.246 ## Platform.typePlaystation 0.135254 0.033868 3.994 ## Platform.typeXbox -0.106733 0.036927 -2.890 ## Rating.typeE10+ -0.058393 0.040759 -1.433 ## Rating.typeM -0.467923 0.045704 -10.238 ## Rating.typeOthers 0.144593 0.545092 0.265 ## Rating.typeT -0.331696 0.035692 -9.293 ## Publisher.typeAtari -0.506235 0.082442 -6.141 ## Publisher.typeCapcom -0.759855 0.078983 -9.620 ## Publisher.typeElectronic Arts -0.114754 0.053315 -2.152 ## Publisher.typeKonami Digital Entertainment -0.705932 0.073042 -9.665 ## Publisher.typeNamco Bandai Games -0.569621 0.074819 -7.613 ## Publisher.typeNintendo 0.111592 0.074409 1.500 ## Publisher.typeOthers -0.618870 0.046433 -13.328 ## Publisher.typeSega -0.517708 0.069222 -7.479 ## Publisher.typeSony Computer Entertainment -0.585506 0.069125 -8.470 ## Publisher.typeTake-Two Interactive -0.276947 0.070848 -3.909 ## Publisher.typeTHQ -0.117032 0.068824 -1.700 ## Publisher.typeUbisoft -0.377192 0.059101 -6.382 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## Year.Release1988 0.017628 * ## Year.Release1992 0.167420 ## Year.Release1994 0.024432 * ## Year.Release1996 0.131733 ## Year.Release1997 0.213591 ## Year.Release1998 0.427996 ## Year.Release1999 0.485550 ## Year.Release2000 0.395791 ## Year.Release2001 0.402522 ## Year.Release2002 0.535836 ## Year.Release2003 0.712180 ## Year.Release2004 0.550211 ## Year.Release2005 0.819326 ## Year.Release2006 0.877964 ## Year.Release2007 0.685409 ## Year.Release2008 0.565410 ## Year.Release2009 0.628070 ## Year.Release2010 0.517516 ## Year.Release2011 0.672679 ## Year.Release2012 0.972799 ## Year.Release2013 0.853660 ## Year.Release2014 0.677690 ## Year.Release2015 0.603940 ## Year.Release2016 0.370418 ## GenreAdventure 1.15e-07 *** ## GenreFighting 0.188157 ## GenreMisc 5.22e-11 *** ## GenrePlatform 0.000793 *** ## GenrePuzzle 0.000165 *** ## GenreRacing 0.043269 * ## GenreRole-Playing &lt; 2e-16 *** ## GenreShooter 0.000414 *** ## GenreSimulation 2.91e-11 *** ## GenreSports 0.811327 ## GenreStrategy 4.01e-14 *** ## Critic.Score &lt; 2e-16 *** ## User.Count.Log &lt; 2e-16 *** ## Platform.typeOthers 0.004248 ** ## Platform.typePC &lt; 2e-16 *** ## Platform.typePlaystation 6.58e-05 *** ## Platform.typeXbox 0.003859 ** ## Rating.typeE10+ 0.152006 ## Rating.typeM &lt; 2e-16 *** ## Rating.typeOthers 0.790814 ## Rating.typeT &lt; 2e-16 *** ## Publisher.typeAtari 8.69e-10 *** ## Publisher.typeCapcom &lt; 2e-16 *** ## Publisher.typeElectronic Arts 0.031403 * ## Publisher.typeKonami Digital Entertainment &lt; 2e-16 *** ## Publisher.typeNamco Bandai Games 3.04e-14 *** ## Publisher.typeNintendo 0.133735 ## Publisher.typeOthers &lt; 2e-16 *** ## Publisher.typeSega 8.44e-14 *** ## Publisher.typeSony Computer Entertainment &lt; 2e-16 *** ## Publisher.typeTake-Two Interactive 9.36e-05 *** ## Publisher.typeTHQ 0.089092 . ## Publisher.typeUbisoft 1.86e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9177 on 6767 degrees of freedom ## Multiple R-squared: 0.5735, Adjusted R-squared: 0.5699 ## F-statistic: 159.6 on 57 and 6767 DF, p-value: &lt; 2.2e-16 model &lt;- aov(Global.Sales.Log ~ ., data = game.lm) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year.Release 24 521 21.7 25.78 &lt;2e-16 *** ## Genre 11 527 47.9 56.87 &lt;2e-16 *** ## Critic.Score 1 1771 1771.3 2103.34 &lt;2e-16 *** ## User.Count.Log 1 1344 1344.0 1595.92 &lt;2e-16 *** ## Platform.type 4 2944 736.1 874.11 &lt;2e-16 *** ## Rating.type 4 165 41.2 48.93 &lt;2e-16 *** ## Publisher.type 12 389 32.4 38.53 &lt;2e-16 *** ## Residuals 6767 5699 0.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The log value of global sales is mostly influenced by factors of critic score, user count log, platform type, Publisher type, and genre in the linear model analysis. ANOVA shows every factor is a key player of significant contributor. Critic.Score and User.Count.Log are the lead actors in this statistical drama. They positively affect the global sales log, while other factors like Platform type and Genre either lift up or pull down the global sales according to their types. This model boasts an explanatory power with an R-Square of 0.57. Given the nonlinear relationship indicated by our previous scatter plot about global sales versus critic scores (Figure 9.11) and the pronounced impact of critic scores in the linear model analysis, a polynomial model is entertained. model &lt;- lm(Global.Sales.Log ~ Critic.Score + I(Critic.Score^2) + I(Critic.Score^3) + I(Critic.Score^4), data = game.lm) summary(model) ## ## Call: ## lm(formula = Global.Sales.Log ~ Critic.Score + I(Critic.Score^2) + ## I(Critic.Score^3) + I(Critic.Score^4), data = game.lm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0029 -0.7972 0.0916 0.8801 5.6201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.030638 1.468428 8.193 3.02e-16 *** ## Critic.Score -0.840810 1.095444 -0.768 0.4428 ## I(Critic.Score^2) 0.385671 0.292104 1.320 0.1868 ## I(Critic.Score^3) -0.060971 0.033147 -1.839 0.0659 . ## I(Critic.Score^4) 0.003434 0.001358 2.528 0.0115 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.281 on 6820 degrees of freedom ## Multiple R-squared: 0.1623, Adjusted R-squared: 0.1618 ## F-statistic: 330.4 on 4 and 6820 DF, p-value: &lt; 2.2e-16 We focus on the third and fourth degrees of the polynomial fit as the initial levels lacked statistical significance in our pre-analysis. model &lt;- lm(Global.Sales.Log ~ I(Critic.Score^3) + I(Critic.Score^4), data = game.lm) summary(model) ## ## Call: ## lm(formula = Global.Sales.Log ~ I(Critic.Score^3) + I(Critic.Score^4), ## data = game.lm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8634 -0.7892 0.0950 0.8837 5.5807 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.193e+01 7.115e-02 167.722 &lt; 2e-16 *** ## I(Critic.Score^3) -2.989e-03 7.972e-04 -3.749 0.000179 *** ## I(Critic.Score^4) 6.076e-04 8.224e-05 7.387 1.67e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.283 on 6822 degrees of freedom ## Multiple R-squared: 0.1597, Adjusted R-squared: 0.1595 ## F-statistic: 648.3 on 2 and 6822 DF, p-value: &lt; 2.2e-16 The coefficients are statistically significant — the simplified model with critic score to the third and fourth-degree terms, brings the drama with an R-squared of 0.16. Not quite a blockbuster hit, but certainly a critical darling. To illustrate our findings: ModelFunc &lt;- function(x) {model$coefficients[1] + x^3*model$coefficients[2] + x^4*model$coefficients[3]} ggplot(data = game.lm, aes(x = Critic.Score, y = Global.Sales.Log)) + geom_point() + stat_function(fun = ModelFunc, color = &#39;blue&#39;, size = 1) Here is the scatter plot of Global.Sales.Log versus Critic Score, along with the predictive curve based on the polynomial model, which forecasts the global sales log using critic score. ::: {.exercise #unnamed-chunk-476} Visualize the distribution of NA_Sales in relation to Year_of_Release, Genre, Rating, and Platform using different plots, individually or combinedly. Interpret the relationship between NA_Sales with these factors. Hint: Consider applying log transformation to NA_Sales for better interpretability and potentially regroup platforms for a more consolidated view. ::: ::: {.exercise #unnamed-chunk-477} What is the correlation between NA_Sales and Critic_Score? Use scatter plot with smooth trend line or a fitted polynomial curve to show the relationship. Give me your findings. ::: ::: {.exercise #unnamed-chunk-478} Utilize linear modeling and ANOVA to analyze the contributing factors to the variance in NA_Sales. Provide a succinct interpretation of the results. Hint: Keep in mind the correlation patterns among those sales and scores highlighted in Figure 9.3. ::: 9.8 Conclusion While raw global and regional sales figures deviate from normality, their logarithmic counterparts dance closely with it. Patterns in regional sales harmonize with the global chorus. Critic scores and user scores exhibit a sweet correlation, although the former typically trails the latter across most game titles. However, no clear-cut connection emerges between scores and the number of reviews. Together, critic score, log-transformed user score count, genre, rating, platform, and publisher play their parts in the grand drama of log-transformation of global sales, with critic scores taking the star role, and the others in the supporting cast. Curtain call! "],["the-messy-salary-data.html", "Chapter 10 The messy salary data 10.1 Data Preparation and Cleaning 10.2 Variable Analysis 10.3 Analysis of gross income 10.4 Analysis of gross income types 10.5 LM and ANOVA analysis 10.6 Conclusion", " Chapter 10 The messy salary data Welcome to the world of payroll data! Our current focus is a somewhat chaotic dataset, sourced from repository, featuring Connecticut’s state employees’ payment from Calendar Year 2015 to the latest pay period. We randomly selected 8500 employees in fiscal year 2017 and saved it in state_empoyee_salary_data_2017.csv. Our mission is to demystify the correlation between income and an array of intriguing variables. 10.1 Data Preparation and Cleaning First things first, let’s tidy up our data. We will focus on the full-time employees and the favorable variables like various incomes, Age, Sex, Ethnic, Union description, and so on. salary &lt;- read.csv(&quot;state_employee_salary_data_2017.csv&quot;) salary &lt;- subset(salary, Full.Part == &quot;F&quot;) # Select full time employees salary &lt;- salary[, c(4, 10, 14, 24:25, 28:31, 37)] # Keep only the interested variables Let’s break down the key variables to understand what each one represents: EmplId.Empl.Rcd: Employee’s personal barcode, unique identifier for every individual Check.Dt: The date each paycheck was issued. Bi.Weekly.Comp.Rate: The bi-weekly salary rate. Tot.Gross: The total gross income. Age: Employee’s age. Job.Indicator: Indicator of the nature of the job. Ethnic.Grp: Ethnic background of the employee. Sex: Gender of the employee. Full.Part: Category of the job. “F” for Full-time, “P” for Part-time. Union.Descr: The union description of the employee. It’s important to factorize the categorical variables first. salary$EmplId.Empl.Rcd &lt;- as.factor(salary$EmplId.Empl.Rcd) salary$Sex &lt;- as.factor(salary$Sex) salary$Ethnic.Grp &lt;- as.factor(salary$Ethnic.Grp) salary$Union.Descr &lt;- as.factor(salary$Union.Descr) Now we will explore the pay checks’ distribution. EmplId.Empl.Rcd &lt;- as.character(salary$EmplId.Empl.Rcd) counts &lt;- data.frame(table(EmplId.Empl.Rcd)) PersonOccurCount &lt;- table(counts[, 2]) plot(PersonOccurCount, col = rainbow(30), xlab = &quot;Occurance of employee&quot;, ylab = &quot;Count of employee&quot;) Figure 10.1: Situation of check occurrence of employees. Most employees were paid 26 times. Let’s remove the employees not receiving 26 pay checks. subEmpl &lt;- subset(counts, Freq != 26) id &lt;- which(salary$EmplId.Empl.Rcd %in% subEmpl$EmplId.Empl.Rcd) salary &lt;- salary[- id, ] Let’s display the distribution of the dates the checks were issued. Check.Dt &lt;- gsub(&quot; 12:00:00 AM&quot;, &quot;&quot;, salary$Check.Dt) counts &lt;- table(Check.Dt) barplot(counts, col = rainbow(26), axis.lty = 1, xlab = &quot;Check Date&quot;, ylab = &quot;Count&quot;) Figure 10.2: The counts of checks for check date in 2017. The counts of checks for bi week are consistent, except a curious dip on 03/08/2017. Let’s gently nudge these anomalies out of our data. subEmpl &lt;- salary[which(salary$Check.Dt == &quot;03/08/2017 12:00:00 AM&quot;), ] salary &lt;- salary[- which(salary$EmplId.Empl.Rcd == subEmpl$EmplId.Empl.Rcd), ] Next, we inspect for any mismatched gender data. library(dplyr) dupliSex_count &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd, Sex) %&gt;% summarise(Total.Gross = sum(Tot.Gross), .groups = &quot;drop&quot;) %&gt;% add_count(EmplId.Empl.Rcd) %&gt;% filter(n &gt; 1) %&gt;% summarise(duplicated_count = n()) Zero duplicated_count indicates no gender data mix-ups. But is there a mysterious “U” gender lurking? nrow(salary[which(salary$Sex == &quot;U&quot;), ]) ## [1] 0 No “U” gender found now. They should have been removed in previous steps. Next, let’s examine the ethnic group duplication. dupliEthnic_count &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd, Ethnic.Grp) %&gt;% summarise(Total.Gross = sum(Tot.Gross), .groups = &quot;drop&quot;) %&gt;% add_count(EmplId.Empl.Rcd) %&gt;% filter(n &gt; 1) %&gt;% summarise(duplicated_count = n()) Again, Zero duplication! It confirms our dataset’s integrity regarding gender and ethnicity. But hold on, we’ve got folks with gross incomes or biweekly rates in the negative zone. How curious! subGross &lt;- subset(salary, Tot.Gross &lt;= 0 | Bi.Weekly.Comp.Rate &lt;= 0) unique_ids &lt;- unique(subGross$EmplId.Empl.Rcd) salary &lt;- salary[- which(salary$EmplId.Empl.Rcd %in% unique_ids), ] We waved goodbye to those cases with impossible financial figures. Now let’s check the distribution of total gross. plot(salary$Tot.Gross) lag.plot(salary$Tot.Gross) Figure 10.3: Scatterplot and lagplot of total gross income. Both scatter plot and lag plot reveal a cozy congregation of salaries under $20000, with a few adventurous ones rocketing past $60000. Next on our cleaning checklist: employees with the mysterious Job Indicator “S”. subGross &lt;- subset(salary, Job.Indicator == &quot;S&quot;) unique_ids &lt;- unique(subGross$EmplId.Empl.Rcd) salary &lt;- salary[- which(salary$EmplId.Empl.Rcd %in% unique_ids), ] Employees marked with “S” have left the building. Now, let’s deal with those duplicated union descriptions. dupliUnion &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd) %&gt;% summarise(UnionCount = n_distinct(Union.Descr), .groups = &quot;drop&quot;) %&gt;% filter(UnionCount &gt; 1) %&gt;% select(EmplId.Empl.Rcd) salary &lt;- salary %&gt;% anti_join(dupliUnion, by = &quot;EmplId.Empl.Rcd&quot;) Finally, with our data now squeaky clean, let’s take one last look, focusing on important variables like sex, ethnicity, union, age, total gross, and biweekly rate. Let’s round off the age to keep it neat. salary &lt;- salary %&gt;% group_by(EmplId.Empl.Rcd, Sex, Ethnic.Grp, Union.Descr) %&gt;% summarise(Age = round(mean(Age), 0), Tot.Gross = mean(Tot.Gross), Bi.Weekly.Rate = mean(Bi.Weekly.Comp.Rate)) %&gt;% group_by(EmplId.Empl.Rcd) %&gt;% filter(n() == 1) %&gt;% droplevels() str(data.frame(salary)) ## &#39;data.frame&#39;: 3441 obs. of 7 variables: ## $ EmplId.Empl.Rcd: Factor w/ 3441 levels &quot;00034CF9004E4E0D7872FEB52CB5933F&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 2 1 1 2 ... ## $ Ethnic.Grp : Factor w/ 8 levels &quot;&quot;,&quot;AMIND&quot;,&quot;ASIAN&quot;,..: 8 8 8 8 8 8 6 5 8 8 ... ## $ Union.Descr : Factor w/ 54 levels &quot;Admin and Residual (P-5)&quot;,..: 9 2 7 24 24 43 35 41 37 44 ... ## $ Age : num 73 64 65 54 50 46 71 47 52 48 ... ## $ Tot.Gross : num 4176 1799 1935 2751 2195 ... ## $ Bi.Weekly.Rate : num 3995 1776 1931 2713 2195 ... One last tweak: there is one level marked as “” in ethnic group. Let’s label it as “unknown”. levels(salary$Ethnic.Grp)[levels(salary$Ethnic.Grp) == &quot;&quot;] &lt;- &quot;unknown&quot; And there we have it! Our data is now primed and ready for the analytical spotlight. Let’s take a moment to summarize our polished data. summary(salary) ## EmplId.Empl.Rcd Sex Ethnic.Grp ## 00034CF9004E4E0D7872FEB52CB5933F: 1 F:1804 WHITE :2101 ## 001D621CF1C85C8F46EFD935CD685BFD: 1 M:1637 BLACK : 485 ## 004C3FFCCFC10A09D65CA3DF60CB69FD: 1 HISPA : 283 ## 0059143835BC7A37482ABF4B933A4642: 1 NSPEC : 236 ## 0059CD55728E16BC6BCFD35EDBA9DA32: 1 unknown: 211 ## 007CE08D82395FFD546943D382911009: 1 ASIAN : 109 ## (Other) :3435 (Other): 16 ## Union.Descr Age Tot.Gross ## Service/Maintenance (NP-2) : 299 Min. :22.00 Min. : 568.7 ## Correctional Officers (NP-4) : 289 1st Qu.:40.00 1st Qu.: 2414.2 ## Social and Human Services(P-2): 263 Median :49.00 Median : 3041.0 ## Administrative Clerical (NP-3): 252 Mean :47.71 Mean : 3222.5 ## Admin and Residual (P-5) : 214 3rd Qu.:56.00 3rd Qu.: 3754.7 ## Health Professional (P-1) : 198 Max. :81.00 Max. :17530.3 ## (Other) :1926 ## Bi.Weekly.Rate ## Min. : 25.78 ## 1st Qu.: 2145.94 ## Median : 2656.14 ## Mean : 2894.14 ## 3rd Qu.: 3443.95 ## Max. :12884.62 ## After cleaning, we’re left with 3,441 employees, each with a unique set of characteristics. Our dataset now reflects a diverse sex group, ranging from 22 to 81 years old, belonging to 8 different ethnicities and 54 unions. The financial landscape they inhabit is vast: The minimum total gross is $568.7 and the maximum is $17530.3. The biweekly rate ranges from $25.78 to $12884.62. The income varies significantly, offering us a rich field for analysis. We’ll now focus on exploring the total gross income of these employees. A series of compelling questions guide our journey: How about the patterns of the total gross income? Are males and females paid equally? Is there a correlation between the numbers on the paycheck and the candles on the birthday cake? Does one’s ethnic background play a role in their earning potential? What’s the most important factor affecting the state employees’ gross income? First, let’s dive into the detailed information of these variables. 10.2 Variable Analysis 10.2.1 Income and age From the preliminary analysis(Figure 10.3), we notice that “Tot.Gross” is not randomly scattered. Let’s log-transform it for better insights. salary$Gross.Log &lt;- log(salary$Tot.Gross) variable &lt;- c(&quot;Tot.Gross&quot;, &quot;Gross.Log&quot;, &quot;Age&quot;) # Pick up the numeric columns par(mfrow = c(3, 2)) # Layout in 3 rows and 2 columns for (i in 1:length(variable)){ sub &lt;- unlist(salary[variable[i]]) submean &lt;- mean(sub) hist(sub, main = paste(&quot;Hist. of&quot;, variable[i], sep = &quot; &quot;), xlab = variable[i]) abline(v = submean, col = &quot;blue&quot;, lwd = 1) qqnorm(sub, main = paste(&quot;Q-Q Plot of&quot;, variable[i], sep = &quot; &quot;)) qqline(sub) if (i == 1) { s.t &lt;- shapiro.test(sub) } else { s.t &lt;- rbind(s.t, shapiro.test(sub)) } } Figure 10.4: Histogram and QQ plots for total gross, log of gross and age. s.t &lt;- s.t[, 1:2] # Take first two columns of shapiro.test result s.t &lt;- cbind(variable, s.t) # Add variable name for the result s.t ## variable statistic p.value ## s.t &quot;Tot.Gross&quot; 0.840075 2.490722e-50 ## &quot;Gross.Log&quot; 0.9888379 7.88885e-16 ## &quot;Age&quot; 0.9848966 1.023745e-18 Our findings reveal that while total gross income is left-skewed, the log-transformed gross gravitates closer to normal distribution. The age distribution, resembling a bell curve, is primarily clustered in the 45-50 years range. Yet, the Shapiro tests challenge our assumptions of normality. Let’s now turn our attention to the ethnic composition of the dataset. 10.2.2 Ethnic group library(ggplot2) counts &lt;- data.frame(sort(table(salary$Ethnic.Grp), decreasing = TRUE)) perc &lt;- paste(round(counts$Freq/sum(counts$Freq), 2)*100, &quot;%&quot;, sep = &quot;&quot;) ggplot(counts, aes(x = reorder(Var1, Freq), y = Freq)) + geom_bar(stat = &#39;identity&#39;, fill = rainbow(8)) + geom_text(aes(x = c(8:1), y = Freq + 100 , label = perc), size = 3.5) + labs(x = &#39;Ethnic Group&#39;, y = &#39;Freqency&#39;, title = &#39;Ethnic group by count&#39;) + theme_bw() + # Classic dark-on-light coord_flip() + # Flip the plot theme(plot.title = element_text(hjust = 0.5)) Our data shows a significant majority (61%) of state employees are White, followed by 14% Black. AMIND and PACIF, however, are represented in significantly smaller numbers. 10.2.3 Sex Next, we explore the gender distribution. library(plotrix) counts &lt;- sort(table(salary$Sex), decreasing = TRUE) # Labels with count number and percentage percentage &lt;- paste(round(counts/sum(counts), 2)*100, &quot;%&quot;, sep = &quot;&quot;) labels &lt;- paste(names(counts), &quot;\\n&quot;, counts, percentage, sep = &quot; &quot;) pie3D(counts, labels = labels, explode = 0.05, main = &quot;Pie chart of sex&quot;, labelcex = 1.0, labelpos = c(1.8, 5.0)) Interestingly, females outnumber males by about 4% in our dataset. 10.2.4 Union descreption Lastly, let’s analyze the top unions both by count and average gross income. top10Union &lt;- data.frame(sort(table(salary$Union.Descr), decreasing = TRUE)[1:10]) # By count names(top10Union)[1] &lt;- &quot;Union.Descr&quot; topUnion &lt;- salary %&gt;% group_by(Union.Descr) %&gt;% summarise(Avg.Gross.Log = round(mean(Gross.Log), 3)) %&gt;% arrange(desc(Avg.Gross.Log)) %&gt;% head(10) #by average gross sales UnionCountGross &lt;- data.frame(top10Union, topUnion) names(UnionCountGross)[1]&lt;- &quot;Top unions by count&quot; names(UnionCountGross)[3]&lt;- &quot;Top unions by log of gross&quot; library(knitr) kable(UnionCountGross, booktabs = TRUE, caption = &quot;Top unions by count and average log value of gross&quot;) Table 10.1: Top unions by count and average log value of gross Top unions by count Freq Top unions by log of gross Avg.Gross.Log Service/Maintenance (NP-2) 299 UCHC - Faculty - AAUP 8.927 Correctional Officers (NP-4) 289 UCHC - Faculty 8.710 Social and Human Services(P-2) 263 Comm College Mgmt Exclusions 8.693 Administrative Clerical (NP-3) 252 UConn - Law School Faculty 8.605 Admin and Residual (P-5) 214 Amercan Fed of School Admin 8.523 Health Professional (P-1) 198 Connecticut Innovations Inc 8.511 Health NonProfessional (NP-6) 189 Exempt/Elected/Appointed 8.510 Engineer Scien Tech (P-4) 181 StatePoliceLts&amp;Captains (NP-9) 8.473 UCHC Univ Hlth Professionals 121 Managerial 8.416 Managerial 119 Crim Justice Managerial Exempt 8.389 The most populous unions include Service Maintain, Correctional officers, and Social &amp; Human Services. However, when ranked by average gross, the top unions largely consist of those from universities or colleges. Interestingly, except for the Managerial union, the top 10 unions by count are significantly different from those by average gross income. 10.3 Analysis of gross income 10.3.1 Age Let’s examine how age influences gross income. # Get bar plot AgeBar &lt;- salary %&gt;% group_by(Age) %&gt;% summarise(Avg.Gross = mean(Tot.Gross)) %&gt;% ggplot(aes(x = Age, y = Avg.Gross)) + geom_bar(stat = &#39;identity&#39;, col = &quot;lightblue&quot;, fill = &quot;blue&quot;) + scale_x_continuous(breaks = seq(20, 90, 10)) + labs(y = &quot;Average gross income&quot;) # Get density2D plot AgeDensity &lt;- ggplot(salary, aes(x = Age, y = Gross.Log)) + stat_density2d(aes(fill = ..density..^0.25), geom = &quot;tile&quot;, contour = FALSE, n = 200) + scale_fill_continuous(low = &quot;white&quot;, high = &quot;dodgerblue3&quot;) + scale_x_continuous(breaks = seq(10, 100, 10)) + scale_y_continuous(breaks = seq(2, 12, 1)) + theme(legend.position = &quot;none&quot;, panel.background = element_blank()) # Display two plots in the same row library(gridExtra) grid.arrange(AgeBar, AgeDensity, nrow = 1) Figure 10.5: Average gross and log of gross distribution by age. We observe that older employees tend to earn more, with the average gross income hovering around $3000. The density plot reveals a concentration of employees with a log gross income near 8. For a granular view, we break age down into decade-long categories: salary$AgeGroup &lt;- cut(salary$Age, breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90), right = FALSE) stat &lt;- salary %&gt;% group_by(AgeGroup) %&gt;% summarise(Avg.Gross.Log = mean(Gross.Log)) ggplot() + geom_boxplot(data = salary, aes(x = AgeGroup, y = Gross.Log, fill = AgeGroup)) + geom_point(data = stat, aes(x = AgeGroup, y = Avg.Gross.Log), color = &quot;brown&quot;) + geom_text(data = stat, aes(label = round(Avg.Gross.Log, 2), x = AgeGroup, y = Avg.Gross.Log - 0.1)) Figure 10.6: Distribution of log gross income by age group. With each decade, there’s a steady increase in log gross income, except in the 80-90 age group，reflecting the progression and experience gained over years of service. model &lt;- aov(salary$Gross.Log~salary$AgeGroup) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## salary$AgeGroup 6 33.0 5.500 54.53 &lt;2e-16 *** ## Residuals 3434 346.4 0.101 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A p-value significantly less than 0.05 in our ANOVA analysis reveals significant differences in mean log gross income across age groups. library(agricolae) # LSD.test needs to provide degrees of freedom and mean square for errors. We can get it from aov. res &lt;- LSD.test(salary$Gross.Log, salary$AgeGroup, DFerror = model$df.residual, MSerror = anova(model)[[&quot;Mean Sq&quot;]][2]) res$groups ## salary$Gross.Log groups ## [70,80) 8.284744 a ## [60,70) 8.122599 b ## [50,60) 8.068892 c ## [40,50) 8.032668 d ## [30,40) 7.913289 e ## [20,30) 7.756642 f ## [80,90) 6.343428 g Pairwise comparisons confirm that each age group significantly differs from the others in mean log gross income. The elder the age, the higher the log gross income, except the 80-90 group, which surprisingly has the lowest mean log gross. Next, we explore union descriptions by age group. library(scales) library(plotly) AgePercBar &lt;- salary %&gt;% group_by(Union.Descr, Age) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% arrange(desc(Avg.Gross)) %&gt;% ggplot(aes(x = Age, y = Avg.Gross, fill = Union.Descr)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + scale_y_continuous(labels = percent_format()) + scale_x_continuous(breaks = seq(10, 100, 10)) + labs(y = &quot;Percentage&quot;) + theme(panel.background = element_rect(fill = &quot;black&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) ggplotly(AgePercBar) Figure 10.7: Percentage of union descriptions by age group. Our interactive plot paints a vivid picture of union representation across different ages. For example, younger employees at 22 display a balanced mix in several unions(Administrative Clerical, Exempt/Elected/Appointed, Service/Maintenance and UCHC Univ Hlth Professional), whereas the 81-year-olds predominantly align with one (Legislative Management). 10.3.2 Sex Our analytical journey now turns to a critical aspect: the comparison of gross income between genders. par(mfrow = c(1, 2)) # Density plot for each sex SubFemale &lt;- salary[which(salary$Sex == &quot;F&quot;), ] SubMale &lt;- salary[which(salary$Sex == &quot;M&quot;), ] plot(density(SubFemale$Gross.Log), col = &quot;red&quot;, xlab = &quot;Log of Gross&quot;, main = &quot;Density plot for each sex&quot;) lines(density(SubMale$Gross.Log), col = &quot;blue&quot;) # Add density line of SubMale to density plot of SubFemale legend(&quot;topright&quot;, c(&quot;M&quot;,&quot;F&quot;), lty = c(1, 1), col = c(&quot;blue&quot;, &quot;red&quot;)) # Add legend # LSD comparison for sex group model &lt;- aov(salary$Gross.Log~salary$Sex) res &lt;- LSD.test(salary$Gross.Log, salary$Sex, DFerror = model$df.residual, MSerror = anova(model)[[&quot;Mean Sq&quot;]][2]) gross &lt;- round(res$groups[, &quot;salary$Gross.Log&quot;], 2) plot(res, xlab = &quot;Sex Group&quot;, ylab = &quot;Range between max and min&quot;, main = &quot;Sex groups and variation range&quot;) text(x = c(seq(from = 1, to = 10, by = 1.2)), y = 8, gross) Figure 10.8: Log of gross comparison by sex group. The blue male line in the density plot overtakes the red female line, indicating higher male income. The earlier density peak of the female group also tells the same story. The variation range plot, with its distinct alpha-beta index, further emphasizes that these sex groups differ significantly in mean log gross income. Yes, the numbers speak: males earn more than females. # Produce the point and line plot PointLine &lt;- salary %&gt;% group_by(Ethnic.Grp, Sex) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% ggplot(aes(x = Ethnic.Grp, y = Avg.Gross, colour = Sex, group = Sex)) + geom_line(size = 1) + geom_point(size = 4, shape = 19) + theme(axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.major = element_blank(), legend.position = &quot;bottom&quot;) + labs(x = &quot;Ethnic Group&quot;, y = &quot;Average Gross Log&quot;) # Produce the density ridge plot library(ggridges) DensityRidge &lt;- salary %&gt;% group_by(Ethnic.Grp, Sex) %&gt;% ggplot(aes(x = Gross.Log, y = Ethnic.Grp, fill = Sex)) + geom_density_ridges(alpha = 0.55) + theme(legend.position = &quot;bottom&quot;) + xlim(6.0, 10.0) # Lay out two plots in the same row library(gridExtra) grid.arrange(PointLine, DensityRidge, nrow = 1) Figure 10.9: Log of gross comparison by ethnic and sex group. The plots unveil a consistent trend: in most ethnic groups, males outearn females, with the AMIND group showing a stark contrast. However, Asian females break this pattern, earning on par with their male counterparts. The small number of PACIF employees results in their absence from the density-ridge plot. # Standardize Gross.Log first. Separate into two groups &quot;Below average&quot; and &quot;Above average&quot; Sd.Gross &lt;- round((salary$Gross.Log - mean(salary$Gross.Log)) / sd(salary$Gross.Log), 2) Gross.Type &lt;- ifelse(Sd.Gross &lt; 0, &quot;Below average&quot;, &quot;Above average&quot;) Sex &lt;- salary$Sex Category &lt;- salary$Union.Descr GrossUnionSex &lt;- data.frame(Category, Sd.Gross, Gross.Type, Sex) ggplot(GrossUnionSex, aes(x = Category, y = Sd.Gross)) + geom_bar(stat = &#39;identity&#39;, aes(fill = Gross.Type), width = .5) + facet_wrap(~ Sex) + # Arrange panels according to sex labs(x = &quot;Union Descreption&quot;, y = &quot;Standardized log of gross&quot;) + coord_flip() + # Horizontal plot theme(legend.position = &quot;top&quot;) Figure 10.10: Diverging bars of stanardized log of gross income. These bars lay bare the income distribution within unions, irrespective of gender. For example, more Uconn-Faculty are paid above the average; more UCHC Univ Hlth Professionals are paid below the average; while for Social and Human Services, it’s half and half. The plots also indicate that certain unions, regardless of gender, tend to lean towards paying either below or above the average. For instance, Administrative Clerical and service/maintenance tend to pay below average, while others like Managerial and Health Professional are on the higher end of the pay scale. In our next phase of analysis, we focus on the intriguing interplay between gross income and ethnic groups. 10.3.3 Ethnic Group # Bar plot with error bar for different ethnic group EthnicBar &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% summarise(mean = mean(Gross.Log), sd = sd(Gross.Log), se = sd(Gross.Log)/sqrt(n())) %&gt;% ggplot(aes(x = Ethnic.Grp, y = mean, color = Ethnic.Grp, fill = Ethnic.Grp)) + geom_bar(stat = &quot;identity&quot;) + geom_errorbar(aes(ymin =mean - se, ymax =mean + se), color = &quot;black&quot;, width = 0.3) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Density ridges for different ethnic group EthnicDensity &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% ggplot(aes(x = Gross.Log, y = Ethnic.Grp, color = Ethnic.Grp, fill = Ethnic.Grp)) + geom_density_ridges(alpha = 0.5) + xlim(6.0, 10.0) + coord_flip() + theme(legend.position = &quot;none&quot;) # Display two plots in same row grid.arrange(EthnicBar, EthnicDensity, nrow = 1) Figure 10.11: Bar and density ridge plot of log of gross by ethnic. The bar plot reveals that Asian, Nspec, and White ethnic groups have higher average gross incomes, with Pacif and Unknown on the lower end. The variance among ethnic groups is notably distinct, with Pacif and Amind showing the most significant differences. The density ridge plots further depict diverse income distributions across ethnicities. Asian exhibits the biggest density above log of gross of 8.5, while unknown falls mostly below 8. Ethnic groups of white, black, and unknown show relatively symmetrical density distribution. # Get ethnic order according to the frequency of count CountEthnic &lt;- data.frame(sort(table(salary$Ethnic.Grp), decreasing = TRUE)) # Get ethnic order according log of gross income GrossEthnic &lt;- salary %&gt;% group_by(Ethnic.Grp) %&gt;% summarise(Avg.Gross = mean(Gross.Log)) %&gt;% arrange(desc(Avg.Gross)) %&gt;% pull(Ethnic.Grp) # build data frame for rank of these two ethnic order EthnicGroup &lt;- unlist(list(CountEthnic[, 1], GrossEthnic)) Rank &lt;- rep(1:8, 2) EthnicType &lt;- rep(c(&quot;CountEthnic&quot;,&quot;GrossEthnic&quot;), each = 8) EthnicRank &lt;- data.frame(Rank, EthnicType, EthnicGroup) # point plot two set of ethnics according rank and connect same ethnic by line ggplot(EthnicRank, aes(y = Rank, x = EthnicType, label = EthnicGroup, group = EthnicGroup) )+ geom_line(size = 1) + geom_point(aes(color = EthnicGroup), size = 5) + scale_y_continuous(breaks = seq(0, 10, 1)) + annotate(&quot;text&quot;, x = 0.7, y = Rank[1:8], label = EthnicGroup[1:8], hjust = 0, cex = 3.5) + annotate(&quot;text&quot;, x = 2.1, y = Rank[9:16], label = EthnicGroup[9:16], hjust = 0, cex = 3.5) Figure 10.12: Ethnic rank plot by count and log of gross. White and Black employees dominate the workforce in numbers, while Asians lead in average gross income. Pacif consistently ranks lowest in both categories. Interestingly, the ranking by income notably differs from those by count. model &lt;- aov(Gross.Log ~ Ethnic.Grp, data = salary) tukey &lt;- TukeyHSD(model) par(mar = c(4, 10, 2, 1)) psig &lt;- as.numeric(apply(tukey$Ethnic.Grp[, 2:3], 1, prod) &gt;= 0) + 1 plot(tukey, col = psig, las = 1, cex.axis = 0.7, yaxt = &quot;n&quot;) for (j in 1:length(psig)){ axis(2, at = j,labels = rownames(tukey$Ethnic.Grp)[length(psig) - j + 1], las = 1, cex.axis = .8, col.axis = psig[length(psig) - j + 1]) } The Tukey HSD test uncovers significant differences in income among various ethnic pairs, like unknown with Asian, Black, Nspec and White; Asian with Black, Hispa and White; Black with Hispa and White; Hispa with Nespa and White. ggplot(salary, aes(x = Age, y = Gross.Log, colour = Sex, shape = Sex)) + geom_point() + ggtitle(&quot;Scatterplot of log of gross vs Age by Sex and Ethnic&quot;) + facet_wrap(~ Ethnic.Grp) + # Arrange panels by Ethnic Group geom_smooth(aes(colour = Sex), method = &#39;lm&#39;,formula = y ~ x) + # Add the regression line theme(plot.title = element_text(hjust = 0.5), legend.position = c(0.85, 0.15)) Finally, a scatterplot analysis of gross income vs. age by sex and ethnic group provides further insights, revealing patterns and disparities across different demographics. In conclusion, White employees are the most numerous, while the Pacif group is scarcely represented; Across ethnic groups, there’s a general trend of increasing log gross income with age. However, this pattern varies by sex within each ethnic group. Some groups, like Unknown and Amind, show faster income increases for males, whereas in groups like Asian, females experience a quicker rise in earnings. 10.4 Analysis of gross income types In this section, we delve into the categorization of gross income types and examine the effects of age, sex, and ethnicity on income. 10.4.1 Defining Gross income types We start by dividing employees into three distinct income groups: 1 High.Gross: Average log of gross greater than 8.5 2 Middle.Gross: Average log of gross between 7.8 and 8.5 3 Low.Gross: Average log of gross equal to or less than 7.8 salary &lt;- salary %&gt;% mutate(Gross.Type = case_when( Gross.Log &gt; 8.5 ~ &quot;High&quot;, Gross.Log &gt; 7.8 ~ &quot;Middle&quot;, TRUE ~ &quot;Low&quot; )) Let’s now analyze each gross type for employee count, percentage distribution, and average incomes. library(janitor) GrossIncome &lt;- salary %&gt;% group_by(Gross.Type) %&gt;% summarise( Count = n(), Avg.Log.Gross = round(mean(Gross.Log), 2), Avg.Tot.Gross = round(mean(Tot.Gross), 2), ) %&gt;% mutate( Percent = round(Count / sum(Count), 2), Gross.Type = c(&quot;High, Gross.Log &gt; 8.5&quot;, &quot;Middle, Gross.Log &gt; 7.8&quot;, &quot;Low, Gross.Log &lt;= 7.8&quot;) ) %&gt;% arrange(desc(Avg.Log.Gross)) %&gt;% select(Gross.Type, Count, Percent, Avg.Log.Gross, Avg.Tot.Gross) kable(GrossIncome, booktabs = TRUE, caption = &quot;Table of gross type&quot;) Table 10.2: Table of gross type Gross.Type Count Percent Avg.Log.Gross Avg.Tot.Gross High, Gross.Log &gt; 8.5 217 0.06 8.72 6307.37 Low, Gross.Log &lt;= 7.8 2329 0.68 8.11 3379.69 Middle, Gross.Log &gt; 7.8 895 0.26 7.62 2065.61 GrossIncome$Gross.Type &lt;- factor(GrossIncome$Gross.Type, levels = GrossIncome$Gross.Type) ggplot(GrossIncome, aes(x = Gross.Type, label = paste(Avg.Tot.Gross, paste(Percent*100, &quot;%&quot;, sep = &quot;&quot;), sep = &quot;, &quot;))) + geom_bar(aes(y = Count), stat = &quot;identity&quot;, fill = rainbow(3)) + geom_line(aes(y = Avg.Log.Gross*250, group = 1, colour = &quot;Avg.Tot.Gross*250&quot;), size = 2) + geom_point(aes(y = Avg.Log.Gross*250, group = 1, colour = &quot;Avg.Tot.Gross*250&quot;), size = 4) + labs(x = &quot;Gross Type&quot;, y = &quot;Employee Number&quot;, title = &quot;Average gross and employee distribution for each gross type&quot;) + theme(plot.title = element_text(hjust = 0.5)) + geom_text(x = c(1, 2, 3), y = c(2000, 1900, 1800), size = 4, color = &quot;black&quot;) As it turns out, the high gross group makes up a mere 6% of the workforce, with the middle group claiming a substantial 68%, and the low gross group rounding out the remaining 26%. The average total gross income takes a nosedive from 6,307 dollars bi-weekly in the high gross group to 3,380 and then to 2,066 for the middle and low groups, respectively. This disparity highlights a threefold income difference between the high and low gross categories. 10.4.2 The Intersection of Ethnic, Sex and age salary %&gt;% mutate(EthnicSex = paste(Ethnic.Grp, Sex, sep = &quot;-&quot;)) %&gt;% tabyl(EthnicSex, Gross.Type, show_missing_levels = FALSE) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_totals(&quot;col&quot;) %&gt;% adorn_percentages(&quot;all&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns %&gt;% adorn_title ## Gross.Type ## EthnicSex High Low Middle Total ## AMIND-F 0.0% (0) 0.1% (5) 0.1% (3) 0.2% (8) ## AMIND-M 0.0% (1) 0.0% (0) 0.1% (5) 0.2% (6) ## ASIAN-F 0.3% (10) 0.3% (11) 1.0% (36) 1.7% (57) ## ASIAN-M 0.2% (7) 0.2% (6) 1.1% (39) 1.5% (52) ## BLACK-F 0.3% (10) 2.5% (86) 5.8% (199) 8.6% (295) ## BLACK-M 0.3% (11) 1.2% (42) 4.0% (137) 5.5% (190) ## HISPA-F 0.1% (3) 1.7% (60) 2.8% (95) 4.6% (158) ## HISPA-M 0.1% (3) 1.4% (47) 2.2% (75) 3.6% (125) ## NSPEC-F 0.2% (7) 1.0% (33) 2.9% (101) 4.1% (141) ## NSPEC-M 0.2% (8) 0.8% (26) 1.8% (61) 2.8% (95) ## PACIF-F 0.0% (0) 0.0% (1) 0.0% (0) 0.0% (1) ## PACIF-M 0.0% (0) 0.0% (0) 0.0% (1) 0.0% (1) ## unknown-F 0.3% (10) 1.9% (64) 1.7% (57) 3.8% (131) ## unknown-M 0.2% (6) 1.2% (41) 1.0% (33) 2.3% (80) ## WHITE-F 1.1% (37) 7.6% (261) 20.8% (715) 29.4% (1,013) ## WHITE-M 3.0% (104) 6.2% (212) 22.4% (772) 31.6% (1,088) ## Total 6.3% (217) 26.0% (895) 67.7% (2,329) 100.0% (3,441) In high gross type, white males take the lion’s share. Other ethnic groups see a more balanced or even female-dominated representation in the high gross category. For middle and low gross types, females generally outnumber males, except in the white ethnic group. Over half of the employees in most ethnic groups fall into the middle gross type, with the unknown group showing a higher propensity towards the low gross type. library(ggmosaic) h_mosaic &lt;- ggplot(data = salary) + geom_mosaic(aes(x = product(Sex, Ethnic.Grp), fill = AgeGroup), na.rm = T, divider = mosaic(&quot;h&quot;)) + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, size = 6), legend.position=&quot;top&quot;,legend.text=element_text(size = 6), panel.background = element_rect(fill=&quot;black&quot;), panel.grid.major = element_blank()) + labs(x = &quot;&quot;, y = &quot;&quot;, title= &quot;Mosaic Plot for Gross Group by Ethnic and Sex Group&quot;) + facet_grid(Gross.Type ~ .) ggplotly(h_mosaic, width = 750, height = 500) %&gt;% layout(legend = list(orientation = &quot;h&quot;, y = 1)) Our mosaic plot vividly illustrates the age, sex, and ethnic diversity within each gross income type. Elderly males, particularly those aged 70-80, are more prevalent in the high gross group. On the other side, white ethnic group dominates more than half sharing in high and middle gross groups, but less than half in low gross group. The other ethnics groups share the remaining distribution in each gross income type. 10.5 LM and ANOVA analysis In our quest to decode the complexities of state employee salaries, we now turn to statistical modeling. Our goal is to understand how factors like age, ethnicity, sex, and union affiliation influence the log of gross income. We begin by prepping our data for the linear model: # Remove first col. of EmplId.Empl.Rcd and other variables salaryglm &lt;- salary[, - c(1, 6:7, 9:10)] str(salaryglm) ## tibble [3,441 × 5] (S3: tbl_df/tbl/data.frame) ## $ Sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 2 1 1 2 2 1 1 2 ... ## $ Ethnic.Grp : Factor w/ 8 levels &quot;unknown&quot;,&quot;AMIND&quot;,..: 8 8 8 8 8 8 6 5 8 8 ... ## $ Union.Descr: Factor w/ 54 levels &quot;Admin and Residual (P-5)&quot;,..: 9 2 7 24 24 43 35 41 37 44 ... ## $ Age : num [1:3441] 73 64 65 54 50 46 71 47 52 48 ... ## $ Gross.Log : num [1:3441] 8.34 7.5 7.57 7.92 7.69 ... We employ a linear model (LM) and an ANOVA analysis to dissect the relationship between our variables and gross income: modelLM &lt;- lm(Gross.Log ~ ., data = salaryglm) ss &lt;- coef(summary(modelLM)) # Take only coefficient of summary ss.sig &lt;- ss[ss[, &quot;Pr(&gt;|t|)&quot;] &lt; 0.05, ][1:20, ] # Show first 20 coefficient with p &lt; 0.05 ss.sig ## Estimate Std. Error t value ## (Intercept) 7.53153247 0.03449654 218.327176 ## SexM 0.06950292 0.00926251 7.503682 ## Ethnic.GrpAMIND 0.21998688 0.06774550 3.247255 ## Ethnic.GrpASIAN 0.19194372 0.03354914 5.721272 ## Ethnic.GrpBLACK 0.15780719 0.02657445 5.938305 ## Ethnic.GrpHISPA 0.11448845 0.02776897 4.122891 ## Ethnic.GrpNSPEC 0.13227360 0.02551390 5.184375 ## Ethnic.GrpWHITE 0.14666022 0.02461445 5.958298 ## Union.DescrAdministrative Clerical (NP-3) -0.39303463 0.02210547 -17.779971 ## Union.DescrAmercan Fed of School Admin 0.41117127 0.11893504 3.457108 ## Union.DescrComm College Admin - CCCC -0.22235053 0.03617872 -6.145893 ## Union.DescrComm College Faculty CCCC -0.09661379 0.03681822 -2.624075 ## Union.DescrComm College Mgmt Exclusions 0.54115377 0.16743739 3.231977 ## Union.DescrConfidential -0.16454268 0.04305299 -3.821864 ## Union.DescrConn Assoc Prosecutors 0.29935059 0.05673950 5.275877 ## Union.DescrConnecticut Innovations Inc 0.55456110 0.09265495 5.985229 ## Union.DescrCorrectional Officers (NP-4) -0.09952380 0.02206728 -4.510017 ## Union.DescrCorrectional Supervisor (NP-8) 0.16602156 0.04387205 3.784222 ## Union.DescrCriminal Justice Residual -0.38863702 0.08485137 -4.580209 ## Union.DescrEducation A (P-3A) 0.21950893 0.06301334 3.483531 ## Pr(&gt;|t|) ## (Intercept) 0.000000e+00 ## SexM 7.890072e-14 ## Ethnic.GrpAMIND 1.176634e-03 ## Ethnic.GrpASIAN 1.149428e-08 ## Ethnic.GrpBLACK 3.171674e-09 ## Ethnic.GrpHISPA 3.831477e-05 ## Ethnic.GrpNSPEC 2.294737e-07 ## Ethnic.GrpWHITE 2.810636e-09 ## Union.DescrAdministrative Clerical (NP-3) 1.118520e-67 ## Union.DescrAmercan Fed of School Admin 5.527464e-04 ## Union.DescrComm College Admin - CCCC 8.878783e-10 ## Union.DescrComm College Faculty CCCC 8.727566e-03 ## Union.DescrComm College Mgmt Exclusions 1.241181e-03 ## Union.DescrConfidential 1.348327e-04 ## Union.DescrConn Assoc Prosecutors 1.404436e-07 ## Union.DescrConnecticut Innovations Inc 2.386971e-09 ## Union.DescrCorrectional Officers (NP-4) 6.702718e-06 ## Union.DescrCorrectional Supervisor (NP-8) 1.568656e-04 ## Union.DescrCriminal Justice Residual 4.812871e-06 ## Union.DescrEducation A (P-3A) 5.011306e-04 # Show Adjusted R-squared value statistic &lt;- paste(&quot;Adjusted R-squared&quot;,round(summary(modelLM)$adj.r.squared, 4), sep = &quot;: &quot;) statistic ## [1] &quot;Adjusted R-squared: 0.497&quot; modelANOVA &lt;- aov(Gross.Log ~ ., data = salaryglm) summary(modelANOVA) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sex 1 4.83 4.833 87.14 &lt;2e-16 *** ## Ethnic.Grp 7 13.90 1.986 35.81 &lt;2e-16 *** ## Union.Descr 53 153.71 2.900 52.29 &lt;2e-16 *** ## Age 1 19.54 19.544 352.35 &lt;2e-16 *** ## Residuals 3378 187.37 0.055 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both the LM and ANOVA models confirm that age, union affiliation, sex, and ethnicity play significant roles in determining gross income. It appears these variables are crucial pieces of the income puzzle. 10.6 Conclusion Income Distribution: Gross income leans leftward in its distribution, with log gross income fitting more snugly into a normal distribution. Age and Income: Age positively correlates with income, though this trend dips in the 80-90 age bracket. Gender Pay Gap: Males, on average, earn more than females across similar age, ethnic, and union groups. Ethnicity and Earnings: In the earnings race, Asians and Whites are leading the pack, while the PACIF and unknown groups trail behind. Income Groups: The high, middle, and low gross groups make up 6%, 68%, and 26% of the workforce, respectively. White males are particularly prominent in the high gross group, especially those older individuals. Predictive Factors: The combined factors of age, union, sex, and ethnicity account for around 50% of the variance in log gross income—a substantial but not all-encompassing influence. Now it’s your turn to jump into the data and see what stories it tells. ::: {.exercise #unnamed-chunk-510} Create a folder, download the data file “state_empoyee_salary_data_2017.csv” into it, and create an Rstudio project in the folder. Read in the data file, peak the structure of your data set, clean it and explore the dataset step by step: Select only columns of “EmplId.Empl.Rcd”, “Bi.Weekly.Comp.Rate”, “Age”, “Ethnic.Grp”, “Sex”, “Full.Part”, “City”, get summary information for all these variables. Make sure “EmplId.Empl.Rcd”, “Sex”, “Ethnic.Grp” to be factors, “Age” and “Bi.Weekly.Comp.Rate” to be numerics. Pick up full-time employees in the city “Hartford”. Focus on only those employees with 26 paychecks for the 2017 fiscal year(Hint: Each employee has a unique ID of “EmplId.Empl.Rcd”). Remove sex group other than “F” and “M” if there is any. Hint: Use str() after droplevels(). Label level of empty space in “Ethnic.Grp” as “unknown”. Exclude employees with non-positive “Bi.Weekly.Comp.Rate” if there is any. Group your data set by “EmplId.Empl.Rcd”, “Ethnic.Grp”, and “Sex”. For each employee, calculate average age and bi-weekly rate. Round the means to the nearest integer and rename them as “Age” and “Bi.Weekly.Rate”. Explore data structure and summary. Viualize the distribution of “Bi.Weekly.Rate” and “Age” using histogram and QQ plots. Test the normality. Make log transformation if it is necessary. Use a pie plot for distribution of “Sex”. List the count number and percent of employees in each ethnic group. conjure up a scatter plot to review the relationship between average “Bi.Weekly.Rate” and “Age”, add a regression line and give your interpretation. Employ a box plot for the relationship between “Bi.Weekly.Rate” and Ethnic groups. Create a density plot similar to Figure 10.8 for the distribution of “Bi.Weekly.Rate” across sex. Package ggplot2 is referred. Do ANOVA test for sex differences in “Bi.Weekly.Rate” in city “Hartford”. Explore the interaction of “Ethnic.Grp” and “Sex” versus “Bi.Weekly.Rate” using density ridges plot similar to Figure 10.9 and provide a succinct interpretation of the results. ::: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
