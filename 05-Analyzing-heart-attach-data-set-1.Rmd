# The heart attack data set (I)

The heart attack data set, accessible at  (http://statland.org/R/RC/tables4R.htm) and included in the ActivStats^1^ CD, contains all 12,844 cases of hospital discharges in New York State in 1993. These are patients admitted for heart attack but who did not have surgery. This information is essential for the interpretation of our results, as this is a purely **observational study**, not a random sample or **controlled experiment**. 

The data set is neatly organized in a table format (see Table \@ref(tab:4-01)), with rows representing cases and columns representing characteristics, a standard format for many datasets. If you download and open the file in NotePad++ or Excel, you will find that the columns are separated by tabs, with **missing values** marked by “NA”. Four columns (DIAGNOSIS, SEX, DRG, DIED) contain **nominal** values, representing labels of **categories**. A detailed explanation of data types can be found [here](http://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/). DIAGNOSIS column contains codes defined in the International Classification of Diseases (IDC), 9th Edition. These are the codes that your doctors send to your insurance company for billing. The numbers, such as 41041, are actually codes for specific parts of the heart affected. Although these are numbers, but are not math-friendly. It does not make any sense to add or subtract or compute the mean. You wouldn't average student IDs, would you? Such **categorical data needs to be recognized as factors in R**. Similarly, DRG column has three possible numbers, 121 for survivors with cardiovascular complications, 122 for survivors without complications, and 123 for deceased patients.  Moreover, DIED are codes using 1 for deceased, and 0 for survived. 

Here's a snippet of code to get you started:
```{r }
URL <- "https://raw.githubusercontent.com/gexijin/learnR/master/datasets/heartatk4R.txt"
heartatk4R <- read.table(URL,
                         header = TRUE, 
                         sep = "\t", 
                         colClasses = c("character", "factor", "factor", "factor", 
                                        "factor", "numeric", "numeric", "numeric"))
```

```{r 4-01, echo=FALSE}
knitr::kable(
  head(heartatk4R[, 1:8], 15), 
  booktabs = TRUE,
  caption = 'First 15 rows of the heart attack dataset')
```

Ponder over these questions as you explore the dataset in Excel. What type of people are more likely to suffer from heart attacks? Who is more likely to survive a heart attack?  Suppose your friend, a 65 years old lady with DIAGNOSIS code of 41081, was just admitted to hospital for heart attack. What is the odds that she survives without complication?  Also, consider yourself as a CEO of an insurance company, would you like to know what types of patients rack up higher charges? Should a particular subgroup, such as men or those over 70, pay a higher premium?

To answer these questions, we will:

•	Import data files into R 

•	Conduct Exploratory Data Analysis (EDA) 

•	Implement statistical modeling (regression)
 
```{r }
x <- heartatk4R  # Clone the data for manipulation, call it x 
str(x)  # View data structure and data types
```

## Begin your analysis by examining each column individually
Think of data like a first date. If you are single and meet someone in a bar, you typically start with small talks and learn some basic information about him/her. Similarly, before diving into hypothesis testing or model building, get to know your data.  
We are going to start by examining each column individually, for numeric column, including very basic things like mean, median, ranges, distributions, and normality. This is important because sometimes the data is so skewed or far from normal distribution. In such cases, before conducting further analyses, we need to use **non-parametric tests**, or to **transform the raw data** using log transformation, or more general box-cox transformation. 

>
```{exercise}

>
Perform the following analysis for the heartatk4R dataset. If you forgot the R commands, revisit our previous chapters, or ask the wise Dr. Google.
>
a.	Graphical EDA: Plot the distribution of **charges** using box plot, histogram, qqplot, lag plot, and sequence plot. Interpret your results in PLAIN English. Note that there are missing values in this column that may cause problems for some plots. You can remove missing values by defining a new variable with **temp = CHARGES [ ! is.na (CHARGES)  ]**  and then plot with **temp**. 
b.	Quantitative EDA: Conduct normality tests and calculate confidence intervals. If the Shapiro-Wilk normality test cannot handle data points over 12,000, try other tests in the nortest library or sample randomly with **temp = sample( CHARGES, 4000)**.
```

For categorical columns like SEX and DIAGNOSIS, we want to count different levels, and their frequencies. In addition to quantitative analysis, we'll visualize our findings with **pie charts and bar plots**, using the trusty **table( )** function followed by **pie( )** and **barplot()**.

```{r fig.keep='none'}
barplot(table(x$DIAGNOSIS))
```

This generates a bar plot of counts. It could be further refined with percentages:

(ref:4-1) Barplot by percentage.

```{r 4-1, fig.cap='(ref:4-1)', fig.align='center'}
counts <- sort(table(x$DIAGNOSIS), decreasing = TRUE)  # Tabulate & sort 
percentages <- 100 * counts / length(x$DIAGNOSIS)  # Convert to %
barplot(percentages, las = 3, ylab = "Percentage", col = "green")  # Figure 5.1
```

Note that “las = 3” changes the orientation of the labels for x-axis to vertical. Try plot without it or set it to 2. You can even do all these in single line.
```{r eval=FALSE}
barplot(100* sort(table(x$DIAGNOSIS), decreasing=T) / length(x$DIAGNOSIS), las = 3, ylab = "Percentage", col = "green")
```

(ref:14-2) Pie chart of patients by SEX.

```{r 14-2, message=FALSE, out.width='50%', fig.cap='(ref:14-2)', fig.align='center'}
table(x$SEX)  # Tally up the Ms and Fs
pie(table(x$SEX))  # And turn it into a pie! Yum!
```

>
```{exercise}
Compute the count and percentage of each level of DRG by filling the blanks below. Use bar plot and pie chart similar to Figure \@ref(fig:4-1) and Figure \@ref(fig:14-2) to visualize. Briefly discuss your results like you're explaining it to a five-year-old.
>
drg <- __________               # Import values of DRG 
>
counts <- sort(________(drg), decreasing = TRUE)  # Tabulate & sort 
>
percentages <- 100 * __________  # Compute the percentage and convert to %
>
barplot(percentages, ylab = "Percentage", col = "blue") # Barplot
>
_________(counts)  # Pie chart
```


## Possible correlation between two numeric columns
To explore potential correlations between two numeric columns, various correlation coefficients can be employed, such as Pearson’s correlation coefficient (PPC). This is expressed by the formula:

$$r=∑_{i=1}^{n}(\frac{x_i-\bar{x}}{s_x}) (\frac{y_i-\bar{y}}{s_y})$$ 

where $x_i$ and $y_i$ represent the $i^{th}$ values, $\bar{x}$ and $\bar{y}$ are the sample means, and $s_x$ and $s_y$ are the sample standard deviations. 

Pearson’s correlation ranges from -1 to 1, indicating perfect negative and positive correlation respectively. Remember, **negative correlations are just as important and informative as positive ones**. 

(ref:4-2) Interpretation of Pearson's correlation coefficient.

```{r 4-2, echo=FALSE, out.width='80%'  ,fig.cap='(ref:4-2)', fig.align='center'}
knitr::include_graphics("images/img0402_coefficient.png")
```

Figure \@ref(fig:4-2) provides visual examples of Pearson’s correlation with various scatter plots. The numbers are Pearson’s correlation coefficient r. The second row illustrates that Pearson’s correlation, despite differing slopes, can be uniformly 1. This highlights that Pearson’s correlation only signifies the correlation degree, not the slope. The third row exposes a limitation: Pearson’s correlation cannot detect nonlinear correlations.
 
Table \@ref(tab:4-02) below gives some guidelines for interpreting Pearson’s correlation coefficient. 

```{r echo=FALSE, message=FALSE}
Correlation <- c("-", "Small", "Medium", "Large")
Negative <- c("-0.09 to 0.0", "-0.3 to -0.1", "-0.5 to -0.3", "-1.0 to -0.5")
Positive <- c("0.0 to 0.09", "0.1 to 0.3", "0.3 to 0.5", "0.5 to 1.0")
dat <- data.frame(Correlation, Negative, Positive)
```

```{r 4-02, echo=FALSE}
knitr::kable(
  data.frame(dat),
  booktabs = TRUE,
  caption = 'Interpretation of correlation coefficient'
)
```

There is a small, but statistically significant correlation between age and length of stay in the hospital after a heart attack. **The interpretation in plain English(the explanation that could be understood by your grandmother)**: Older people tend to stay slightly longer in the hospital after a heart attack. 
```{r}
cor.test(x$AGE, x$LOS)
```
Note that the correlation coefficient r and the p value serve two different purposes: r indicates the size of effect, while p value tells statistical significance. Based on the statistic sample, p value tells how certain we are about the difference being real, namely not due to random fluctuation. A large sample size allows us to detect even minor correlations as significant. Conversely, if we only have a few observations, a large r could have large p value, hence not significant. Distinguishing between effect size and significance is vital in statistical analyses.

```{r echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/img0400_sample.png")
```

Like many commonly-used parametric statistical methods which rely on means and standard deviations, the Pearson’s correlation coefficient is not robust, meaning its value are sensitive to outliers and can be misleading. It is also very sensitive to distribution. 

**Non-parametric** approaches typically rank original data and do calculations on the ranks instead of raw data. They are often more robust but might lack sensitivity.  There are corresponding non-parametric versions for most of the parametric tests.

**Spearman’s rank correlation coefficient $\rho$** is a non-parametric correlation measure. The Spearman correlation coefficient ρ is often thought as the Pearson correlation coefficient for ranked variables. However, ρ is calculated more straightforwardly in practice. The n raw scores $X_i$, $Y_i$ are converted to ranks $R_{x_i}$, $R_{y_i}$, and the differences   $d_i$ = $R_{x_i}$-$R_{y_i}$ between the ranks of each observation on the two variables are calculated.

If there are no tied ranks, $\rho$ is given by:$$ρ=1-\frac{6∑d_{i}^{2}}{n(n_{}^{2}-1)}$$

In R, Spearman’s $\rho$ and its significance can be tested by customizing the cor.test() function:
```{r warning=FALSE}
cor.test(x$AGE, x$LOS, method = "spearman")
```

Interpretation of Spearman’s $\rho$ parallels that of Pearson’s r. The statistical significance can also be determined similarly as demonstrated above.

Kendall tau rank correlation coefficient is an alternative non-parametric statistic for correlation. 

We already know that scatter plots are great for visualizing correlation between two numeric columns. However, with many data points, such as our dataset with over 12,000 points, understanding these plots can be challenging, especially when the data is integers and there are a lot of overlapping data points. Yes, graphics can be misleading in such situation. This is where smoothed scatter plot, which uses color densities to represent overlapping data points, become useful.

```{r echo=c(1, 3), fig.show='hold', out.width='50%', fig.cap='Smoothed Scatter plots use colors to code for the density of data points.', fig.align='center'}
plot(x$AGE, x$LOS)  # Standard scatter plot    
text(30, 37, labels = "plot(AGE, LOS)", col = "red")
smoothScatter(x$AGE, x$LOS)  # Smoothed color density representation of a scatterplot  
text(37, 37, labels = "smoothScatter(AGE, LOS)", col = "red")
```

>
```{exercise}

>
Investigate the correlation between length of stay(LOS) and charges by filling in the blanks below. Remember to include plain English interpretation of your results even your grandpa could understand. 
>
LOS <- ______________       # Load LOS data from the heart attack dataset
>
CHARGES <- _____________    # Load CHARGES data
>
cor.test(LOS, CHARGES, method = ___________)  # Conduct Pearson correlation
>
cor.test(LOS, CHARGES, method = __________)   # Conduct Spearman correlation
>
plot(LOS, CHARGES)  # Create a standard scatter plot 
>
__________(LOS, CHARGES)  # Create a smoothed color density representation of a scatterplot
```


## Associations between categorical variables
In the heart attack dataset, four columns—DIAGNOSIS, DRG, SEX, and DIED—contain categorical values. These columns could be associated with each other. For example, we might wonder if men and women are equally likely to survive a heart attack.

Let's start by tabulating SEX and DIED variables:
```{r results='hide', message=FALSE}
counts <- table(x$SEX, x$DIED)  # Create a two-dimensional array of SEX and DIED
counts
```

```{r echo=FALSE, results='hide'}
Sex <- c("F", "M")
DIED_0 <- c("4298", "7136")
DIED_1 <- c("767", "643")
dat <- data.frame(Sex, DIED_0, DIED_1)
data.frame(dat)
```

From these counts, we create a contingency table as seen in Table \@ref(tab:5-01):
```{r 5-01, echo=FALSE}
knitr::kable(
  data.frame(dat),
  booktabs = TRUE,
  caption = 'A 2x2 contingency table summarizing the distribution of DIED frequency by SEX'
  )
```

To explore percentages of survival, we calculate:
```{r}
counts / rowSums(counts) 
```

It turns out that 15.1% of females, compared to 8.27% of males, died in the hospital.  This gender difference is quite a surprise to me. But could this happen just by chance? To answer this question, we turn to a statistical test. Chi-square test can be used for testing the correlation of two categorical variables. The null hypothesis here is that men and women are equally likely to die from a heart attack. 
```{r}
chisq.test(counts)
```

Have you seen such a tiny p-value before? Probably! It is one of the smallest non-zero values R displays for many tests. This p is definitely small! Therefore, we reject the hypothesis that the mortality rate is the same for men and women. Looking at the data, it is higher for women. The chi-square test gives accurate p-values if expected observation is greater than 5. If not, the Fisher Exact test, more computationally intense but accurate for small datasets, is advisable. The chi-square test is an approximation to the Fisher Exact test. Karl Pearson developed the chi-square approximation before we had computers. Today's computational power makes the Fisher Exact test feasible for small datasets.

You can also use the chi-square test for contingency tables that have more than two rows or two columns. For more complex contingency tables, the Chi-square approximation remains accurate if each cell's expected observation is over 1, and less than 20% of cells have expectations under 5. Again, the Fisher Exact test can handle quite large data sets with today’s computers, without the Chi-square limitations.
```{r }
fisher.test(counts)  # Fisher’s Exact test
```
In this case, the result of fisher's test is the same as chi-square test.

To present findings to a boss who is either stupid or too busy, you need a chart. Below, we display two types of bar plots: stacked and side by side.  

(ref:5-1) Barplot illustrating correlation of two categorical variables. A. Stacked; B. Side by side.

```{r 5-1, echo=c(1, 2, 4), fig.show='hold', out.width='50%', fig.cap='(ref:5-1)', fig.align='center'}
counts <- table(x$DIED, x$SEX) 

# Figure A
barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = "DIED", beside = F) 
text(0.42, 7500, labels = "A")

# Figure B
barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = "DIED", beside = T) 
text(1, 7000, labels = "B")
```

To change the position of legend, add an argument such as "*args.legend = list(x = "topleft")*", in the function barplot(). For example, we can move the legend to the top-left of Figure \@ref(fig:5-1) A.

```{r}
barplot(counts, legend = rownames(counts), col = rainbow(2), xlab = "DIED", 
        beside = F, args.legend = list(x = "topleft"))
```


Mosaic plots are another effective way to visualize proportions, where the size and color of blocks represent different combinations of categories.

(ref:5-2) Mosaic plot of DIED by SEX.

```{r 5-2, fig.width=6, fig.height=4, fig.cap='(ref:5-2)', fig.align='center'}
mosaicplot(table(x$SEX, x$DIED), color = T, main = "") 
```

The mosaic plot in Figure \@ref(fig:5-2) is similar to the barplot in Figure \@ref(fig:5-1), but the bars are stretched to the same height, and the widths are defined by proportion of Male vs. Female. The size of the four blocks represents the counts of the corresponding combination. The blocks are also color-coded for different combination. Horizontally, the blocks are divided by SEX; There are more men than women in this dataset. Vertically, the blocks are divided by DIED (1 for died in hospital); Regardless of gender, only a small proportion of patients died in hospital. We also observe higher percentage of women died in hospital — a trend that raises questions and concerns.

Mosaic plots are also useful for visualizing relationships involving multiple factors.

(ref:5-3) Mosaic plot of three factors.

```{r 5-3, fig.cap='(ref:5-3)', fig.align='center'}
mosaicplot(table(x$SEX, x$DIED, x$DRG), color = rainbow(3), main = "")
```

Here we nested the tabulate command inside the mosaic plot. As shown in Figure \@ref(fig:5-3), we further divided each of the 4 quadrants of Figure \@ref(fig:5-2) into three parts in red, green and blue, according to DRG codes. Interestingly,a smaller proportion of surviving males developed complications, compared with females. 

The Titanic dataset, built into R, offers another opportunity for mosaic plot to explore survival patterns.
```{r message=FALSE, eval = FALSE}
? Titanic   # Access information on the famous Titanic dataset
```


```{r message=FALSE}
mosaicplot(~ Sex + Age + Survived, data = Titanic, color = rainbow(2))  
```

Inspect the full dataset via a mosaic plot: 
```{r}
mosaicplot(Titanic, color = rainbow(2)) 
```

Think about the questions: Were survival rates different between men and women? Did girls and women survive by equal proportion?

>
```{exercise}

>
The DIAGNOSIS column contains IDC codes that specify the parts of the affected hearts. Use a stacked bar plot and a mosaic plot to compare DIAGNOSIS frequency differences between men and women. Based on your observation, do you think men and women are equal in the diagnoses frequencies? 
>
DIAGNOSIS <- heartatk4R$DIAGNOSIS
>
SEX <- heartatk4R$SEX
>
counts <- _____________(DIAGNOSIS, SEX)  # Take SEX as columns 
>
barplot(_____________________________________________________)
>
legend("topleft", legend = rownames(counts), fill = rainbow(9), ncol = 3, cex = 0.75)
>
mosaicplot(_____________________________________________)
```

## Associations between a categorical and a numeric variables 
Exploring associations between different types of variables, like categorical and numeric, can offer valuable insights. Questions such as "Do women stay longer in hospitals?" or "Do charges vary with different diagnoses?" can be addressed with techniques like T-tests and ANOVA, akin to our analysis of the Iris flower dataset. For visualization, **boxplots** are straightforward, but the ggplot2 package allows a more detailed examination of variable distributions across groups.

(ref:new1) Histogram of AGE by SEX.

```{r new1, fig.width=6, fig.height=4, fig.cap='(ref:new1)', fig.align='center'}
library(ggplot2)
ggplot(heartatk4R, aes(x = AGE)) +
  geom_histogram(aes(y = after_stat(count / sum(count) * 100), fill = factor(SEX)), 
                 binwidth = 6, colour = "black") +
  facet_grid(SEX ~ .) + # Split a graph according to panels defined by SEX
  labs(y = "Percent of Total", x = "Age") 
```

The histograms reveal that women's ages are more skewed to the right, indicating that women in the dataset are considerably older than men. I am surprised at first by this huge difference, as the average age of women is bigger by 11. Further research aligns with this finding that heart attack symptoms in women are often milder and less noticeable. 

We can further divide the population according to survival status by adding another factor:

(ref:new2) Histogram of AGE by SEX and DIED.
```{r new2, warning=FALSE, message=FALSE, fig.cap='(ref:new2)', fig.align='center'}
# Creating a new grouping variable 'GROUP' as a combination of 'DIED' and 'SEX'
heartatk4R$GROUP <- paste(heartatk4R$DIED, heartatk4R$SEX, sep = "-")

# Plot the histogram
ggplot(heartatk4R, aes(x = AGE, group = GROUP)) +
  geom_histogram(
    aes(y = after_stat(count / tapply(count, PANEL, sum)[PANEL] * 100), fill = GROUP), 
    binwidth = 5, 
    color = "black"
  ) +
  facet_grid(DIED ~ SEX) + # Split a graph according to a matrix of panels defined by DIED and SEX
  labs(y = "Percent of Total", x = "Age") 
```


This analysis shows that older patients, regardless of sex, are less likely to survive a heart attack. This trend is even more apparent in a density plot. Instead of splitting into multiple panels, the curves are overlaid.

(ref:new3) Density plot of AGE by SEX.

```{r new3, fig.width=6, fig.height=3, fig.cap='(ref:new3)', fig.align='center'}
ggplot(heartatk4R, aes(x = AGE, fill = SEX)) + geom_density(alpha = .3) 
```
The density plot reaffirms the age skew observed in the histograms \@ref(fig:new1).  

Now for each gender, we further divide the patients by their survival status.  

(ref:new4) Density plot of AGE by SEX and DIED.

```{r new4, fig.width=6, fig.height=4, fig.cap='(ref:new4)', fig.align='center'}
ggplot(heartatk4R, aes(x = AGE, fill = DIED)) + 
  geom_density(alpha = .3) + 
  facet_grid(SEX ~ .) 
```
Overlaying survival curves indicates a clear age-related trend in survival rates: younger patients tend to survive for both men and women.

>
```{exercise}

>
Use the ggplot2 package to compare the length of hospital stay distributions between survivors and non-survivors. Include both histogram plot and density plot. Interpret your results. 
```

>
```{exercise}

>
Separate the above analysis by gender, as demonstrated in Figure \@ref(fig:new4)). 
```

>
```{exercise}

>
Examine age distribution differences between survivors and non-survivors using t-tests, boxplots, histograms, and density plots.
```

>
```{exercise}

>
Compare charge variations among different DRG codes using ANOVA, boxplots, histograms, and density plots. 
```

## Associations between multiple columns
The ggplot2 package can also shed light on correlations between multiple variables through multi-panel figures.

(ref:5-6) Multiple boxplots of AGE by DRG and SEX.

```{r 5-6, fig.width=5, fig.height=3, fig.cap='(ref:5-6)', fig.align='center'}
ggplot(heartatk4R, aes(x = DRG, y = AGE)) + 
  geom_boxplot(color = "blue") + 
  facet_grid(SEX ~ .)
```

Recall that 121 indicates survivors with complication, 122 for survivors with no complication, and 123 for died. This visualization suggests older patients are more likely to die in hospital and those with complications are generally older. It raises the question: Does this mean they also had longer hospital stays?

>
```{exercise}

>
Are the surviving women younger than the women who died? Similar question can be asked for men. Produce a figure to compare age distributions between spatients who died in the hospital and those who survived, separated by gender.
```

>
```{exercise}

>
Use ggplot2 to create boxplots comparing the lengths of stay across different DRG categories, differentiated by gender. The produced plots should be similar to Figure 5.13.  Offer interpretation.
```
>
```{r echo=FALSE, fig.width=6, fig.height=4, fig.cap='Multiple boxplots.', fig.align='center'}
ggplot(heartatk4R, aes(x = SEX, y = AGE)) + 
  geom_boxplot(color = "blue") + 
  coord_flip() + 
  facet_grid(DRG ~ .) +
  ggtitle("Multiple boxplots using ggplot2 package") +
  theme(plot.title = element_text(hjust = 0.5))
```

Scatterplots offer another dimension of analysis:
```{r fig.width=6, fig.height=4, fig.keep='last', fig.align='center'}
# Scatterplot of LOS vs. AGE
ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() 
# Scatterplot of LOS vs. AGE, divided by SEX
ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .)  
# Scatterplot colored by DIED
ggplot(heartatk4R, aes(x = AGE, y = LOS, color = DIED)) + 
  geom_point() + 
  facet_grid(SEX ~ .)  
```

Here we only show the figure for the third code.
Note that ggplot(heartatk4R, aes(x = DRG, y = AGE)) + geom_point() + facet_grid(SEX ~ .) generates multiple scatter plots of LOS ~ AGE according to different values of SEX, while color = DIED adds these two color-coded scatter plots into the same figure. 

(ref:5-7) A scatter plot of LOS vs. AGE, using SEX and DIED as factors.

```{r 5-7, echo=FALSE, fig.cap='(ref:5-7)', fig.align='center'}
# Scatterplot of LOS vs. AGE, divided by SEX and DIED
ggplot(heartatk4R, aes(x = AGE, y = LOS)) + 
  geom_point(color = "blue") + 
  facet_grid(SEX ~ DIED)  
```

Figure \@ref(fig:5-7) suggests the positive association between age and length of stay in hospital for the survivors, regardless of sex. This is a statistician’s language. Try this instead that could be understood by both the statistician and his/her grandmother: Older patients tend to stay longer in the hospital after surviving a heart attack. This is true for both men and women. 

Another way to visualize complex correlation is **bubble plot**.  It's an enhanced scatter plot, using an additional dimension of data to determine the size of the symbols.  Check out the interesting video using bubble plot: [http://youtu.be/jbkSRLYSojo](http://youtu.be/jbkSRLYSojo) 

(ref:5-8) Bubble plot example.

```{r 5-8, fig.cap='(ref:5-8)', fig.align='center'}
y <- x[sample(1:12844, 200), ]   # Randomly sample 200 patients
plot(y$AGE, y$LOS, cex = y$CHARGES / 6000, col = rainbow(2)[y$SEX], 
     xlab = "AGE", ylab = "LOS")
legend("topleft", levels(y$SEX), col = rainbow(2), pch = 1)
```

Figure \@ref(fig:5-8) is a busy plot. Female patients are shown in red while males in blue. Size of the symbol is proportional to charges. So this plot offers a multi-dimensional view!

Additional common methods to detect complex correlations and structures include principal component analysis (PCA), Multidimensional scaling (MDS), hierarchical clustering etc. 

All of these techniques we introduced so far enable us to **LEARN** about the dataset without any of priory hypothesis, ideas, and judgments. Many companies claim that they want to know their customers first as individuals and then do business. Same thing applies to data mining. You need to know you dataset as it is before making predictions, classifications, etc. 
  
Remember, data analysis is a conversation with your dataset: **INTERACT** with your data by asking questions based on domain knowledge and common sense. Then generate lots and lots of plots to support or challenge the hypothesis you may have, and be open to surprising discoveries. This approach, exemplified with the heart attack dataset, is universally applicable. Sometimes, the insights you uncover are more important than the initial objectives. 


1.	Velleman, P. F.; Data Description Inc. ActivStats, 2000-2001 release.; A.W. Longman,: Glenview, IL, 2001.